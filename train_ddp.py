#!/usr/bin/env python3
"""
DistributedDataParallel (DDP) è¨“ç·´è…³æœ¬ - å¤š GPU é«˜æ•ˆè¨“ç·´

ç›¸æ¯” DataParallel:
- æ›´é«˜çš„è¨“ç·´æ•ˆç‡ï¼ˆç„¡ä¸» GPU ç“¶é ¸ï¼‰
- æ›´å¥½çš„è¨˜æ†¶é«”åˆ©ç”¨
- å¯ä»¥è·¨ç¯€é»æ“´å±•

ä½¿ç”¨æ–¹æ³•:
    # å–®æ©Ÿå¤š GPUï¼ˆæ¨è–¦ï¼‰
    python -m torch.distributed.launch --nproc_per_node=8 train_ddp.py

    # æˆ–ä½¿ç”¨ torchrunï¼ˆPyTorch 1.10+ï¼‰
    torchrun --nproc_per_node=8 train_ddp.py

    # æŒ‡å®šç‰¹å®š GPU
    CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_ddp.py

Author: TTS ETL Pipeline
Version: 1.0
"""

import copy
import gc
import os
import random
from datetime import datetime
from typing import List, Optional, Tuple

import numpy as np
import sentencepiece as spm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from peft import LoraConfig, TaskType, get_peft_model
from peft.optimizers import create_loraplus_optimizer
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import get_cosine_schedule_with_warmup

from indextts.BigVGAN.models import BigVGAN
from indextts.data_utils import (
    collate_finetune_fn,
    load_finetune_datasets,
    load_speaker_conditions,
)
from indextts.gpt.model import UnifiedVoice

# Import train utilities
from train import (
    load_UnifiedVoice,
    clear_torch_cache,
    forward_gpt2,
    forward_fn,
    top_k_accuracy,
)


def setup_ddp(rank: int, world_size: int):
    """åˆå§‹åŒ– DDP ç’°å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # åˆå§‹åŒ–é€²ç¨‹çµ„
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """æ¸…ç† DDP ç’°å¢ƒ"""
    dist.destroy_process_group()


class DDPTrainer:
    """DDP è¨“ç·´å™¨ - é«˜æ•ˆå¤š GPU è¨“ç·´"""

    def __init__(self, config: DictConfig, rank: int, world_size: int):
        """
        åˆå§‹åŒ– DDP è¨“ç·´å™¨

        Args:
            config: é…ç½®ç‰©ä»¶
            rank: ç•¶å‰é€²ç¨‹çš„ GPU ID (0-7)
            world_size: ç¸½ GPU æ•¸é‡ (8)
        """
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank}")
        self.is_main_process = (rank == 0)

        # åªæœ‰ä¸»é€²ç¨‹æ‰“å°è¨Šæ¯
        if not self.is_main_process:
            logger.remove()  # ç§»é™¤å…¶ä»–é€²ç¨‹çš„æ—¥èªŒè¼¸å‡º

        # è¨­å®šéš¨æ©Ÿç¨®å­
        self._set_seed(self.config.train.seed + rank)  # æ¯å€‹ rank ä¸åŒçš„ç¨®å­

        # æº–å‚™ç›®éŒ„å’Œæ—¥èªŒ
        self.finetune_dir = self.config.train.finetune_model_dir
        self.checkpoint_dir = os.path.join(self.finetune_dir, "checkpoints")
        if self.is_main_process:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            self._setup_logging()

        # è¼‰å…¥èªªè©±äººæ¢ä»¶å‘é‡
        self.speaker_conditions = load_speaker_conditions(config)
        self.speaker_list = list(self.speaker_conditions.keys())
        if self.is_main_process:
            logger.info(f"Loaded conditions for {len(self.speaker_list)} speakers: {self.speaker_list}")

        # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
        self._load_models()

        # è¨­å®šæœ€ä½³åŒ–å™¨å’Œæ’ç¨‹å™¨
        self._setup_optimizer_and_scheduler()

        # åˆå§‹åŒ–è¨“ç·´ç‹€æ…‹
        self.best_val_loss = (0, float('inf'), float('inf'))
        self.update_steps = 0

    def _set_seed(self, seed: int):
        """è¨­å®šéš¨æ©Ÿç¨®å­"""
        random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        if self.is_main_process:
            logger.info(f"Set random seed to {seed} for rank {self.rank}")

    def _setup_logging(self):
        """é…ç½®æ—¥èªŒè¨˜éŒ„å™¨ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        log_path = os.path.join(self.checkpoint_dir, f"train_ddp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        logger.add(log_path, level="INFO", encoding="utf-8")
        logger.info(f"ğŸš€ DDP Training with {self.world_size} GPUs")
        logger.info("Full configuration:\n" + OmegaConf.to_yaml(self.config))

    def _load_models(self):
        """è¼‰å…¥æ¨¡å‹"""
        if self.is_main_process:
            logger.info("Loading models...")

        # BPE
        bpe_model_path = os.path.join(self.finetune_dir, self.config.dataset.bpe_model)
        self.bpe_model = spm.SentencePieceProcessor(bpe_model_path)

        # UnifiedVoice
        gpt_checkpoint_path = os.path.join(self.finetune_dir, self.config.gpt_checkpoint)
        self.model = load_UnifiedVoice(self.config.gpt, gpt_checkpoint_path, self.device)

        # æ‡‰ç”¨ LoRA
        self.model = self._apply_lora(self.model)

        # ä½¿ç”¨ DDP åŒ…è£æ¨¡å‹
        self.model = DDP(
            self.model,
            device_ids=[self.rank],
            output_device=self.rank,
            find_unused_parameters=False  # è¨­ç‚º True å¦‚æœæœ‰æœªä½¿ç”¨çš„åƒæ•¸
        )

        if self.is_main_process:
            logger.info(f"âœ… Model wrapped with DDP on {self.world_size} GPUs")

        # è¨»å†Šèªªè©±äººæ¢ä»¶
        self.speaker_mean_conditions = {}
        for speaker_id, condition in self.speaker_conditions.items():
            if condition.ndim == 2:
                condition = condition.unsqueeze(0)
            param = torch.nn.Parameter(condition.to(self.device), requires_grad=True)
            param_name = f"mean_condition_{speaker_id}"
            self.model.module.register_parameter(param_name, param)
            self.speaker_mean_conditions[speaker_id] = param

        if self.is_main_process:
            logger.info(f"Loaded {len(self.speaker_mean_conditions)} speaker conditions")

    def _apply_lora(self, model: UnifiedVoice) -> UnifiedVoice:
        """æ‡‰ç”¨ LoRA é…ç½®"""
        lora_cfg = self.config.train.lora
        gpt_lora_config = LoraConfig(
            r=lora_cfg.r,
            target_modules=lora_cfg.target_modules,
            task_type=TaskType.CAUSAL_LM,
            lora_alpha=lora_cfg.lora_alpha,
            lora_dropout=lora_cfg.lora_dropout,
            bias="none",
        )
        model.requires_grad_(False)
        model.inference_model = get_peft_model(model.inference_model, gpt_lora_config)
        return model

    def _setup_optimizer_and_scheduler(self, num_training_steps: int = 1000):
        """è¨­å®šæœ€ä½³åŒ–å™¨å’Œæ’ç¨‹å™¨"""
        opt_cfg = self.config.train.optimizer
        self.optimizer = create_loraplus_optimizer(
            model=self.model,
            optimizer_cls=AdamW,
            lr=opt_cfg.learning_rate,
            loraplus_lr_ratio=opt_cfg.loraplus_lr_ratio,
            loraplus_lr_embedding=opt_cfg.get("loraplus_lr_embedding", 1e-6),
            weight_decay=opt_cfg.weight_decay,
        )

        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=opt_cfg.warmup_steps,
            num_training_steps=num_training_steps,
        )

    def train(self, train_ds: Dataset, valid_ds: Dataset):
        """è¨“ç·´æµç¨‹"""
        train_cfg = self.config.train

        # ä½¿ç”¨ DistributedSampler
        train_sampler = DistributedSampler(
            train_ds,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        )

        # è¨ˆç®—è¨“ç·´æ­¥æ•¸ï¼ˆæ³¨æ„ï¼šDDP ä¸‹æ¯å€‹é€²ç¨‹çœ‹åˆ°çš„è³‡æ–™é‡æ˜¯ç¸½é‡çš„ 1/world_sizeï¼‰
        samples_per_epoch = len(train_ds) // self.world_size
        total_update_steps = samples_per_epoch * train_cfg.epochs
        self._setup_optimizer_and_scheduler(num_training_steps=total_update_steps)

        if self.is_main_process:
            logger.info(f"Starting DDP training for {train_cfg.epochs} epochs")
            logger.info(f"Samples per epoch (per GPU): {samples_per_epoch}")
            logger.info(f"Total samples: {len(train_ds)}")
            logger.info(f"Total update steps (per GPU): {total_update_steps}")

        text_weight = train_cfg.text_weight

        for epoch in range(train_cfg.epochs):
            # è¨­å®š epoch ä»¥ç¢ºä¿ shuffle æ­£ç¢º
            train_sampler.set_epoch(epoch)

            if self.is_main_process:
                logger.info(f"=" * 60)
                logger.info(f"EPOCH {epoch + 1}/{train_cfg.epochs} started")
                logger.info(f"=" * 60)

            self.model.train()

            for batch_idx, batch in enumerate(train_ds):
                # å°‡è³‡æ–™ç§»åˆ°å°æ‡‰çš„ GPU
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, mel_accuracy = self._train_step(tuple(data_batch))
                acc_1, acc_10, acc_20 = mel_accuracy["acc_1"], mel_accuracy["acc_10"], mel_accuracy["acc_20"]

                weighted_loss = text_weight * loss_text + (1.0 - text_weight) * loss_mel

                if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                    if self.is_main_process:
                        logger.warning(f"NaN/Inf loss at epoch {epoch}, batch {batch_idx}. Skipping.")
                    continue

                # æœ€ä½³åŒ–æ­¥é©Ÿ
                self.optimizer.zero_grad()
                weighted_loss.backward()
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg.max_grad_norm)
                self.optimizer.step()
                self.scheduler.step()
                self.update_steps += 1

                # åªæœ‰ä¸»é€²ç¨‹æ‰“å°æ—¥èªŒ
                if self.is_main_process and batch_idx % 10 == 0:  # æ¯ 10 batch æ‰“å°ä¸€æ¬¡
                    logger.info(
                        f"[GPU 0/{self.world_size}] Epoch {epoch + 1}/{train_cfg.epochs} | "
                        f"Batch {batch_idx}/{len(train_ds)} | "
                        f"text_loss={loss_text.item():.4f}, mel_loss={loss_mel.item():.4f}, "
                        f"acc@1={acc_1:.2f}%, acc@10={acc_10:.2f}%, acc@20={acc_20:.2f}%, "
                        f"grad_norm={grad_norm.item():.2f}"
                    )

            # é©—è­‰ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰
            if self.is_main_process:
                val_text_loss, val_mel_loss, _, _, _ = self._validate_epoch(valid_ds, epoch)
                self._save_checkpoint(epoch, val_text_loss, val_mel_loss)

            # åŒæ­¥æ‰€æœ‰é€²ç¨‹
            dist.barrier()

    def _train_step(self, batch: Tuple) -> Tuple[torch.Tensor, torch.Tensor, dict]:
        """å–®å€‹è¨“ç·´æ­¥é©Ÿ"""
        # è§£åŒ… batchï¼ˆèˆ‡åŸå§‹ Trainer ç›¸åŒï¼‰
        text_ids, text_lengths, mel_spec, mel_lengths, mel_codes, codes_lengths, speaker_ids = batch

        # ä½¿ç”¨èªªè©±äººæ¢ä»¶
        batch_speaker_ids = list(speaker_ids)
        speaker_means = [self.speaker_mean_conditions[sid] for sid in batch_speaker_ids]
        speaker_means = torch.cat(speaker_means, dim=0)

        # å‰å‘å‚³æ’­
        outputs = forward_fn(
            self.model.module,  # DDP éœ€è¦ä½¿ç”¨ .module
            text_ids,
            text_lengths,
            mel_spec,
            mel_lengths,
            mel_codes,
            codes_lengths,
            speaker_ids=batch_speaker_ids,
            output_loss=True,
            output_logits=False,
            add_mel_stop_token=self.config.train.add_mel_stop_token,
        )

        loss_text, loss_mel = outputs["loss"]
        mel_accuracy = outputs["mel_accuracy"]

        return loss_text, loss_mel, mel_accuracy

    def _validate_epoch(self, valid_ds: Dataset, epoch: int):
        """é©—è­‰ï¼ˆç°¡åŒ–ç‰ˆï¼Œåªåœ¨ä¸»é€²ç¨‹åŸ·è¡Œï¼‰"""
        self.model.eval()
        total_text_loss = 0.0
        total_mel_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in valid_ds:
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, _ = self._train_step(tuple(data_batch))
                total_text_loss += loss_text.item()
                total_mel_loss += loss_mel.item()
                num_batches += 1

        avg_text_loss = total_text_loss / max(num_batches, 1)
        avg_mel_loss = total_mel_loss / max(num_batches, 1)

        logger.info(f"Validation Epoch {epoch + 1}: text_loss={avg_text_loss:.4f}, mel_loss={avg_mel_loss:.4f}")

        return avg_text_loss, avg_mel_loss, 0.0, 0.0, 0.0

    def _save_checkpoint(self, epoch: int, val_text_loss: float, val_mel_loss: float):
        """å„²å­˜ checkpointï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        checkpoint_path = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pt")
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.module.state_dict(),  # DDP éœ€è¦ä½¿ç”¨ .module
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_text_loss': val_text_loss,
            'val_mel_loss': val_mel_loss,
        }, checkpoint_path)
        logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")


def main():
    """ä¸»å‡½æ•¸"""
    # ç²å– DDP ç’°å¢ƒè®Šæ•¸
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("âŒ è«‹ä½¿ç”¨ torch.distributed.launch æˆ– torchrun å•Ÿå‹•æ­¤è…³æœ¬")
        print("ç¯„ä¾‹: torchrun --nproc_per_node=8 train_ddp.py")
        return

    # åˆå§‹åŒ– DDP
    setup_ddp(local_rank, world_size)

    try:
        # è¼‰å…¥é…ç½®
        config = OmegaConf.load("config.yaml")

        # å‰µå»ºè¨“ç·´å™¨
        trainer = DDPTrainer(config, local_rank, world_size)

        # è¼‰å…¥è³‡æ–™é›†
        train_ds, valid_ds = load_finetune_datasets(config)

        # é–‹å§‹è¨“ç·´
        trainer.train(train_ds, valid_ds)

    finally:
        # æ¸…ç†
        cleanup_ddp()


if __name__ == "__main__":
    main()
