#!/usr/bin/env python3
"""
DistributedDataParallel (DDP) è¨“ç·´è…³æœ¬ - å¤š GPU é«˜æ•ˆè¨“ç·´

ç›¸æ¯” DataParallel:
- æ›´é«˜çš„è¨“ç·´æ•ˆç‡ï¼ˆç„¡ä¸» GPU ç“¶é ¸ï¼‰
- æ›´å¥½çš„è¨˜æ†¶é«”åˆ©ç”¨
- å¯ä»¥è·¨ç¯€é»æ“´å±•

ä½¿ç”¨æ–¹æ³•:
    # å–®æ©Ÿå¤š GPUï¼ˆæ¨è–¦ï¼‰
    python -m torch.distributed.launch --nproc_per_node=8 train_ddp.py

    # æˆ–ä½¿ç”¨ torchrunï¼ˆPyTorch 1.10+ï¼‰
    torchrun --nproc_per_node=8 train_ddp.py

    # æŒ‡å®šç‰¹å®š GPU
    CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_ddp.py

Author: TTS ETL Pipeline
Version: 1.0
"""

import copy
import gc
import os
import random
from datetime import datetime
from typing import List, Optional, Tuple

import numpy as np
import sentencepiece as spm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from peft import LoraConfig, TaskType, get_peft_model
from peft.optimizers import create_loraplus_optimizer
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import get_cosine_schedule_with_warmup

from indextts.BigVGAN.models import BigVGAN
from indextts.data_utils import (
    collate_finetune_fn,
    load_finetune_datasets,
    load_speaker_conditions,
)
from indextts.gpt.model import UnifiedVoice

# Import train utilities
from train import (
    load_UnifiedVoice,
    normalize_state_dict_keys,
    clear_torch_cache,
    forward_gpt2,
    forward_UnifiedVoice,
    top_k_accuracy,
)


def setup_ddp(rank: int, world_size: int):
    """åˆå§‹åŒ– DDP ç’°å¢ƒ"""
    # è¨­å®šé è¨­å€¼ï¼ˆå¦‚æœ torchrun æ²’æœ‰è¨­å®šï¼‰
    if 'MASTER_ADDR' not in os.environ:
        os.environ['MASTER_ADDR'] = 'localhost'
    if 'MASTER_PORT' not in os.environ:
        os.environ['MASTER_PORT'] = '12355'

    # åˆå§‹åŒ–é€²ç¨‹çµ„
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """æ¸…ç† DDP ç’°å¢ƒ"""
    dist.destroy_process_group()


class DDPTrainer:
    """DDP è¨“ç·´å™¨ - é«˜æ•ˆå¤š GPU è¨“ç·´"""

    def __init__(self, config: DictConfig, rank: int, world_size: int):
        """
        åˆå§‹åŒ– DDP è¨“ç·´å™¨

        Args:
            config: é…ç½®ç‰©ä»¶
            rank: ç•¶å‰é€²ç¨‹çš„ GPU ID (0-7)
            world_size: ç¸½ GPU æ•¸é‡ (8)
        """
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank}")
        self.is_main_process = (rank == 0)

        # åªæœ‰ä¸»é€²ç¨‹æ‰“å°è¨Šæ¯
        if not self.is_main_process:
            logger.remove()  # ç§»é™¤å…¶ä»–é€²ç¨‹çš„æ—¥èªŒè¼¸å‡º

        # è¨­å®šéš¨æ©Ÿç¨®å­
        self._set_seed(self.config.train.seed + rank)  # æ¯å€‹ rank ä¸åŒçš„ç¨®å­

        # æº–å‚™ç›®éŒ„å’Œæ—¥èªŒ
        self.finetune_dir = self.config.train.finetune_model_dir
        self.checkpoint_dir = os.path.join(self.finetune_dir, "checkpoints")
        if self.is_main_process:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            self._setup_logging()

        # è¼‰å…¥èªªè©±äººæ¢ä»¶å‘é‡
        self.speaker_conditions = load_speaker_conditions(config)
        self.speaker_list = list(self.speaker_conditions.keys())
        if self.is_main_process:
            logger.info(f"Loaded conditions for {len(self.speaker_list)} speakers: {self.speaker_list}")

        # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
        self._load_models()

        # åˆå§‹åŒ–è¨“ç·´ç‹€æ…‹ï¼ˆoptimizer æœƒåœ¨ train() è£¡è¨­å®šï¼‰
        self.optimizer = None
        self.scheduler = None
        self.best_val_loss = (0, float('inf'), float('inf'))
        self.update_steps = 0

    def _set_seed(self, seed: int):
        """è¨­å®šéš¨æ©Ÿç¨®å­"""
        random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        if self.is_main_process:
            logger.info(f"Set random seed to {seed} for rank {self.rank}")

    def _setup_logging(self):
        """é…ç½®æ—¥èªŒè¨˜éŒ„å™¨ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        log_path = os.path.join(self.checkpoint_dir, f"train_ddp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        logger.add(log_path, level="INFO", encoding="utf-8")
        logger.info(f"ğŸš€ DDP Training with {self.world_size} GPUs")
        logger.info("Full configuration:\n" + OmegaConf.to_yaml(self.config))

    def _load_models(self):
        """è¼‰å…¥æ¨¡å‹"""
        if self.is_main_process:
            logger.info("Loading models...")

        # BPE
        bpe_model_path = os.path.join(self.finetune_dir, self.config.dataset.bpe_model)
        self.bpe_model = spm.SentencePieceProcessor(bpe_model_path)

        # UnifiedVoice
        gpt_checkpoint_path = os.path.join(self.finetune_dir, self.config.gpt_checkpoint)
        self.model = load_UnifiedVoice(self.config.gpt, gpt_checkpoint_path, self.device)

        # æ‡‰ç”¨ LoRA
        self.model = self._apply_lora(self.model)

        # ä½¿ç”¨ DDP åŒ…è£æ¨¡å‹
        self.model = DDP(
            self.model,
            device_ids=[self.rank],
            output_device=self.rank,
            find_unused_parameters=False  # è¨­ç‚º True å¦‚æœæœ‰æœªä½¿ç”¨çš„åƒæ•¸
        )

        if self.is_main_process:
            logger.info(f"âœ… Model wrapped with DDP on {self.world_size} GPUs")

        # è¨»å†Šèªªè©±äººæ¢ä»¶
        self.speaker_mean_conditions = {}
        from tqdm import tqdm
        iterator = self.speaker_conditions.items()
        if self.is_main_process:
            iterator = tqdm(iterator, desc="Register speaker conditions", ncols=100)
        for speaker_id, condition in iterator:
            if condition.ndim == 2:
                condition = condition.unsqueeze(0)
            param = torch.nn.Parameter(condition.to(self.device), requires_grad=True)
            param_name = f"mean_condition_{speaker_id}"
            self.model.module.register_parameter(param_name, param)
            self.speaker_mean_conditions[speaker_id] = param
            if self.is_main_process and hasattr(iterator, "set_postfix"):
                iterator.set_postfix({"last": speaker_id})
        if self.is_main_process and hasattr(iterator, "close"):
            iterator.close()

        if self.is_main_process:
            logger.info(f"Loaded {len(self.speaker_mean_conditions)} speaker conditions")

    def _apply_lora(self, model: UnifiedVoice) -> UnifiedVoice:
        """æ‡‰ç”¨ LoRA é…ç½®"""
        lora_cfg = self.config.train.lora
        gpt_lora_config = LoraConfig(
            r=lora_cfg.r,
            target_modules=lora_cfg.target_modules,
            task_type=TaskType.CAUSAL_LM,
            lora_alpha=lora_cfg.lora_alpha,
            lora_dropout=lora_cfg.lora_dropout,
            bias="none",
        )
        model.requires_grad_(False)
        model.inference_model = get_peft_model(model.inference_model, gpt_lora_config)
        return model

    def _setup_optimizer_and_scheduler(self, num_training_steps: int = 0):
        """è¨­å®šæœ€ä½³åŒ–å™¨å’Œæ’ç¨‹å™¨"""
        opt_cfg = self.config.train.optimizer
        self.optimizer = create_loraplus_optimizer(
            model=self.model,
            optimizer_cls=AdamW,
            lr=opt_cfg.learning_rate,
            loraplus_lr_ratio=opt_cfg.loraplus_lr_ratio,
            loraplus_lr_embedding=opt_cfg.get("loraplus_lr_embedding", 1e-6),
            weight_decay=opt_cfg.weight_decay,
        )

        warmup_steps = opt_cfg.get("warmup_steps", None)
        if warmup_steps is None:
            warmup_ratio = opt_cfg.get("warmup_ratio", 0.0)
            if warmup_ratio > 0 and num_training_steps > 0:
                warmup_steps = max(1, int(num_training_steps * warmup_ratio))
            else:
                warmup_steps = 0

        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=max(num_training_steps, warmup_steps + 1),
        )

    def train(self, train_ds: Dataset, valid_ds: Dataset, resume_checkpoint: str = None):
        """è¨“ç·´æµç¨‹"""
        train_cfg = self.config.train
        start_epoch = 0

        # ä½¿ç”¨ DistributedSampler
        train_sampler = DistributedSampler(
            train_ds,
            num_replicas=self.world_size,
            rank=self.rank,
            shuffle=True
        )

        train_batch_size = train_cfg.get("batch_size", 1)
        train_num_workers = train_cfg.get("num_workers", 2)
        train_loader = DataLoader(
            train_ds,
            batch_size=train_batch_size,
            sampler=train_sampler,
            collate_fn=collate_finetune_fn,
            num_workers=train_num_workers,
            pin_memory=True,
            drop_last=False,
        )

        valid_batch_size = train_cfg.get("valid_batch_size", 4)
        valid_num_workers = train_cfg.get("valid_num_workers", 2)
        valid_loader = DataLoader(
            valid_ds,
            batch_size=valid_batch_size,
            shuffle=False,
            collate_fn=collate_finetune_fn,
            num_workers=valid_num_workers,
            pin_memory=True,
            drop_last=False,
        )

        steps_per_epoch = len(train_loader)
        total_update_steps = steps_per_epoch * train_cfg.epochs

        # å…ˆå‰µå»º optimizer å’Œ scheduler
        self._setup_optimizer_and_scheduler(num_training_steps=total_update_steps)

        # å¦‚æœæœ‰ checkpointï¼Œåœ¨å‰µå»º optimizer å¾Œè¼‰å…¥ç‹€æ…‹
        if resume_checkpoint:
            loaded_epoch = self._load_checkpoint_states(resume_checkpoint)
            if loaded_epoch > 0:
                start_epoch = loaded_epoch
                # é‡æ–°è¨ˆç®—å‰©é¤˜æ­¥æ•¸ä¸¦æ›´æ–° scheduler
                remaining_epochs = train_cfg.epochs - start_epoch
                remaining_steps = steps_per_epoch * remaining_epochs
                if self.is_main_process:
                    logger.info(f"Remaining training steps: {remaining_steps}")

        if self.is_main_process:
            if start_epoch > 0:
                logger.info(f"ğŸ”„ Resuming DDP training from epoch {start_epoch + 1}")
            else:
                logger.info(f"Starting DDP training for {train_cfg.epochs} epochs")
            logger.info(f"Steps per epoch (per GPU): {steps_per_epoch}")
            logger.info(f"Total samples: {len(train_ds)}")
            logger.info(f"Total update steps (per GPU): {total_update_steps}")

        text_weight = train_cfg.text_weight

        for epoch in range(start_epoch, train_cfg.epochs):
            # è¨­å®š epoch ä»¥ç¢ºä¿ shuffle æ­£ç¢º
            train_sampler.set_epoch(epoch)

            if self.is_main_process:
                logger.info(f"=" * 60)
                logger.info(f"EPOCH {epoch + 1}/{train_cfg.epochs} started")
                logger.info(f"=" * 60)

            self.model.train()

            for batch_idx, batch in enumerate(train_loader):
                # å°‡è³‡æ–™ç§»åˆ°å°æ‡‰çš„ GPU
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, mel_accuracy = self._train_step(tuple(data_batch))
                acc_1, acc_10, acc_20 = mel_accuracy["acc_1"], mel_accuracy["acc_10"], mel_accuracy["acc_20"]

                weighted_loss = text_weight * loss_text + (1.0 - text_weight) * loss_mel

                if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                    if self.is_main_process:
                        logger.warning(f"NaN/Inf loss at epoch {epoch}, batch {batch_idx}. Skipping.")
                    continue

                # æœ€ä½³åŒ–æ­¥é©Ÿ
                self.optimizer.zero_grad()
                weighted_loss.backward()
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg.max_grad_norm)
                self.optimizer.step()
                self.scheduler.step()
                self.update_steps += 1

                # åªæœ‰ä¸»é€²ç¨‹æ‰“å°æ—¥èªŒ
                if self.is_main_process and batch_idx % 10 == 0:  # æ¯ 10 batch æ‰“å°ä¸€æ¬¡
                    logger.info(
                        f"[GPU 0/{self.world_size}] Epoch {epoch + 1}/{train_cfg.epochs} | "
                        f"Batch {batch_idx}/{steps_per_epoch} | "
                        f"text_loss={loss_text.item():.4f}, mel_loss={loss_mel.item():.4f}, "
                        f"acc@1={acc_1:.2f}%, acc@10={acc_10:.2f}%, acc@20={acc_20:.2f}%, "
                        f"grad_norm={grad_norm.item():.2f}"
                    )

            # é©—è­‰ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰
            if self.is_main_process:
                val_text_loss, val_mel_loss, _, _, _ = self._validate_epoch(valid_loader, epoch)
                self._save_checkpoint(epoch, val_text_loss, val_mel_loss)

            # åŒæ­¥æ‰€æœ‰é€²ç¨‹
            dist.barrier()

    def _train_step(self, batch: Tuple) -> Tuple[torch.Tensor, torch.Tensor, dict]:
        """å–®å€‹è¨“ç·´æ­¥é©Ÿ"""
        # è§£åŒ… batch
        mel_spec, mel_codes, text_ids, conditions, speaker_ids, mel_lengths, codes_lengths, text_lengths = batch
        batch_speaker_ids = list(speaker_ids)

        # å‰å‘å‚³æ’­
        outputs = forward_UnifiedVoice(
            self.model.module,  # DDP éœ€è¦ä½¿ç”¨ .module
            mel_spec,
            mel_codes,
            text_ids,
            mel_lengths,
            codes_lengths,
            text_lengths,
            speaker_ids=batch_speaker_ids,
            add_mel_stop_token=self.config.train.get('add_mel_stop_token', True),
            output_loss=True,
            output_logits=True,
        )

        loss_text, loss_mel = outputs["loss"]
        mel_accuracy = outputs["mel_accuracy"]

        return loss_text, loss_mel, mel_accuracy

    def _validate_epoch(self, valid_ds: Dataset, epoch: int):
        """é©—è­‰ï¼ˆç°¡åŒ–ç‰ˆï¼Œåªåœ¨ä¸»é€²ç¨‹åŸ·è¡Œï¼‰"""
        self.model.eval()
        total_text_loss = 0.0
        total_mel_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for batch in valid_ds:
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, _ = self._train_step(tuple(data_batch))
                total_text_loss += loss_text.item()
                total_mel_loss += loss_mel.item()
                num_batches += 1

        avg_text_loss = total_text_loss / max(num_batches, 1)
        avg_mel_loss = total_mel_loss / max(num_batches, 1)

        logger.info(f"Validation Epoch {epoch + 1}: text_loss={avg_text_loss:.4f}, mel_loss={avg_mel_loss:.4f}")

        return avg_text_loss, avg_mel_loss, 0.0, 0.0, 0.0

    def _load_checkpoint_states(self, checkpoint_path: str) -> int:
        """
        å¾ checkpoint æ¢å¾©è¨“ç·´

        Args:
            checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘

        Returns:
            start_epoch: è¦å¾å“ªå€‹ epoch é–‹å§‹ç¹¼çºŒè¨“ç·´
        """
        if not os.path.exists(checkpoint_path):
            if self.is_main_process:
                logger.error(f"âŒ Checkpoint not found: {checkpoint_path}")
            return 0

        if self.is_main_process:
            logger.info(f"ğŸ“‚ Loading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # è¼‰å…¥æ¨¡å‹æ¬Šé‡
        cleaned_state = normalize_state_dict_keys(checkpoint['model_state_dict'])
        self.model.module.load_state_dict(cleaned_state)
        if self.is_main_process:
            logger.info("âœ“ Model state loaded")

        # è¼‰å…¥ optimizer å’Œ schedulerï¼ˆå¦‚æœå·²ç¶“åˆå§‹åŒ–ï¼‰
        if hasattr(self, 'optimizer') and self.optimizer is not None:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if self.is_main_process:
                logger.info("âœ“ Optimizer state loaded")

        if hasattr(self, 'scheduler') and self.scheduler is not None:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if self.is_main_process:
                logger.info("âœ“ Scheduler state loaded")

        start_epoch = checkpoint['epoch'] + 1  # å¾ä¸‹ä¸€å€‹ epoch é–‹å§‹
        if self.is_main_process:
            logger.info(f"âœ“ Resuming from epoch {start_epoch}")
            logger.info(f"   Last val_text_loss: {checkpoint.get('val_text_loss', 'N/A'):.4f}")
            logger.info(f"   Last val_mel_loss: {checkpoint.get('val_mel_loss', 'N/A'):.4f}")

        return start_epoch

    def _save_checkpoint(self, epoch: int, val_text_loss: float, val_mel_loss: float):
        """å„²å­˜ checkpointï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        # 1. å…ˆå„²å­˜è¨“ç·´ç”¨çš„ checkpointï¼ˆåŒ…å« optimizer ç­‰ï¼Œç”¨æ–¼æ¢å¾©è¨“ç·´ï¼‰
        checkpoint_path = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pt")
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.module.state_dict(),  # DDP éœ€è¦ä½¿ç”¨ .module
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_text_loss': val_text_loss,
            'val_mel_loss': val_mel_loss,
        }, checkpoint_path)
        logger.info(f"ğŸ’¾ Training checkpoint saved: {checkpoint_path}")

        # 2. å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹ï¼ˆç”¨æ–¼æ¨ç†ï¼‰
        merged_model_path = os.path.join(self.checkpoint_dir, f"gpt_epoch_{epoch + 1}.pth")
        logger.info("ğŸ”„ Merging LoRA weights for inference model...")

        # ç²å–å¯¦éš›æ¨¡å‹ï¼ˆDDP wrapperï¼‰
        actual_model = self.model.module

        # å»ºç«‹æ·±è¤‡è£½ä»¥é¿å…å½±éŸ¿ç¹¼çºŒè¨“ç·´
        import copy
        logger.info("Creating a deep copy of the model for merge...")
        model_to_save = copy.deepcopy(actual_model)

        # åœ¨æ·±è¤‡è£½ä¸Šé€²è¡Œ LoRA èåˆèˆ‡è§£é™¤å®‰è£
        fused_inference_model = model_to_save.inference_model.merge_and_unload()
        model_to_save.inference_model = fused_inference_model
        logger.info("âœ“ LoRA weights merged and unloaded")

        # å„²å­˜å®Œæ•´æ¨¡å‹ï¼ˆæ ¼å¼èˆ‡ train.py ä¸€è‡´ï¼‰
        state_dict = model_to_save.state_dict()
        checkpoint_data = {
            'model': state_dict,
            'speakers': self.speaker_list,
            'speaker_conditions': {speaker_id: param.detach().cpu().numpy()
                                 for speaker_id, param in self.speaker_mean_conditions.items()}
        }

        torch.save(checkpoint_data, merged_model_path)
        logger.info(f"ğŸ’¾ Merged model saved: {merged_model_path}")
        logger.info(f"   Saved conditions for speakers: {self.speaker_list}")

        # æ¸…ç†æ·±è¤‡è£½
        del model_to_save
        torch.cuda.empty_cache()
        logger.info("âœ“ Cleaned up temporary merged model")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    parser = argparse.ArgumentParser(description='DDP Training with resume support')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from (e.g., finetune_models/checkpoints/checkpoint_epoch_5.pt)')
    args = parser.parse_args()

    # ç²å– DDP ç’°å¢ƒè®Šæ•¸
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("âŒ è«‹ä½¿ç”¨ torch.distributed.launch æˆ– torchrun å•Ÿå‹•æ­¤è…³æœ¬")
        print("ç¯„ä¾‹: torchrun --nproc_per_node=8 train_ddp.py")
        print("æ¢å¾©è¨“ç·´: torchrun --nproc_per_node=8 train_ddp.py --resume finetune_models/checkpoints/checkpoint_epoch_5.pt")
        return

    # åˆå§‹åŒ– DDP
    setup_ddp(local_rank, world_size)

    try:
        # è¼‰å…¥é…ç½®
        config = OmegaConf.load("finetune_models/config.yaml")

        # å‰µå»ºè¨“ç·´å™¨
        trainer = DDPTrainer(config, local_rank, world_size)

        # è¼‰å…¥è³‡æ–™é›†
        bpe_model_path = os.path.join(
            config.train.finetune_model_dir,
            config.dataset.bpe_model
        )
        train_ds, valid_ds = load_finetune_datasets(config, bpe_model_path)

        # é–‹å§‹è¨“ç·´ï¼ˆresume_checkpoint æœƒåœ¨ train() å…§éƒ¨è™•ç†ï¼‰
        trainer.train(train_ds, valid_ds, resume_checkpoint=args.resume)

    finally:
        # æ¸…ç†
        cleanup_ddp()


if __name__ == "__main__":
    main()
