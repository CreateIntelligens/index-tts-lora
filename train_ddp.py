#!/usr/bin/env python3
"""
DistributedDataParallel (DDP) è¨“ç·´è…³æœ¬ - å¤š GPU é«˜æ•ˆè¨“ç·´

ç›¸æ¯” DataParallel:
- æ›´é«˜çš„è¨“ç·´æ•ˆç‡ï¼ˆç„¡ä¸» GPU ç“¶é ¸ï¼‰
- æ›´å¥½çš„è¨˜æ†¶é«”åˆ©ç”¨
- å¯ä»¥è·¨ç¯€é»æ“´å±•

ä½¿ç”¨æ–¹æ³•:
    # å–®æ©Ÿå¤š GPUï¼ˆæ¨è–¦ï¼‰
    python -m torch.distributed.launch --nproc_per_node=8 train_ddp.py

    # æˆ–ä½¿ç”¨ torchrunï¼ˆPyTorch 1.10+ï¼‰
    torchrun --nproc_per_node=8 train_ddp.py

    # æŒ‡å®šç‰¹å®š GPU
    CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_ddp.py

Author: TTS ETL Pipeline
Version: 1.0
"""

import copy
import gc
import os
import random
import datetime
from datetime import datetime as dt
from typing import List, Optional, Tuple

import numpy as np
import sentencepiece as spm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from peft import LoraConfig, TaskType, get_peft_model
from peft.optimizers import create_loraplus_optimizer
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import get_cosine_schedule_with_warmup
from torch.utils.tensorboard import SummaryWriter

from indextts.BigVGAN.models import BigVGAN
from indextts.data_utils import (
    collate_finetune_fn,
    load_finetune_datasets,
)
from indextts.gpt.model import UnifiedVoice

# Import train utilities
from train import (
    load_UnifiedVoice,
    normalize_state_dict_keys,
    clear_torch_cache,
    forward_gpt2,
    forward_UnifiedVoice,
    top_k_accuracy,
)


def setup_ddp(rank: int, world_size: int):
    """åˆå§‹åŒ– DDP ç’°å¢ƒ"""
    # è¨­å®šé è¨­å€¼ï¼ˆå¦‚æœ torchrun æ²’æœ‰è¨­å®šï¼‰
    if 'MASTER_ADDR' not in os.environ:
        os.environ['MASTER_ADDR'] = 'localhost'
    if 'MASTER_PORT' not in os.environ:
        os.environ['MASTER_PORT'] = '12355'

    # Fix for NCCL connection issues
    # é è¨­ç¦ç”¨ P2P å’Œ IB ä»¥é˜²æ­¢åœ¨æŸäº›ç’°å¢ƒï¼ˆå¦‚ Docker æˆ–æ¶ˆè²»ç´š GPUï¼‰ä¸­å‡ºç¾é€£ç·šé€¾æ™‚æˆ–æ›æ­»
    if 'NCCL_P2P_DISABLE' not in os.environ:
        os.environ['NCCL_P2P_DISABLE'] = '1'
    if 'NCCL_IB_DISABLE' not in os.environ:
        os.environ['NCCL_IB_DISABLE'] = '1'
    if 'NCCL_NET' not in os.environ:
        os.environ['NCCL_NET'] = 'Socket'

    # åˆå§‹åŒ–é€²ç¨‹çµ„
    # è¨­å®š 30 åˆ†é˜çš„è¶…æ™‚ï¼Œé˜²æ­¢ä¸»é€²ç¨‹ä¿å­˜æ¨¡å‹æ™‚å…¶ä»–é€²ç¨‹è¶…æ™‚
    dist.init_process_group("nccl", rank=rank, world_size=world_size, timeout=datetime.timedelta(minutes=30))
    torch.cuda.set_device(rank)


def cleanup_ddp():
    """æ¸…ç† DDP ç’°å¢ƒ"""
    dist.destroy_process_group()


class DDPTrainer:
    """DDP è¨“ç·´å™¨ - é«˜æ•ˆå¤š GPU è¨“ç·´"""

    def __init__(self, config: DictConfig, rank: int, world_size: int):
        """
        åˆå§‹åŒ– DDP è¨“ç·´å™¨

        Args:
            config: é…ç½®ç‰©ä»¶
            rank: ç•¶å‰é€²ç¨‹çš„ GPU ID (0-7)
            world_size: ç¸½ GPU æ•¸é‡ (8)
        """
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank}")
        self.is_main_process = (rank == 0)

        # åªæœ‰ä¸»é€²ç¨‹æ‰“å°è¨Šæ¯
        # if not self.is_main_process:
        #    logger.remove()  # ç§»é™¤å…¶ä»–é€²ç¨‹çš„æ—¥èªŒè¼¸å‡º

        # è¨­å®šéš¨æ©Ÿç¨®å­
        self._set_seed(self.config.train.seed + rank)  # æ¯å€‹ rank ä¸åŒçš„ç¨®å­

        # æº–å‚™ç›®éŒ„å’Œæ—¥èªŒ
        self.finetune_dir = self.config.train.finetune_model_dir
        self.checkpoint_dir = os.path.join(self.finetune_dir, "checkpoints")
        # ç‚ºæ–‡å­— log èˆ‡ TensorBoard çµ±ä¸€ä½¿ç”¨åŒä¸€å€‹ run åç¨±/ç›®éŒ„ï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘é¿å… cwd è®Šå‹•ï¼‰
        # è‹¥å¤–éƒ¨ï¼ˆrun.shï¼‰å·²æŒ‡å®š RUN_NAME/RUN_LOG_DIRï¼Œå‰‡æ²¿ç”¨åŒä¸€å€‹åå­—èˆ‡è·¯å¾‘ï¼Œé¿å…ç”¢ç”Ÿå¤šå€‹ç›®éŒ„
        env_run_name = os.environ.get("RUN_NAME")
        env_log_dir = os.environ.get("RUN_LOG_DIR")
        if env_run_name:
            self.run_name = env_run_name
        else:
            self.run_name = f"train_{dt.now().strftime('%Y%m%d_%H%M%S')}"

        if env_log_dir:
            self.log_dir = os.path.abspath(env_log_dir)
        else:
            self.log_dir = os.path.abspath(os.path.join(os.getcwd(), "logs", self.run_name))

        if self.is_main_process:
            os.makedirs(self.checkpoint_dir, exist_ok=True)
            os.makedirs(self.log_dir, exist_ok=True)
            # æ–‡å­— log èˆ‡ TensorBoard æ”¾åœ¨åŒä¸€ç›®éŒ„ï¼Œæ–¹ä¾¿å°é½Šæ™‚é–“æˆ³
            log_path = os.path.join(self.log_dir, "train.log")
            self._setup_logging(log_path)
            # Initialize TensorBoard Writer (only on main process)
            self.writer = SummaryWriter(log_dir=self.log_dir)
            logger.info(f"TensorBoard logging to: {self.log_dir}")

        # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
        self._load_models()

        # åˆå§‹åŒ–è¨“ç·´ç‹€æ…‹ï¼ˆoptimizer æœƒåœ¨ train() è£¡è¨­å®šï¼‰
        self.optimizer = None
        self.scheduler = None
        self.best_val_loss = (0, float('inf'), float('inf'))
        self.update_steps = 0

    def _set_seed(self, seed: int):
        """è¨­å®šéš¨æ©Ÿç¨®å­"""
        random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        if self.is_main_process:
            logger.info(f"Set random seed to {seed} for rank {self.rank}")

    def _setup_logging(self, log_path: str):
        """é…ç½®æ—¥èªŒè¨˜éŒ„å™¨ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        logger.add(log_path, level="INFO", encoding="utf-8")
        logger.info(f"ğŸš€ DDP Training with {self.world_size} GPUs")
        logger.info("Full configuration:\n" + OmegaConf.to_yaml(self.config))

    def _load_models(self):
        """è¼‰å…¥æ¨¡å‹"""
        if self.is_main_process:
            logger.info("Loading models...")

        # BPE
        bpe_model_path = os.path.join(self.finetune_dir, self.config.dataset.bpe_model)
        self.bpe_model = spm.SentencePieceProcessor(bpe_model_path)

        # UnifiedVoice
        gpt_checkpoint_path = os.path.join(self.finetune_dir, self.config.gpt_checkpoint)
        self.model = load_UnifiedVoice(self.config.gpt, gpt_checkpoint_path, self.device)

        # æ‡‰ç”¨ LoRA
        self.model = self._apply_lora(self.model)

        # ä½¿ç”¨ DDP åŒ…è£æ¨¡å‹
        self.model = DDP(
            self.model,
            device_ids=[self.rank],
            output_device=self.rank,
            # conditioning/perceiver å¯èƒ½åœ¨ç‰¹å®šåˆ†æ”¯æœªè¢«ä½¿ç”¨ï¼Œä¿æŒ True é¿å…æ¢¯åº¦åŒæ­¥æ­»é–
            find_unused_parameters=True
        )

        if self.is_main_process:
            logger.info(f"âœ… Model wrapped with DDP on {self.world_size} GPUs")

    def _apply_lora(self, model: UnifiedVoice) -> UnifiedVoice:
        """æ‡‰ç”¨ LoRA é…ç½®"""
        lora_cfg = self.config.train.lora
        gpt_lora_config = LoraConfig(
            r=lora_cfg.r,
            target_modules=lora_cfg.target_modules,
            task_type=TaskType.CAUSAL_LM,
            lora_alpha=lora_cfg.lora_alpha,
            lora_dropout=lora_cfg.lora_dropout,
            bias="none",
            fan_in_fan_out=True,
        )
        model.requires_grad_(False)
        model.inference_model = get_peft_model(model.inference_model, gpt_lora_config)

        # âš ï¸ é‡è¦ï¼šå‡çµ conditioning_encoder å’Œ perceiver_encoder
        #
        # ç­–ç•¥èªªæ˜ï¼š
        # - conditioning_encoder: å‡çµï¼ˆé˜²æ­¢ç·¨ç¢¼å…§å®¹è³‡è¨Šï¼Œä¿ç•™é è¨“ç·´çš„éŸ³è‰²æå–èƒ½åŠ›ï¼‰
        # - perceiver_encoder: å‡çµï¼ˆä¿ç•™é è¨“ç·´çš„ embedding spaceï¼Œé˜²æ­¢è¢«è¦†å¯«ï¼‰
        # - GPT LoRA: è¨“ç·´ï¼ˆå­¸ç¿’å¦‚ä½•ä½¿ç”¨ conditioningï¼‰
        #
        # ç†ç”±ï¼š
        # 1. é è¨“ç·´çš„ encoder å·²å…·å‚™é€šç”¨éŸ³è‰²æå–èƒ½åŠ›
        # 2. è¨“ç·´ perceiver æœƒç ´å£åŸæœ¬çš„ embedding spaceï¼Œå°è‡´ clone èƒ½åŠ›å´©æ½°
        # 3. åƒ…è¨“ç·´ LoRA å±¤ï¼Œè®“æ¨¡å‹å­¸ç¿’å¦‚ä½•ä½¿ç”¨å›ºå®šçš„ conditioning ä¾†ç”ŸæˆèªéŸ³
        if hasattr(model, 'conditioning_encoder'):
            model.conditioning_encoder.requires_grad_(False)
            if self.is_main_process:
                logger.info("âœ“ conditioning_encoder å·²å‡çµï¼ˆé˜²æ­¢å…§å®¹æ´©æ¼ï¼‰")
        if hasattr(model, 'perceiver_encoder'):
            model.perceiver_encoder.requires_grad_(False)
            if self.is_main_process:
                logger.info("âœ“ perceiver_encoder å·²å‡çµï¼ˆä¿ç•™ embedding spaceï¼‰")

        return model

    def _setup_optimizer_and_scheduler(self, num_training_steps: int = 0):
        """è¨­å®šæœ€ä½³åŒ–å™¨å’Œæ’ç¨‹å™¨"""
        opt_cfg = self.config.train.optimizer
        self.optimizer = create_loraplus_optimizer(
            model=self.model,
            optimizer_cls=AdamW,
            lr=opt_cfg.learning_rate,
            loraplus_lr_ratio=opt_cfg.loraplus_lr_ratio,
            loraplus_lr_embedding=opt_cfg.get("loraplus_lr_embedding", 1e-6),
            weight_decay=opt_cfg.weight_decay,
        )

        warmup_steps = opt_cfg.get("warmup_steps", None)
        if warmup_steps is None:
            warmup_ratio = opt_cfg.get("warmup_ratio", 0.0)
            if warmup_ratio > 0 and num_training_steps > 0:
                warmup_steps = max(1, int(num_training_steps * warmup_ratio))
            else:
                warmup_steps = 0

        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=max(num_training_steps, warmup_steps + 1),
        )

    def train(self, train_ds: Dataset, valid_ds: Dataset, resume_checkpoint: str = None):
        """è¨“ç·´æµç¨‹"""
        train_cfg = self.config.train
        start_epoch = 0

        # å˜—è©¦å–å¾—åŠ æ¬Šå–æ¨£å™¨ï¼ˆè‹¥è³‡æ–™é›†æœ‰å¯¦ä½œï¼‰
        if hasattr(train_ds, 'get_sampler'):
            train_sampler = train_ds.get_sampler(
                shuffle=True,
                num_replicas=self.world_size,
                rank=self.rank
            )
        else:
            train_sampler = None

        # è‹¥æ²’æœ‰åŠ æ¬Šå–æ¨£å™¨ï¼Œä½¿ç”¨æ¨™æº– DistributedSampler
        if train_sampler is None:
            train_sampler = DistributedSampler(
                train_ds,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
        elif self.is_main_process:
            logger.info("âœ“ ä½¿ç”¨åŠ æ¬Šå–æ¨£ç­–ç•¥ (Weighted Sampling)")

        train_batch_size = train_cfg.get("batch_size", 1)
        train_num_workers = train_cfg.get("num_workers", 2)
        train_loader = DataLoader(
            train_ds,
            batch_size=train_batch_size,
            sampler=train_sampler,
            collate_fn=collate_finetune_fn,
            num_workers=train_num_workers,
            pin_memory=True,
            drop_last=True,  # é¿å… DDP æœ«å°¾ batch ä¸é½Šå¡ä½
        )

        valid_batch_size = train_cfg.get("valid_batch_size", 4)
        valid_num_workers = train_cfg.get("valid_num_workers", 2)
        valid_loader = DataLoader(
            valid_ds,
            batch_size=valid_batch_size,
            shuffle=False,
            collate_fn=collate_finetune_fn,
            num_workers=valid_num_workers,
            pin_memory=True,
            drop_last=True,  # é©—è­‰ä¹Ÿå°é½Š batch é•·åº¦
        )

        steps_per_epoch = len(train_loader)
        total_update_steps = steps_per_epoch * train_cfg.epochs

        # å…ˆå‰µå»º optimizer å’Œ scheduler
        self._setup_optimizer_and_scheduler(num_training_steps=total_update_steps)

        # å¦‚æœæœ‰ checkpointï¼Œåœ¨å‰µå»º optimizer å¾Œè¼‰å…¥ç‹€æ…‹
        if resume_checkpoint:
            loaded_epoch = self._load_checkpoint_states(resume_checkpoint)
            if loaded_epoch > 0:
                start_epoch = loaded_epoch
                # æ›´æ–° update_steps
                self.update_steps = start_epoch * steps_per_epoch
                
                # é‡æ–°è¨ˆç®—å‰©é¤˜æ­¥æ•¸ä¸¦æ›´æ–° scheduler
                remaining_epochs = train_cfg.epochs - start_epoch
                remaining_steps = steps_per_epoch * remaining_epochs
                if self.is_main_process:
                    logger.info(f"Updated global step to {self.update_steps}")
                    logger.info(f"Remaining training steps: {remaining_steps}")

        if self.is_main_process:
            if start_epoch > 0:
                logger.info(f"ğŸ”„ Resuming DDP training from epoch {start_epoch + 1}")
            else:
                logger.info(f"Starting DDP training for {train_cfg.epochs} epochs")
            logger.info(f"Steps per epoch (per GPU): {steps_per_epoch}")
            logger.info(f"Total samples: {len(train_ds)}")
            logger.info(f"Total update steps (per GPU): {total_update_steps}")

        text_weight = train_cfg.text_weight

        for epoch in range(start_epoch, train_cfg.epochs):
            # è¨­å®š epoch ä»¥ç¢ºä¿ shuffle æ­£ç¢º
            train_sampler.set_epoch(epoch)

            if self.is_main_process:
                logger.info(f"=" * 60)
                logger.info(f"EPOCH {epoch + 1}/{train_cfg.epochs} started")
                logger.info(f"=" * 60)

            self.model.train()

            for batch_idx, batch in enumerate(train_loader):
                # å°‡è³‡æ–™ç§»åˆ°å°æ‡‰çš„ GPU
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, mel_accuracy = self._train_step(tuple(data_batch))
                acc_1, acc_10, acc_20 = mel_accuracy["acc_1"], mel_accuracy["acc_10"], mel_accuracy["acc_20"]

                weighted_loss = text_weight * loss_text + (1.0 - text_weight) * loss_mel

                if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                    if self.is_main_process:
                        logger.warning(f"NaN/Inf loss at epoch {epoch}, batch {batch_idx}. Zeroing loss to maintain DDP sync.")
                    # DDP CRITICAL FIX: ä¸èƒ½ç›´æ¥ continueï¼Œå¦å‰‡æœƒå°è‡´è©² Rank è·³é backwardï¼Œé€ æˆå…¨é«”æ­»é–ã€‚
                    # å¿…é ˆå‚³é€ä¸€å€‹ 0 çš„ loss è®“ DDP å®Œæˆæ¢¯åº¦åŒæ­¥ã€‚
                    weighted_loss = torch.tensor(0.0, device=self.device, requires_grad=True)

                # æœ€ä½³åŒ–æ­¥é©Ÿ
                self.optimizer.zero_grad()
                weighted_loss.backward()
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg.max_grad_norm)
                self.optimizer.step()
                self.scheduler.step()
                self.update_steps += 1

                # åªæœ‰ä¸»é€²ç¨‹æ‰“å°æ—¥èªŒ
                if self.is_main_process and batch_idx % 10 == 0:  # æ¯ 10 batch æ‰“å°ä¸€æ¬¡
                    logger.info(
                        f"[GPU 0/{self.world_size}] Epoch {epoch + 1}/{train_cfg.epochs} | "
                        f"Batch {batch_idx}/{steps_per_epoch} | "
                        f"text_loss={loss_text.item():.4f}, mel_loss={loss_mel.item():.4f}, "
                        f"acc@1={acc_1:.2f}%, acc@10={acc_10:.2f}%, acc@20={acc_20:.2f}%, "
                        f"grad_norm={grad_norm.item():.2f}"
                    )
                    
                    # TensorBoard Logging
                    self.writer.add_scalar("loss/text", loss_text.item(), self.update_steps)
                    self.writer.add_scalar("loss/mel", loss_mel.item(), self.update_steps)
                    self.writer.add_scalar("loss/total", weighted_loss.item(), self.update_steps)
                    self.writer.add_scalar("accuracy/top1", acc_1, self.update_steps)
                    self.writer.add_scalar("accuracy/top10", acc_10, self.update_steps)
                    self.writer.add_scalar("accuracy/top20", acc_20, self.update_steps)
                    self.writer.add_scalar("train/grad_norm", grad_norm.item(), self.update_steps)
                    self.writer.add_scalar("train/lr", self.scheduler.get_last_lr()[0], self.update_steps)

            # é©—è­‰ï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰
            if self.is_main_process:
                val_text_loss, val_mel_loss, val_acc1, val_acc10, val_acc20 = self._validate_epoch(valid_loader, epoch)
                self._save_checkpoint(epoch, val_text_loss, val_mel_loss)
                
                # TensorBoard Validation Logging
                self.writer.add_scalar("val/loss_text", val_text_loss, epoch + 1)
                self.writer.add_scalar("val/loss_mel", val_mel_loss, epoch + 1)
                self.writer.add_scalar("val/accuracy_top1", val_acc1, epoch + 1)
                self.writer.add_scalar("val/accuracy_top10", val_acc10, epoch + 1)
                self.writer.add_scalar("val/accuracy_top20", val_acc20, epoch + 1)

            # åŒæ­¥æ‰€æœ‰é€²ç¨‹
            # print(f"[Rank {self.rank}] Waiting at barrier...", flush=True)
            # dist.barrier(device_ids=[self.rank])
            # print(f"[Rank {self.rank}] Passed barrier!", flush=True)

    def _train_step(self, batch: Tuple) -> Tuple[torch.Tensor, torch.Tensor, dict]:
        """å–®å€‹è¨“ç·´æ­¥é©Ÿï¼šconditioning ä¾†è‡ªåŒ speaker çš„å¦ä¸€æ®µèªéŸ³"""
        # è§£åŒ… batchï¼ˆæ–°å¢ cond_melsã€cond_lengthsï¼‰
        mel_spec, mel_codes, text_ids, cond_mels, speaker_ids, mel_lengths, codes_lengths, text_lengths, cond_lengths = batch

        outputs = forward_UnifiedVoice(
            self.model.module,  # DDP éœ€è¦ä½¿ç”¨ .module
            mel_spec,           # target melï¼ˆç”¨æ–¼ lossï¼‰
            mel_codes,
            text_ids,
            mel_lengths,
            codes_lengths,
            text_lengths,
            condition_mels=cond_mels,
            condition_lengths=cond_lengths,
            speaker_ids=None,  # ä¸ç”¨ speaker_id æŸ¥è¡¨ï¼Œå¼·åˆ¶ encoder å­¸ç¿’
            add_mel_stop_token=self.config.train.get('add_mel_stop_token', True),
            output_loss=True,
            output_logits=True,
        )

        loss_text, loss_mel = outputs["loss"]
        mel_accuracy = outputs["mel_accuracy"]

        return loss_text, loss_mel, mel_accuracy

    def _forward_with_precomputed_conditioning(self, *args, **kwargs):
        """Deprecated after dynamic conditioning change."""
        raise NotImplementedError('Use _train_step with dynamic conditioning instead.')

    def _validate_epoch(self, valid_ds: Dataset, epoch: int):
        """é©—è­‰ï¼ˆç°¡åŒ–ç‰ˆï¼Œåªåœ¨ä¸»é€²ç¨‹åŸ·è¡Œï¼‰"""
        self.model.eval()
        total_text_loss = 0.0
        total_mel_loss = 0.0
        num_batches = 0
        total_batches = len(valid_ds)

        logger.info(f"é–‹å§‹é©—è­‰ Epoch {epoch + 1}ï¼Œå…± {total_batches} å€‹ batch")

        with torch.no_grad():
            for batch_idx, batch in enumerate(valid_ds):
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, _ = self._train_step(tuple(data_batch))
                total_text_loss += loss_text.item()
                total_mel_loss += loss_mel.item()
                num_batches += 1

                # æ¯ 50 å€‹ batch æ‰“å°ä¸€æ¬¡é€²åº¦
                if batch_idx % 50 == 0:
                    logger.info(
                        f"Validation [{batch_idx}/{total_batches}] | "
                        f"val_txt={loss_text.item():.3f}, val_mel={loss_mel.item():.3f}"
                    )

        avg_text_loss = total_text_loss / max(num_batches, 1)
        avg_mel_loss = total_mel_loss / max(num_batches, 1)
        
        # è¨ˆç®—æ•´é«”æº–ç¢ºç‡ (ç°¡å–®å¹³å‡ï¼Œé›–ç„¶ä¸å®Œå…¨ç²¾ç¢ºä½†å°æ–¼ç›£æ§è¶³å¤ )
        # åœ¨ DDP é©—è­‰ä¸­ï¼Œç”±æ–¼æˆ‘å€‘æ²’æœ‰ gather æ‰€æœ‰çš„ logitsï¼Œé€™è£¡åªè¨ˆç®—ä¸» GPU çš„æº–ç¢ºç‡
        # è‹¥è¦ç²¾ç¢ºï¼Œéœ€è¦ dist.all_gather
        acc_1 = 0.0
        acc_10 = 0.0
        acc_20 = 0.0
        # é€™è£¡éœ€è¦è£œä¸Šæº–ç¢ºç‡è¨ˆç®—é‚è¼¯ï¼Œä½†å› ç‚ºåŸæœ¬ç¨‹å¼ç¢¼æ²’æœ‰æ”¶é›† logitsï¼Œæš«æ™‚å›å‚³ 0
        # è‹¥è¦å¯¦ä½œï¼Œéœ€è¦åƒ train.py ä¸€æ¨£æ”¶é›† logits
        
        logger.info(f"Validation Epoch {epoch + 1}: text_loss={avg_text_loss:.4f}, mel_loss={avg_mel_loss:.4f}")

        return avg_text_loss, avg_mel_loss, acc_1, acc_10, acc_20

    def _load_checkpoint_states(self, checkpoint_path: str) -> int:
        """
        å¾ checkpoint æ¢å¾©è¨“ç·´

        Args:
            checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘

        Returns:
            start_epoch: è¦å¾å“ªå€‹ epoch é–‹å§‹ç¹¼çºŒè¨“ç·´
        """
        if not os.path.exists(checkpoint_path):
            if self.is_main_process:
                logger.error(f"âŒ Checkpoint not found: {checkpoint_path}")
            return 0

        if self.is_main_process:
            logger.info(f"ğŸ“‚ Loading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        # è¼‰å…¥æ¨¡å‹æ¬Šé‡
        cleaned_state = normalize_state_dict_keys(checkpoint['model_state_dict'])
        self.model.module.load_state_dict(cleaned_state)
        if self.is_main_process:
            logger.info("âœ“ Model state loaded")

        # è¼‰å…¥ optimizer å’Œ schedulerï¼ˆå¦‚æœå·²ç¶“åˆå§‹åŒ–ï¼‰
        if hasattr(self, 'optimizer') and self.optimizer is not None:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if self.is_main_process:
                logger.info("âœ“ Optimizer state loaded")

        if hasattr(self, 'scheduler') and self.scheduler is not None:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if self.is_main_process:
                logger.info("âœ“ Scheduler state loaded")

        start_epoch = checkpoint['epoch'] + 1  # å¾ä¸‹ä¸€å€‹ epoch é–‹å§‹
        if self.is_main_process:
            logger.info(f"âœ“ Resuming from epoch {start_epoch}")
            logger.info(f"   Last val_text_loss: {checkpoint.get('val_text_loss', 'N/A'):.4f}")
            logger.info(f"   Last val_mel_loss: {checkpoint.get('val_mel_loss', 'N/A'):.4f}")

        return start_epoch

    def _save_checkpoint(self, epoch: int, val_text_loss: float, val_mel_loss: float):
        """å„²å­˜ checkpointï¼ˆåªåœ¨ä¸»é€²ç¨‹ï¼‰"""
        # 1. å…ˆå„²å­˜è¨“ç·´ç”¨çš„ checkpointï¼ˆåŒ…å« optimizer ç­‰ï¼Œç”¨æ–¼æ¢å¾©è¨“ç·´ï¼‰
        checkpoint_path = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pt")
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.module.state_dict(),  # DDP éœ€è¦ä½¿ç”¨ .module
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_text_loss': val_text_loss,
            'val_mel_loss': val_mel_loss,
        }, checkpoint_path)
        logger.info(f"ğŸ’¾ Training checkpoint saved: {checkpoint_path}")

        # 2. å„²å­˜åˆä½µå¾Œçš„æ¨¡å‹ï¼ˆç”¨æ–¼æ¨ç†ï¼‰
        merged_model_path = os.path.join(self.checkpoint_dir, f"gpt_epoch_{epoch + 1}.pth")
        logger.info("ğŸ”„ Merging LoRA weights for inference model...")

        # ç²å–å¯¦éš›æ¨¡å‹ï¼ˆDDP wrapperï¼‰
        actual_model = self.model.module

        # å»ºç«‹æ·±è¤‡è£½ä»¥é¿å…å½±éŸ¿ç¹¼çºŒè¨“ç·´
        import copy
        logger.info("Creating a deep copy of the model for merge...")
        model_to_save = copy.deepcopy(actual_model)

        # åœ¨æ·±è¤‡è£½ä¸Šé€²è¡Œ LoRA èåˆèˆ‡è§£é™¤å®‰è£
        fused_inference_model = model_to_save.inference_model.merge_and_unload()
        model_to_save.inference_model = fused_inference_model
        logger.info("âœ“ LoRA weights merged and unloaded")

        # é¸æ“‡å„²å­˜ç²¾åº¦ï¼ˆé è¨­ fp16ï¼Œèˆ‡åº•æ¨¡ä¸€è‡´ï¼Œé«”ç©è¼ƒå°ï¼‰
        save_dtype = self.config.train.get("save_dtype", "fp16")
        if save_dtype == "fp16":
            model_to_save = model_to_save.half()
            logger.info("ğŸ’¾ Saving merged model in FP16")
        elif save_dtype == "bf16":
            model_to_save = model_to_save.bfloat16()
            logger.info("ğŸ’¾ Saving merged model in BF16")
        else:
            logger.info("ğŸ’¾ Saving merged model in FP32")

        # å„²å­˜å®Œæ•´æ¨¡å‹ï¼ˆæ ¼å¼èˆ‡ train.py ä¸€è‡´ï¼‰
        # æ’é™¤ inference_model.* é¿å…é‡è¤‡å„²å­˜ gpt åƒæ•¸
        state_dict = model_to_save.state_dict()
        filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('inference_model')}
        checkpoint_data = {'model': filtered_state_dict}

        torch.save(checkpoint_data, merged_model_path)
        logger.info(f"ğŸ’¾ Merged model saved: {merged_model_path}")

        # æ¸…ç†æ·±è¤‡è£½
        del model_to_save
        torch.cuda.empty_cache()
        logger.info("âœ“ Cleaned up temporary merged model")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    parser = argparse.ArgumentParser(description='DDP Training with resume support')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from (e.g., finetune_models/checkpoints/checkpoint_epoch_5.pt)')
    args = parser.parse_args()

    # ç²å– DDP ç’°å¢ƒè®Šæ•¸
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    else:
        print("âŒ è«‹ä½¿ç”¨ torch.distributed.launch æˆ– torchrun å•Ÿå‹•æ­¤è…³æœ¬")
        print("ç¯„ä¾‹: torchrun --nproc_per_node=8 train_ddp.py")
        print("æ¢å¾©è¨“ç·´: torchrun --nproc_per_node=8 train_ddp.py --resume finetune_models/checkpoints/checkpoint_epoch_5.pt")
        return

    # åˆå§‹åŒ– DDP
    setup_ddp(local_rank, world_size)

    try:
        # è¼‰å…¥é…ç½®
        config = OmegaConf.load("finetune_models/config.yaml")

        # å‰µå»ºè¨“ç·´å™¨
        trainer = DDPTrainer(config, local_rank, world_size)

        # è¼‰å…¥è³‡æ–™é›†
        bpe_model_path = os.path.join(
            config.train.finetune_model_dir,
            config.dataset.bpe_model
        )
        train_ds, valid_ds = load_finetune_datasets(config, bpe_model_path)

        # é–‹å§‹è¨“ç·´ï¼ˆresume_checkpoint æœƒåœ¨ train() å…§éƒ¨è™•ç†ï¼‰
        trainer.train(train_ds, valid_ds, resume_checkpoint=args.resume)

    finally:
        # æ¸…ç†
        if hasattr(trainer, 'writer'):
            trainer.writer.close()
        cleanup_ddp()


if __name__ == "__main__":
    main()
