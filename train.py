import copy
import gc
import os
import random
from datetime import datetime
from typing import List, Optional, Tuple, Dict

import numpy as np
import sentencepiece as spm
import torch
import torch.nn as nn
import torch.nn.functional as F
from loguru import logger
from omegaconf import DictConfig, OmegaConf
from peft import LoraConfig, TaskType, get_peft_model
from peft.optimizers import create_loraplus_optimizer
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import get_cosine_schedule_with_warmup
from torch.amp import GradScaler
from torch.utils.tensorboard import SummaryWriter

from indextts.BigVGAN.models import BigVGAN
from indextts.data_utils import (
    collate_finetune_fn,
    load_finetune_datasets,
)
from indextts.gpt.model import UnifiedVoice

# å˜—è©¦è¼‰å…¥ GPU ç®¡ç†å™¨
try:
    from indextts.gpu_manager import GPUManager, get_global_gpu_manager
    GPU_MANAGER_AVAILABLE = True
except ImportError:
    GPU_MANAGER_AVAILABLE = False
    logger.warning("âš ï¸  GPU Manager æœªå®‰è£ï¼Œå°‡åœç”¨å¤š GPU æ”¯æ´")


def normalize_state_dict_keys(state_dict: dict) -> dict:
    """
    æ¨™æº–åŒ–ç‹€æ…‹å­—å…¸çš„éµå€¼åç¨±ï¼Œç§»é™¤ DataParallel/DDP ç”¢ç”Ÿçš„ `module.` å‰ç¶´ã€‚

    Args:
        state_dict (dict): åŸå§‹çš„ç‹€æ…‹å­—å…¸ã€‚

    Returns:
        dict: è™•ç†å¾Œçš„ç‹€æ…‹å­—å…¸ã€‚
    """
    if not any(key.startswith("module.") for key in state_dict.keys()):
        return state_dict
    return {key.removeprefix("module."): value for key, value in state_dict.items()}


def load_UnifiedVoice(gpt_config: DictConfig, gpt_checkpoint_path: str, device: torch.device) -> UnifiedVoice:
    """
    è¼‰å…¥ä¸¦åˆå§‹åŒ– UnifiedVoice æ¨¡å‹ã€‚

    Args:
        gpt_config (DictConfig): GPT æ¨¡å‹é…ç½®åƒæ•¸ã€‚
        gpt_checkpoint_path (str): æ¨¡å‹æ¬Šé‡æª”æ¡ˆè·¯å¾‘ã€‚
        device (torch.device): ç›®æ¨™é‹ç®—è£ç½®ã€‚

    Returns:
        UnifiedVoice: åˆå§‹åŒ–å®Œæˆçš„æ¨¡å‹å¯¦ä¾‹ã€‚
    """
    state_dict = torch.load(gpt_checkpoint_path, map_location=device, weights_only=True)
    state_dict = state_dict["model"] if "model" in state_dict else state_dict
    state_dict = normalize_state_dict_keys(state_dict)
    
    model = UnifiedVoice(**gpt_config)
    model.load_state_dict(state_dict, strict=True)
    model.post_init_gpt2_config()
    del state_dict
    return model.to(device)

def clear_torch_cache():
    """æ¸…ç† PyTorch çš„ CUDA å¿«å–è¨˜æ†¶é«”ã€‚"""
    if torch.cuda.is_available():
        gc.collect()
        torch.cuda.synchronize()
        torch.cuda.empty_cache()

def forward_gpt2(
    model: UnifiedVoice,
    inputs_embeds: torch.FloatTensor,
    text_lengths: torch.LongTensor,
    codes_lengths: torch.LongTensor,
    attention_mask: Optional[torch.Tensor] = None,
    output_latent: bool = False,
    output_logits: bool = True,
):
    """
    åŸ·è¡Œ UnifiedVoice GPT2 éƒ¨åˆ†çš„å‰å‘å‚³æ’­ã€‚
    
    æ­¤å‡½æ•¸è™•ç†è¼¸å…¥åµŒå…¥ï¼Œé€šé GPT æ¨¡å‹ï¼Œä¸¦è¨ˆç®—æ–‡å­—å’Œ Mel çš„ logitsã€‚
    
    Args:
        model (UnifiedVoice): æ¨¡å‹å¯¦ä¾‹ã€‚
        inputs_embeds (torch.FloatTensor): è¼¸å…¥åµŒå…¥å¼µé‡ã€‚
        text_lengths (torch.LongTensor): æ–‡å­—åºåˆ—é•·åº¦ã€‚
        codes_lengths (torch.LongTensor): Mel ä»£ç¢¼åºåˆ—é•·åº¦ã€‚
        attention_mask (Optional[torch.Tensor]): æ³¨æ„åŠ›é®ç½©ã€‚
        output_latent (bool): æ˜¯å¦è¼¸å‡ºéš±å‘é‡ã€‚
        output_logits (bool): æ˜¯å¦è¼¸å‡º logitsã€‚

    Returns:
        dict: åŒ…å« logits å’Œ/æˆ– latent çš„å­—å…¸ã€‚
    """
    assert attention_mask is not None, "UnifiedVoice å‰å‘å‚³æ’­å¿…é ˆæä¾› attention_maskã€‚"

    # è™•ç† DataParallel å°è£
    actual_model = model.module if isinstance(model, nn.DataParallel) else model

    b = inputs_embeds.shape[0]
    gpt_out = actual_model.gpt(inputs_embeds=inputs_embeds, attention_mask=attention_mask, return_dict=True)
    hidden_state = gpt_out.last_hidden_state

    # å‘é‡åŒ–å¯¦ç¾ä»¥æ›¿ä»£è¿´åœˆ
    conditioning_len = 32

    # ç§»é™¤æ¢ä»¶éƒ¨åˆ†
    h_no_cond = hidden_state[:, conditioning_len:]  # [b, seq_len, hidden_dim]
    attention_no_cond = attention_mask[:, conditioning_len:]  # [b, seq_len]

    # æ‰¹æ¬¡æ‡‰ç”¨ final_norm
    latent = actual_model.final_norm(h_no_cond)  # [b, seq_len, hidden_dim]
    
    max_text_len = text_lengths.max().item()
    max_mel_len = codes_lengths.max().item()
    
    # å»ºç«‹æ‰¹æ¬¡å¼µé‡
    batch_text_latents = torch.zeros(b, max_text_len, latent.shape[-1], device=latent.device, dtype=latent.dtype)
    batch_mel_latents = torch.zeros(b, max_mel_len, latent.shape[-1], device=latent.device, dtype=latent.dtype)
    
    # å¡«å……æ‰¹æ¬¡å¼µé‡
    for i in range(b):
        text_len = text_lengths[i].item()
        mel_len = codes_lengths[i].item()
        
        # æå–æœ‰æ•ˆ latent
        sample_valid_mask = attention_no_cond[i] == 1
        sample_latent = latent[i][sample_valid_mask]  # [valid_len, hidden_dim]
        
        expected_len = text_len + mel_len
        assert sample_latent.shape[0] == expected_len, \
            f"Expected valid_latent shape {expected_len}, got {sample_latent.shape[0]}, " \
            f"text_len: {text_len}, mel_len: {mel_len}"
        
        # åˆ†å‰²ä¸¦åˆ†é…
        batch_text_latents[i, :text_len] = sample_latent[:text_len]
        batch_mel_latents[i, :mel_len] = sample_latent[text_len:text_len + mel_len]
    
    # å‘é‡åŒ–é ­éƒ¨è™•ç†
    batch_text_logits = actual_model.text_head(batch_text_latents)  # [b, max_text_len, vocab_size]
    batch_text_logits = batch_text_logits.permute(0, 2, 1)  # [b, vocab_size, max_text_len]

    batch_mel_logits = actual_model.mel_head(batch_mel_latents)  # [b, max_mel_len, vocab_size]
    batch_mel_logits = batch_mel_logits.permute(0, 2, 1)  # [b, vocab_size, max_mel_len]
    
    output = {}
    if output_logits:
        output["logits"] = (batch_text_logits, batch_mel_logits)
    if output_latent:
        output["latent"] = (batch_text_latents, batch_mel_latents)
    return output

def forward_UnifiedVoice(
    model: UnifiedVoice,
    mel_spec: torch.FloatTensor,
    mel_codes: torch.LongTensor,
    text_ids: torch.LongTensor,
    mel_lengths: torch.LongTensor,
    codes_lengths: torch.LongTensor,
    text_lengths: torch.LongTensor,
    condition_mels: torch.FloatTensor = None,
    condition_lengths: torch.LongTensor = None,
    speaker_ids: List[str] = None,
    add_mel_stop_token: bool = True,
    output_loss: bool = True,
    output_logits: bool = True,
    output_latent: bool = False,
    loss_reduction: str = "mean",
):
    """
    åŸ·è¡Œ UnifiedVoice æ¨¡å‹çš„å®Œæ•´å‰å‘å‚³æ’­æµç¨‹ã€‚

    æ­¤å‡½æ•¸æ•´åˆäº†è¼¸å…¥åµŒå…¥ã€ä½ç½®ç·¨ç¢¼ã€æ¢ä»¶è¼¸å…¥è™•ç†ï¼Œä¸¦èª¿ç”¨ GPT2 æ¨¡å‹é€²è¡Œè¨ˆç®—ã€‚
    å¦‚æœéœ€è¦ï¼Œé‚„æœƒè¨ˆç®— Lossã€‚

    Args:
        model (UnifiedVoice): æ¨¡å‹å¯¦ä¾‹ã€‚
        mel_spec (torch.FloatTensor): Mel é »è­œåœ–è¼¸å…¥ã€‚
        mel_codes (torch.LongTensor): Mel ä»£ç¢¼è¼¸å…¥ã€‚
        text_ids (torch.LongTensor): æ–‡å­— Token IDã€‚
        mel_lengths, codes_lengths, text_lengths (torch.LongTensor): å„åºåˆ—çš„é•·åº¦ã€‚
        condition_mels (torch.FloatTensor, optional): æ¢ä»¶ Mel é »è­œã€‚
        speaker_ids (List[str], optional): èªªè©±äºº ID åˆ—è¡¨ã€‚
        add_mel_stop_token (bool): æ˜¯å¦æ·»åŠ  Mel çµæŸ Tokenã€‚
        output_loss (bool): æ˜¯å¦è¨ˆç®—ä¸¦å›å‚³ Lossã€‚
        output_logits (bool): æ˜¯å¦å›å‚³ logitsã€‚
        output_latent (bool): æ˜¯å¦å›å‚³ latentã€‚
        loss_reduction (str): Loss ç¸®æ¸›æ–¹å¼ã€‚

    Returns:
        dict: åŒ…å« loss, logits, targets, mel_accuracy ç­‰çµæœçš„å­—å…¸ã€‚
    """

    actual_model = model.module if isinstance(model, nn.DataParallel) else model

    # è™•ç†æ¢ä»¶è¼¸å…¥ä¾†æº
    cond_source = condition_mels if condition_mels is not None else mel_spec
    cond_lengths = condition_lengths if condition_lengths is not None else mel_lengths
    conditioning_latent = actual_model.get_conditioning(cond_source, cond_lengths, speaker_ids=speaker_ids)
    
    # æ§‹å»ºæ–‡å­—è¼¸å…¥ (åŠ å…¥ start/stop tokens)
    B, T_pad = text_ids.shape
    max_out_text = T_pad + 2
    text_inputs = text_ids.new_zeros((B, max_out_text))
    for i, L in enumerate(text_lengths):
        L = L.item()
        text_inputs[i, 0] = actual_model.start_text_token
        text_inputs[i, 1 : L + 1] = text_ids[i, :L]
        text_inputs[i, L + 1] = actual_model.stop_text_token
    text_targets = text_inputs[:, 1:].clone().contiguous()

    # æ§‹å»º Mel è¼¸å…¥ (åŠ å…¥ start/stop tokens)
    B, M_pad = mel_codes.shape
    extra_stop = 1 if add_mel_stop_token else 0
    max_out_mel = M_pad + 1 + extra_stop
    mel_inputs = mel_codes.new_zeros((B, max_out_mel))
    for i, L in enumerate(codes_lengths):
        L = L.item()
        mel_inputs[i, 0] = actual_model.start_mel_token
        mel_inputs[i, 1 : L + 1] = mel_codes[i, :L]
        if add_mel_stop_token:
            mel_inputs[i, L + 1] = actual_model.stop_mel_token
    mel_targets = mel_inputs[:, 1:].clone().contiguous()

    # è¨ˆç®—åµŒå…¥
    text_emb = actual_model.text_embedding(text_inputs) + actual_model.text_pos_embedding(text_inputs)
    mel_emb = actual_model.mel_embedding(mel_inputs) + actual_model.mel_pos_embedding(mel_inputs)

    mel_codes = mel_inputs
    
    inputs_embeds = torch.cat([conditioning_latent, text_emb, mel_emb], dim=1)
    
    # å»ºç«‹æ³¨æ„åŠ›é®ç½©
    batch_size, total_seq_len = inputs_embeds.shape[:2]
    attention_mask = torch.zeros(batch_size, total_seq_len, dtype=torch.long, device=inputs_embeds.device)
    
    conditioning_len = conditioning_latent.shape[1]
    actual_text_lengths = text_lengths + 2
    actual_mel_lengths = codes_lengths + 1 + int(add_mel_stop_token)
    
    for i in range(batch_size):
        attention_mask[i, :conditioning_len] = 1
        
        text_start = conditioning_len
        text_end = text_start + actual_text_lengths[i].item()
        attention_mask[i, text_start:text_end] = 1
        
        mel_start = conditioning_len + text_emb.shape[1]
        mel_end = mel_start + actual_mel_lengths[i].item()
        attention_mask[i, mel_start:mel_end] = 1

    gpt2_outputs = forward_gpt2(
        model,
        inputs_embeds,
        text_lengths + 2,
        codes_lengths + 1 + int(add_mel_stop_token),
        attention_mask=attention_mask,
        output_latent=output_latent,
        output_logits=output_logits or output_loss,
    )
    
    outputs = {}
    if output_logits or output_loss:
        text_logits, mel_logits = gpt2_outputs["logits"]
        text_logits = text_logits[:, :, :-1].contiguous()
        mel_logits = mel_logits[:, :, :-1].contiguous()
        if output_loss:
            batch_size = text_targets.size(0)
            
            # è¨ˆç®—æ–‡å­—é®ç½©
            text_mask = torch.zeros_like(text_targets, dtype=torch.bool)
            for i in range(batch_size):
                actual_text_len = text_lengths[i].item() + 1
                text_mask[i, :actual_text_len] = True
            
            # è¨ˆç®— Mel é®ç½©
            mel_mask = torch.zeros_like(mel_targets, dtype=torch.bool)
            for i in range(batch_size):
                actual_mel_len = codes_lengths[i].item() + int(add_mel_stop_token)
                mel_mask[i, :actual_mel_len] = True
            
            loss_text = F.cross_entropy(text_logits, text_targets.long(), reduction='none')
            loss_mel = F.cross_entropy(mel_logits, mel_targets.long(), reduction='none')
            
            # æ‡‰ç”¨é®ç½©ä¸¦è¨ˆç®—å¹³å‡ Loss
            loss_text = (loss_text * text_mask).sum() / text_mask.sum() if text_mask.sum() > 0 else torch.tensor(0.0, device=text_logits.device)
            loss_mel = (loss_mel * mel_mask).sum() / mel_mask.sum() if mel_mask.sum() > 0 else torch.tensor(0.0, device=mel_logits.device)
            
            outputs["loss"] = (loss_text, loss_mel)
            
            # è¨ˆç®— Mel é æ¸¬æº–ç¢ºç‡
            with torch.no_grad():
                mel_logits_flat = mel_logits.permute(0, 2, 1).reshape(-1, mel_logits.size(1))
                mel_targets_flat = mel_targets.view(-1)
                mel_mask_flat = mel_mask.view(-1)
                
                if mel_mask_flat.sum() > 0:
                    valid_mel_logits = mel_logits_flat[mel_mask_flat]
                    valid_mel_targets = mel_targets_flat[mel_mask_flat]
                    mel_acc_1, mel_acc_10, mel_acc_20 = top_k_accuracy(valid_mel_logits, valid_mel_targets, k=(1, 10, 20))
                    outputs["mel_accuracy"] = {"acc_1": mel_acc_1, "acc_10": mel_acc_10, "acc_20": mel_acc_20}
                else:
                    outputs["mel_accuracy"] = {"acc_1": 0.0, "acc_10": 0.0, "acc_20": 0.0}
                    
        if output_logits:
            outputs["logits"] = (text_logits, mel_logits)
            outputs["targets"] = (text_targets, mel_targets)
        

    if output_latent:
        outputs["latent"] = gpt2_outputs["latent"]
        
    clear_torch_cache()
    return outputs

def top_k_accuracy(logits: torch.Tensor, targets: torch.Tensor, k: Tuple[int, ...] = (1, 10, 20)) -> List[float]:
    """
    è¨ˆç®— Top-K æº–ç¢ºç‡ã€‚

    Args:
        logits (torch.Tensor): é æ¸¬çš„ logitsã€‚
        targets (torch.Tensor): çœŸå¯¦æ¨™ç±¤ã€‚
        k (Tuple[int, ...]): è¦è¨ˆç®—çš„ k å€¼åˆ—è¡¨ã€‚

    Returns:
        List[float]: å°æ‡‰æ¯å€‹ k å€¼çš„æº–ç¢ºç‡åˆ—è¡¨ã€‚
    """
    max_k = max(k)
    _, topk_preds = torch.topk(logits, max_k, dim=1)  # (B*L, max_k)
    
    targets_reshaped = targets.view(-1, 1) # (B*L, 1)
    topk_preds_reshaped = topk_preds.view(-1, max_k) # (B*L, max_k)

    res = []
    for ki in k:
        correct_k = (topk_preds_reshaped[:, :ki] == targets_reshaped).any(dim=-1)
        acc = correct_k.float().mean().item() * 100
        res.append(acc)
    return res

class Trainer:
    """
    UnifiedVoice æ¨¡å‹çš„è¨“ç·´ç®¡ç†å™¨ã€‚

    è² è²¬ç®¡ç†è¨“ç·´æµç¨‹ã€é©—è­‰ã€æª¢æŸ¥é»å„²å­˜ä»¥åŠæ··åˆç²¾åº¦è¨“ç·´è¨­å®šã€‚
    """

    def __init__(self, config: DictConfig, use_multi_gpu: bool = True):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨ã€‚

        Args:
            config (DictConfig): è¨“ç·´é…ç½®åƒæ•¸ (OmegaConf)ã€‚
            use_multi_gpu (bool): æ˜¯å¦å•Ÿç”¨å¤š GPU è¨“ç·´æ”¯æ´ã€‚
        """
        self.config = config
        self.use_multi_gpu = use_multi_gpu and GPU_MANAGER_AVAILABLE
        self.gpu_manager = None
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

        if self.use_multi_gpu and torch.cuda.is_available():
            try:
                self.gpu_manager = get_global_gpu_manager()
                gpu_count = self.gpu_manager.get_gpu_count()
                if gpu_count > 1:
                    logger.info(f"ğŸ® å¤š GPU è¨“ç·´æ¨¡å¼ï¼šåµæ¸¬åˆ° {gpu_count} å€‹ GPU")
                    self.gpu_manager.print_summary()
                elif gpu_count == 1:
                    logger.info("ğŸ® å–® GPU è¨“ç·´æ¨¡å¼")
                    self.use_multi_gpu = False
            except Exception as e:
                logger.warning(f"âš ï¸  GPU Manager åˆå§‹åŒ–å¤±æ•—: {e}")
                self.use_multi_gpu = False

        self._set_seed(self.config.train.seed)
        self.grad_scaler = None
        self.train_dtype, self.use_amp = self._setup_mixed_precision()

        # è¨­å®šè·¯å¾‘
        self.finetune_dir = self.config.train.finetune_model_dir
        self.checkpoint_dir = os.path.join(self.finetune_dir, "checkpoints")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        env_run_name = os.environ.get("RUN_NAME")
        env_log_dir = os.environ.get("RUN_LOG_DIR")
        self.run_name = env_run_name if env_run_name else f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        if env_log_dir:
            self.log_dir = os.path.abspath(env_log_dir)
        else:
            self.log_dir = os.path.abspath(os.path.join("logs", self.run_name))
        os.makedirs(self.log_dir, exist_ok=True)
        
        self._setup_logging(os.path.join(self.log_dir, "train.log"))

        self.writer = SummaryWriter(log_dir=self.log_dir)
        logger.info(f"TensorBoard è¨˜éŒ„ç›®éŒ„: {self.log_dir}")

        self._load_models()
        self._setup_optimizer_and_scheduler()

        self.best_val_loss = (0, float('inf'), float('inf'))  # (epoch, text_loss, mel_loss)
        self.update_steps = 0

    def _set_seed(self, seed: int):
        """è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯¦é©—å¯é‡è¤‡æ€§ã€‚"""
        random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        logger.info(f"è¨­å®šéš¨æ©Ÿç¨®å­ç‚º: {seed}")

    def _resolve_dtype(self, precision_str: str):
        """
        å°‡ç²¾åº¦è¨­å®šå­—ä¸²è½‰æ›ç‚º torch.dtypeã€‚
        
        Args:
            precision_str (str): ç²¾åº¦è¨­å®š (å¦‚ 'fp16', 'bf16', 'auto').

        Returns:
            torch.dtype: å°æ‡‰çš„ PyTorch è³‡æ–™å‹æ…‹ã€‚
        """
        def supports_fp8():
            if not torch.cuda.is_available():
                return False
            capability = torch.cuda.get_device_capability()
            compute_capability = capability[0] * 10 + capability[1]
            return compute_capability >= 89

        if precision_str == "no" or precision_str == "fp32":
            return torch.float32
        elif precision_str == "auto":
            if supports_fp8():
                return torch.float8_e4m3fn if hasattr(torch, 'float8_e4m3fn') else torch.bfloat16
            elif torch.cuda.is_bf16_supported():
                return torch.bfloat16
            else:
                return torch.float16
        elif precision_str == "fp8":
            if supports_fp8() and hasattr(torch, 'float8_e4m3fn'):
                return torch.float8_e4m3fn
            else:
                logger.warning(f"âš ï¸  ç•¶å‰ç¡¬é«”ä¸æ”¯æ´ FP8ï¼Œé€€å› BF16")
                return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        elif precision_str == "bf16":
            return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        elif precision_str == "fp16":
            return torch.float16
        else:
            logger.warning(f"âš ï¸  æœªçŸ¥ç²¾åº¦è¨­å®š: {precision_str}ï¼Œå°‡ä½¿ç”¨ FP32")
            return torch.float32

    def _setup_mixed_precision(self):
        """
        é…ç½®æ··åˆç²¾åº¦è¨“ç·´ç’°å¢ƒã€‚

        æ ¹æ“šé…ç½®é¸æ“‡é©ç•¶çš„ç²¾åº¦ (BF16/FP16/FP8)ï¼Œä¸¦åœ¨éœ€è¦æ™‚åˆå§‹åŒ– GradScalerã€‚

        Returns:
            Tuple[torch.dtype, bool]: (é‹ç®—ç²¾åº¦, æ˜¯å¦å•Ÿç”¨ AMP)
        """
        mixed_precision = self.config.train.get("mixed_precision", "auto")

        if not torch.cuda.is_available():
            logger.warning("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå¼·åˆ¶ä½¿ç”¨ FP32 è¨“ç·´")
            return None, False

        dtype = self._resolve_dtype(mixed_precision)

        logger.info("ğŸš€ æ··åˆç²¾åº¦è¨“ç·´é…ç½®")
        logger.info(f"   é‹ç®—ç²¾åº¦: {dtype}")

        use_grad_scaler = (dtype == torch.float16)

        if use_grad_scaler:
            self.grad_scaler = GradScaler()
            logger.info("   ğŸ“Š å•Ÿç”¨ GradScaler (é‡å° FP16 é˜²æ­¢æ¢¯åº¦ä¸‹æº¢)")
        else:
            self.grad_scaler = None

        logger.info("=" * 50)
        return dtype, True

    def _setup_logging(self, log_path: str):
        """é…ç½® loguru æ—¥èªŒç³»çµ±ã€‚"""
        logger.add(log_path, level="INFO", encoding="utf-8")
        logger.info("æ—¥èªŒç³»çµ±å·²é…ç½®ã€‚")
        logger.info("å®Œæ•´é…ç½®åƒæ•¸:\n" + OmegaConf.to_yaml(self.config))

    def _load_models(self):
        """è¼‰å…¥ BPE æ¨¡å‹ã€BigVGAN èˆ‡ UnifiedVoice æ¨¡å‹ï¼Œä¸¦æ‡‰ç”¨ LoRAã€‚"""
        logger.info("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
        
        # è¼‰å…¥ BPE
        bpe_model_path = os.path.join(self.finetune_dir, self.config.dataset.bpe_model)
        self.bpe_model = spm.SentencePieceProcessor(bpe_model_path)
        logger.info("BPE æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
        
        # è¼‰å…¥ UnifiedVoice
        gpt_checkpoint_path = os.path.join(self.finetune_dir, self.config.gpt_checkpoint)
        self.model = load_UnifiedVoice(self.config.gpt, gpt_checkpoint_path, self.device)
        logger.info("UnifiedVoice åŸºç¤æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
    
        # æ‡‰ç”¨ LoRA
        self.model = self._apply_lora(self.model)
        logger.info("LoRA é©é…å™¨å·²æ‡‰ç”¨ã€‚")

        # å¤š GPU æ”¯æ´
        if self.use_multi_gpu and self.gpu_manager and self.gpu_manager.get_gpu_count() > 1:
            logger.info("ğŸš€ å•Ÿç”¨ DataParallel å¤š GPU è¨“ç·´")
            device_ids = list(range(torch.cuda.device_count()))
            logger.info(f"  ä½¿ç”¨ GPU è£ç½®: {device_ids}")
            self.model = nn.DataParallel(self.model, device_ids=device_ids)
            logger.info(f"  æ¨¡å‹å·²åˆ†æ•£è‡³ {len(device_ids)} å€‹ GPU")

    def _apply_lora(self, model: UnifiedVoice) -> UnifiedVoice:
        """
        ç‚ºæ¨¡å‹é…ç½®ä¸¦æ‡‰ç”¨ LoRA (Low-Rank Adaptation)ã€‚
        
        é€™æœƒå‡çµå¤§éƒ¨åˆ†åƒæ•¸ï¼Œåƒ…é–‹æ”¾ LoRA å±¤èˆ‡ç‰¹å®šç·¨ç¢¼å™¨é€²è¡Œè¨“ç·´ã€‚
        """
        lora_cfg = self.config.train.lora
        gpt_lora_config = LoraConfig(
            r=lora_cfg.r,
            target_modules=lora_cfg.target_modules,
            task_type=TaskType.CAUSAL_LM,
            lora_alpha=lora_cfg.lora_alpha,
            lora_dropout=lora_cfg.lora_dropout,
            bias="none",
            fan_in_fan_out=True,
        )
        model.requires_grad_(False)
        model.inference_model = get_peft_model(model.inference_model, gpt_lora_config)

        # âš ï¸ é‡è¦ï¼šå‡çµ conditioning_encoder å’Œ perceiver_encoder
        # åŸå› ï¼šé è¨“ç·´çš„ encoder å·²ç¶“å­¸æœƒæŠ½å–éŸ³è‰²ï¼Œå¦‚æœç¹¼çºŒè¨“ç·´ï¼Œ
        # å®ƒæœƒå­¸ç¿’ç·¨ç¢¼æ›´å¤šè³‡è¨Šï¼ˆåŒ…æ‹¬å…§å®¹ï¼‰ï¼Œå°è‡´æ¨è«–æ™‚è¤‡è£½åƒè€ƒéŸ³æª”çš„æ–‡å­—å…§å®¹ã€‚
        # é€™æ˜¯ zero-shot TTS çš„å¸¸è¦‹å•é¡Œã€‚
        if hasattr(model, "conditioning_encoder"):
            model.conditioning_encoder.requires_grad_(False)
            logger.info("âœ“ conditioning_encoder å·²å‡çµï¼ˆé˜²æ­¢å…§å®¹æ´©æ¼ï¼‰")
        if hasattr(model, "perceiver_encoder"):
            model.perceiver_encoder.requires_grad_(False)
            logger.info("âœ“ perceiver_encoder å·²å‡çµï¼ˆé˜²æ­¢å…§å®¹æ´©æ¼ï¼‰")

        # åªè¨“ç·´ LoRA å±¤ï¼Œè®“ GPT å­¸ç¿’å¦‚ä½•æ ¹æ“šã€Œå›ºå®šçš„éŸ³è‰² embeddingã€ç”Ÿæˆå°æ‡‰å…§å®¹
        return model

    def _setup_optimizer_and_scheduler(self, num_training_steps: int = 1000):
        """é…ç½® LoRA+ æœ€ä½³åŒ–å™¨èˆ‡ Cosine å­¸ç¿’ç‡æ’ç¨‹å™¨ã€‚"""
        opt_cfg = self.config.train.optimizer
        optimizer = create_loraplus_optimizer(
            model=self.model,
            optimizer_cls=AdamW,
            lr=opt_cfg.learning_rate,
            loraplus_lr_ratio=opt_cfg.loraplus_lr_ratio,
            loraplus_weight_decay=opt_cfg.weight_decay,
        )
        self.optimizer = optimizer

        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(num_training_steps * opt_cfg.warmup_ratio),
            num_training_steps=num_training_steps,
        )
        self.scheduler = scheduler
        logger.info("Optimizer (LoRA+) èˆ‡ Scheduler å·²é…ç½®å®Œæˆã€‚")

    def _train_step(self, data_batch: tuple) -> Tuple[torch.Tensor, torch.Tensor, dict]:
        """
        åŸ·è¡Œå–®ä¸€è¨“ç·´æ­¥ï¼šå‰å‘å‚³æ’­ã€Loss è¨ˆç®—ã€‚

        Args:
            data_batch (tuple): åŒ…å«æ‰€æœ‰è¼¸å…¥è³‡æ–™çš„ tupleã€‚

        Returns:
            Tuple[torch.Tensor, torch.Tensor, dict]: (text_loss, mel_loss, mel_accuracy_dict)
        """
        self.model.train()
        actual_model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        actual_model.inference_model.kv_cache = False
    
        mel_spec, mel_codes, text_ids, cond_mels, speaker_ids, mel_lengths, codes_lengths, text_lengths, cond_lengths = data_batch

        # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        context = torch.autocast(device_type='cuda', dtype=self.train_dtype) if (self.use_amp and self.train_dtype) else torch.no_grad()
        if not (self.use_amp and self.train_dtype):
             # å¦‚æœä¸æ˜¯ AMPï¼Œå‰‡ä½¿ç”¨ dummy context (no_grad åªæ˜¯ placeholderï¼Œå¯¦éš›ä¸Šä¸æœƒç”¨)
             # ä½†ç‚ºäº†é¿å… indent æ··äº‚ï¼Œç›´æ¥ä½¿ç”¨æ¢ä»¶åˆ¤æ–·
             pass

        if self.use_amp and self.train_dtype:
             with torch.autocast(device_type='cuda', dtype=self.train_dtype):
                outputs = forward_UnifiedVoice(
                    self.model, mel_spec, mel_codes, text_ids, mel_lengths, codes_lengths, text_lengths,
                    condition_mels=cond_mels, condition_lengths=cond_lengths, speaker_ids=None,
                    output_loss=True, output_logits=True,
                )
        else:
             outputs = forward_UnifiedVoice(
                self.model, mel_spec, mel_codes, text_ids, mel_lengths, codes_lengths, text_lengths,
                condition_mels=cond_mels, condition_lengths=cond_lengths, speaker_ids=None,
                output_loss=True, output_logits=True,
            )

        loss_text, loss_mel = outputs["loss"]
        mel_accuracy = outputs.get("mel_accuracy", {"acc_1": 0.0, "acc_10": 0.0, "acc_20": 0.0})
        return loss_text, loss_mel, mel_accuracy

    @torch.no_grad()
    def _validate_epoch(self, valid_ds: Dataset, epoch: int):
        """
        åŸ·è¡Œé©—è­‰æµç¨‹ã€‚

        Args:
            valid_ds (Dataset): é©—è­‰è³‡æ–™é›†ã€‚
            epoch (int): ç•¶å‰ Epoch æ•¸ã€‚

        Returns:
            Tuple: (avg_text_loss, avg_mel_loss, acc_1, acc_10, acc_20)
        """
        self.model.eval()
        logger.info(f"æ­£åœ¨é€²è¡Œç¬¬ {epoch + 1} è¼ªé©—è­‰...")
        
        total_text_loss, total_mel_loss = 0.0, 0.0
        total_text_tokens, total_mel_tokens = 0, 0
        all_mel_logits, all_mel_targets = [], []

        for batch in tqdm(valid_ds, desc="é©—è­‰ä¸­", dynamic_ncols=True):
            data_batch = []
            for item in batch:
                if torch.is_tensor(item):
                    data_batch.append(item.to(self.device))
                else:
                    data_batch.append(item)

            mel_spec, mel_codes, text_ids, cond_mels, speaker_ids, mel_lengths, codes_lengths, text_lengths, cond_lengths = data_batch

            if self.use_amp and self.train_dtype:
                with torch.autocast(device_type='cuda', dtype=self.train_dtype):
                    outputs = forward_UnifiedVoice(
                        self.model, mel_spec, mel_codes, text_ids, mel_lengths, codes_lengths, text_lengths,
                        condition_mels=cond_mels, condition_lengths=cond_lengths, speaker_ids=None,
                        output_loss=True, output_logits=True,
                    )
            else:
                outputs = forward_UnifiedVoice(
                    self.model, mel_spec, mel_codes, text_ids, mel_lengths, codes_lengths, text_lengths,
                    condition_mels=cond_mels, condition_lengths=cond_lengths, speaker_ids=None,
                    output_loss=True, output_logits=True,
                )
            
            loss_text, loss_mel = outputs["loss"]
            batch_text_tokens = text_lengths.sum().item()
            batch_mel_tokens = (codes_lengths + 1).sum().item()

            total_text_loss += loss_text.item() * batch_text_tokens
            total_mel_loss += loss_mel.item() * batch_mel_tokens
            total_text_tokens += batch_text_tokens
            total_mel_tokens += batch_mel_tokens

            # æ”¶é›†æ•¸æ“šè¨ˆç®—æº–ç¢ºç‡
            current_mel_logits = outputs["logits"][1]
            current_mel_targets = outputs["targets"][1]
            if current_mel_logits.numel() > 0 and current_mel_targets.numel() > 0:
                batch_size = current_mel_targets.size(0)
                mel_mask = torch.zeros_like(current_mel_targets, dtype=torch.bool)
                for i in range(batch_size):
                    actual_mel_len = codes_lengths[i].item() + 1
                    mel_mask[i, :actual_mel_len] = True
                
                valid_mask = mel_mask.view(-1)
                if valid_mask.sum() > 0:
                    mel_logits_filtered = current_mel_logits.permute(0, 2, 1).reshape(-1, current_mel_logits.size(1))[valid_mask]
                    mel_targets_filtered = current_mel_targets.view(-1)[valid_mask]
                    all_mel_logits.append(mel_logits_filtered)
                    all_mel_targets.append(mel_targets_filtered)
            
            clear_torch_cache()

        avg_text_loss = total_text_loss / total_text_tokens
        avg_mel_loss = total_mel_loss / total_mel_tokens
        
        all_mel_logits = torch.cat(all_mel_logits, dim=0)
        all_mel_targets = torch.cat(all_mel_targets, dim=0)
        acc_1, acc_10, acc_20 = top_k_accuracy(all_mel_logits, all_mel_targets, k=(1, 10, 20))

        logger.info(f"**ç¬¬ {epoch + 1} è¼ªé©—è­‰çµæœ**")
        logger.info(f"Text Loss: {avg_text_loss:.4f}, Mel Loss: {avg_mel_loss:.4f}")
        logger.info(f"Accuracy@1: {acc_1:.2f}%, Accuracy@10: {acc_10:.2f}%, Accuracy@20: {acc_20:.2f}%")
        
        return avg_text_loss, avg_mel_loss, acc_1, acc_10, acc_20

    def _save_checkpoint(self, file_name: str, merge_lora: bool, unload_after_merge: bool):
        """
        å„²å­˜æ¨¡å‹æª¢æŸ¥é»ã€‚

        Args:
            file_name (str): æª”æ¡ˆåç¨±ã€‚
            merge_lora (bool): æ˜¯å¦å°‡ LoRA æ¬Šé‡åˆä½µé€²ä¸»æ¨¡å‹ã€‚
            unload_after_merge (bool): åˆä½µå¾Œæ˜¯å¦å¸è¼‰ LoRA (è‹¥ç‚º True å‰‡ä¸å½±éŸ¿è¨“ç·´ä¸­çš„æ¨¡å‹å¯¦ä¾‹)ã€‚
        """
        checkpoint_path = os.path.join(self.checkpoint_dir, file_name)
        self.model.eval()

        actual_model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        model_to_save = actual_model

        if merge_lora:
            logger.info("æ­£åœ¨åˆä½µ LoRA æ¬Šé‡ä»¥é€²è¡Œå„²å­˜...")
            if unload_after_merge:
                # å»ºç«‹æ·±è¤‡è£½ä»¥é¿å…å½±éŸ¿è¨“ç·´ç‹€æ…‹
                logger.info("æ­£åœ¨è¤‡è£½æ¨¡å‹ä»¥é€²è¡Œå®‰å…¨åˆä½µ...")
                model_to_save = copy.deepcopy(actual_model)
                fused_inference_model = model_to_save.inference_model.merge_and_unload()
                model_to_save.inference_model = fused_inference_model
                logger.info("åˆä½µå®Œæˆã€‚")
            else:
                actual_model.inference_model.merge_adapter()
    
        state_dict = model_to_save.state_dict()
        checkpoint_data = {'model': state_dict}
        
        torch.save(checkpoint_data, checkpoint_path)
        logger.info(f"æª¢æŸ¥é»å·²å„²å­˜è‡³: {checkpoint_path}")
    
        if merge_lora and unload_after_merge:
            del model_to_save
            clear_torch_cache()
            logger.info("æš«å­˜æ¨¡å‹å·²æ¸…ç†ã€‚")
        
        if merge_lora and not unload_after_merge:
            logger.info("æ­£åœ¨è§£é™¤åˆä½µ LoRA æ¬Šé‡ä»¥ç¹¼çºŒè¨“ç·´...")
            actual_model.inference_model.unmerge_adapter()

        self.model.train()

    def train(self, train_ds: Dataset, valid_ds: Dataset):
        """
        åŸ·è¡Œä¸»è¦è¨“ç·´è¿´åœˆã€‚

        Args:
            train_ds (Dataset): è¨“ç·´è³‡æ–™é›†ã€‚
            valid_ds (Dataset): é©—è­‰è³‡æ–™é›†ã€‚
        """
        train_cfg = self.config.train
        total_ds_count = len(train_ds)
        
        samples_per_epoch = total_ds_count
        total_update_steps = samples_per_epoch * train_cfg.epochs
        self._setup_optimizer_and_scheduler(num_training_steps=total_update_steps)
        
        logger.info(f"é–‹å§‹è¨“ç·´ï¼Œå…± {train_cfg.epochs} è¼ª (Epochs)ã€‚")
        logger.info(f"æ¯è¼ªæ¨£æœ¬æ•¸: {samples_per_epoch}")
        logger.info(f"ç¸½æ›´æ–°æ­¥æ•¸: {total_update_steps}")

        text_weight = train_cfg.text_weight

        for epoch in range(train_cfg.epochs):
            logger.info(f"EPOCH {epoch + 1}/{train_cfg.epochs} é–‹å§‹ " + "=" * 30)

            # ä½¿ç”¨ tqdm åŒ…è£è¨“ç·´è³‡æ–™è¼‰å…¥å™¨ï¼Œå¯¦ç¾é€²åº¦æ¢é¡¯ç¤º
            pbar = tqdm(enumerate(train_ds), total=len(train_ds), desc=f"Epoch {epoch + 1}", dynamic_ncols=True)
            
            for batch_idx, batch in pbar:
                data_batch = []
                for item in batch:
                    if torch.is_tensor(item):
                        data_batch.append(item.to(self.device))
                    else:
                        data_batch.append(item)

                loss_text, loss_mel, mel_accuracy = self._train_step(tuple(data_batch))
                acc_1, acc_10, acc_20 = mel_accuracy["acc_1"], mel_accuracy["acc_10"], mel_accuracy["acc_20"]

                weighted_loss = text_weight * loss_text + (1.0 - text_weight) * loss_mel
                
                # æª¢æŸ¥ NaN/Inf Loss
                if torch.isnan(weighted_loss) or torch.isinf(weighted_loss):
                    logger.warning(f"Epoch {epoch}, Batch {batch_idx} ç™¼ç¾ NaN æˆ– Inf Lossï¼Œå·²è·³éã€‚")
                    continue

                # æœ€ä½³åŒ–æ­¥é©Ÿ
                self.optimizer.zero_grad()
                
                if self.grad_scaler is not None:
                    self.grad_scaler.scale(weighted_loss).backward()
                    self.grad_scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg.max_grad_norm)
                    self.grad_scaler.step(self.optimizer)
                    self.grad_scaler.update()
                else:
                    weighted_loss.backward()
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), train_cfg.max_grad_norm)
                    self.optimizer.step()
                
                self.scheduler.step()
                self.update_steps += 1

                # æ›´æ–°é€²åº¦æ¢é¡¯ç¤ºè³‡è¨Š
                pbar.set_postfix({
                    "txt_loss": f"{loss_text.item():.3f}",
                    "mel_loss": f"{loss_mel.item():.3f}",
                    "acc@1": f"{acc_1:.1f}%",
                    "lr": f"{self.scheduler.get_last_lr()[0]:.2e}"
                })

                # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™è‡³ TensorBoard
                self.writer.add_scalar("loss/text", loss_text.item(), self.update_steps)
                self.writer.add_scalar("loss/mel", loss_mel.item(), self.update_steps)
                self.writer.add_scalar("loss/total", weighted_loss.item(), self.update_steps)
                self.writer.add_scalar("accuracy/top1", acc_1, self.update_steps)
                self.writer.add_scalar("accuracy/top10", acc_10, self.update_steps)
                self.writer.add_scalar("accuracy/top20", acc_20, self.update_steps)
                self.writer.add_scalar("train/grad_norm", grad_norm.item(), self.update_steps)
                self.writer.add_scalar("train/lr", self.scheduler.get_last_lr()[0], self.update_steps)

            # --- Epoch çµæŸ ---
            val_text_loss, val_mel_loss, val_acc1, val_acc10, val_acc20 = self._validate_epoch(valid_ds, epoch)
            
            self.writer.add_scalar("val/loss_text", val_text_loss, epoch + 1)
            self.writer.add_scalar("val/loss_mel", val_mel_loss, epoch + 1)
            self.writer.add_scalar("val/accuracy_top1", val_acc1, epoch + 1)
            self.writer.add_scalar("val/accuracy_top10", val_acc10, epoch + 1)
            self.writer.add_scalar("val/accuracy_top20", val_acc20, epoch + 1)

            epoch_checkpoint_name = f"gpt_epoch_{epoch + 1}.pth"
            logger.info(f"å„²å­˜ Epoch {epoch + 1} æ¨¡å‹: {epoch_checkpoint_name}")
            self._save_checkpoint(epoch_checkpoint_name, merge_lora=True, unload_after_merge=True)
            
            if val_mel_loss < self.best_val_loss[2]:
                logger.info(f"ç™¼ç¾æœ€ä½³é©—è­‰ Mel Loss: {val_mel_loss:.4f}ã€‚å„²å­˜æœ€ä½³æ¨¡å‹ã€‚")
                self.best_val_loss = (epoch, val_text_loss, val_mel_loss)
                self._save_checkpoint("gpt_best.pth", merge_lora=True, unload_after_merge=True)

            clear_torch_cache()

        # --- è¨“ç·´çµæŸ ---
        logger.info("è¨“ç·´å®Œæˆã€‚å„²å­˜æœ€çµ‚æ¨¡å‹ã€‚")
        self._save_checkpoint("gpt_finetuned.pth", merge_lora=True, unload_after_merge=True)
        
        final_config_path = os.path.join(self.finetune_dir, "config_finetuned.yaml")
        final_config = self.config.copy()
        final_config.gpt_checkpoint = "checkpoints/gpt_finetuned.pth"
        OmegaConf.save(final_config, final_config_path)
        logger.info(f"æœ€çµ‚é…ç½®å·²å„²å­˜è‡³ {final_config_path}")
        
        logger.info(f"æœ€ä½³é©—è­‰çµæœ (Epoch {self.best_val_loss[0] + 1}): "
                    f"text_loss: {self.best_val_loss[1]:.4f}, mel_loss: {self.best_val_loss[2]:.4f}")
        
        self.writer.close()

def main():
    config_path = "finetune_models/config.yaml"
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°é…ç½®æª”æ¡ˆ: {config_path}")
    
    config = OmegaConf.load(config_path)
    bpe_model_path = os.path.join(config.train.finetune_model_dir, config.dataset.bpe_model)

    train_ds, valid_ds = load_finetune_datasets(config, bpe_model_path) 
    train_ds = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_finetune_fn, num_workers=4)
    valid_ds = DataLoader(valid_ds, batch_size=8, shuffle=False, collate_fn=collate_finetune_fn, num_workers=2)

    trainer = Trainer(config)
    trainer.train(train_ds, valid_ds)
    logger.info("UnifiedVoice å¾®èª¿æµç¨‹çµæŸã€‚")


if __name__ == "__main__":
    main()

