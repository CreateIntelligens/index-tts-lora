# =============================================================================
# Index-TTS 推理精度配置文件
# 此文件控制推理時的模型精度，優先級高於 config.yaml
# =============================================================================

inference:
  # ============================================================================
  # 標準混合精度配置（適用於各種 GPU）
  # ============================================================================
  gpt: "bf16"        # 選項: bf16 | fp16 | fp32
  vocoder: "bf16"    # 選項: bf16 | fp16 | fp32

  # 量化配置（省顯存用）
  quantization:
    enabled: false            # 大顯存 GPU 不需要（如 H100/A100）
    weight_dtype: "int8"      # int8（省75%）| int4（省87.5%）
    compute_dtype: "bf16"     # bf16 | fp16 | fp32

# =============================================================================
# 使用範例：根據您的 GPU 選擇合適的配置
# =============================================================================

# -----------------------------------------------------------------------------
# 範例 1: H100/A100 80GB（大顯存，高性能）
# -----------------------------------------------------------------------------
# inference:
#   gpt: "bf16"
#   vocoder: "bf16"
#   quantization:
#     enabled: false   # 顯存充足，不需要量化

# -----------------------------------------------------------------------------
# 範例 2: RTX 3090/4090 24GB（標準配置）
# -----------------------------------------------------------------------------
# inference:
#   gpt: "bf16"
#   vocoder: "bf16"
#   quantization:
#     enabled: false

# -----------------------------------------------------------------------------
# 範例 3: RTX 3060/4060 Ti 12GB（低顯存）
# -----------------------------------------------------------------------------
# inference:
#   gpt: "bf16"
#   vocoder: "bf16"
#   quantization:
#     enabled: true
#     weight_dtype: "int8"    # 省 75% GPT 顯存
#     compute_dtype: "bf16"

# -----------------------------------------------------------------------------
# 範例 4: RTX 3050 8GB（極致省顯存）
# -----------------------------------------------------------------------------
# inference:
#   gpt: "fp16"
#   vocoder: "fp16"
#   quantization:
#     enabled: true
#     weight_dtype: "int4"    # 省 87.5% GPT 顯存
#     compute_dtype: "fp16"

# -----------------------------------------------------------------------------
# 範例 5: 舊 GPU / CPU（FP32 模式）
# -----------------------------------------------------------------------------
# inference:
#   gpt: "fp32"
#   vocoder: "fp32"
#   quantization:
#     enabled: false

# =============================================================================
# 精度說明
# =============================================================================
#
# 1. BF16 (bfloat16):
#    - 優點: 動態範圍大，訓練和推理都穩定，速度快（需 Ampere+ GPU）
#    - 缺點: 精度略低於 FP16（但實際影響很小）
#    - 推薦: RTX 30/40 系列, A100, H100
#
# 2. FP16 (float16):
#    - 優點: 精度高於 BF16，速度快，兼容性好
#    - 缺點: 動態範圍小，訓練時需要 GradScaler
#    - 推薦: RTX 20 系列及更早的 GPU
#
# 3. FP32 (float32):
#    - 優點: 最高精度
#    - 缺點: 速度慢，顯存消耗大（約 BF16 的 2 倍）
#    - 推薦: 僅用於品質測試或 CPU 推理
#
# 4. INT8 量化:
#    - 優點: 省 ~75% GPT 權重顯存，速度與 BF16 相當
#    - 缺點: 略微損失精度（通常 < 1% 品質下降）
#    - 推薦: 12GB 顯存的 GPU
#
# 5. INT4 量化:
#    - 優點: 省 ~87.5% GPT 權重顯存
#    - 缺點: 精度損失較明顯（約 2-5% 品質下降）
#    - 推薦: 8GB 顯存的 GPU，或需要極致省顯存時
#
# =============================================================================
# 與訓練精度的關係
# =============================================================================
#
# 訓練精度 (config.yaml train.mixed_precision) 只影響訓練時的運算，
# 模型權重始終以 FP32 存儲。推理時可以自由選擇精度：
#
# - 訓練用 BF16 → 推理可用 BF16（無損）或 INT8（省顯存，略損精度）
# - 訓練用 FP16 → 推理可用 FP16（無損）或 INT8（省顯存，略損精度）
# - 訓練用 FP32 → 推理可用 BF16/FP16（可能略損精度，需測試）或 FP32（無損）
#
# 建議: 訓練和推理使用相同精度（如都用 BF16），或推理使用量化版本（省顯存）
#
# =============================================================================
