#!/bin/bash

# è¨“ç·´æ¨¡å‹è…³æœ¬

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/lib_common.sh"

# TensorBoard å•Ÿå‹•å‡½æ•¸
start_tensorboard() {
    local port="${1:-7859}"
    local logdir="${2:-logs}"

    print_header "å•Ÿå‹• TensorBoard"
    print_info "Log ç›®éŒ„: $logdir"
    print_info "Port: $port"

    check_container

    if [ "$USE_DOCKER" -eq 1 ]; then
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰ TensorBoard åœ¨é‹è¡Œ
        local existing=$(docker compose exec -T index-tts-lora pgrep -f "tensorboard" 2>/dev/null)
        if [ -n "$existing" ]; then
            print_warning "TensorBoard å·²åœ¨é‹è¡Œä¸­ (PID: $existing)"
            print_info "å­˜å–ç¶²å€: http://localhost:$port"
            return 0
        fi

        print_info "åœ¨ Docker å®¹å™¨å…§å•Ÿå‹• TensorBoard..."
        docker compose exec -d index-tts-lora tensorboard \
            --logdir="/workspace/index-tts-lora/$logdir" \
            --host=0.0.0.0 \
            --port="$port"
    else
        local existing=$(pgrep -f "tensorboard")
        if [ -n "$existing" ]; then
            print_warning "TensorBoard å·²åœ¨é‹è¡Œä¸­ (PID: $existing)"
            print_info "å­˜å–ç¶²å€: http://localhost:$port"
            return 0
        fi

        tensorboard --logdir="$logdir" --host=0.0.0.0 --port="$port" &
    fi

    sleep 2
    print_success "TensorBoard å·²å•Ÿå‹•"
    print_info "å­˜å–ç¶²å€: http://localhost:$port"
}

# åœæ­¢ TensorBoard
stop_tensorboard() {
    print_header "åœæ­¢ TensorBoard"

    check_container

    if [ "$USE_DOCKER" -eq 1 ]; then
        docker compose exec -T index-tts-lora pkill -f "tensorboard" 2>/dev/null || true
    else
        pkill -f "tensorboard" 2>/dev/null || true
    fi

    print_success "TensorBoard å·²åœæ­¢"
}

train_model() {
    print_header "é–‹å§‹è¨“ç·´æ¨¡å‹"

    local mode="auto"  # auto, ddp, dp
    local num_gpus=""

    while [ $# -gt 0 ]; do
        case "$1" in
            --ddp)
                mode="ddp"
                ;;
            --dp)
                mode="dp"
                ;;
            --gpus)
                shift
                num_gpus="$1"
                ;;
            *)
                print_warning "æœªçŸ¥åƒæ•¸: $1"
                ;;
        esac
        shift
    done

    check_container

    # å»ºç«‹ log ç›®éŒ„
    RUN_NAME="train_$(date +%Y%m%d_%H%M%S)"
    LOG_DIR="logs/${RUN_NAME}"
    mkdir -p "$LOG_DIR"
    LOG_FILE="$LOG_DIR/train.log"
    print_info "è¨“ç·´ log å°‡å„²å­˜è‡³: $LOG_FILE"

    # è‡ªå‹•æª¢æ¸¬ GPU æ•¸é‡ï¼ˆä½¿ç”¨ PyTorchï¼Œå°Šé‡ CUDA_VISIBLE_DEVICESï¼‰
    if [ -z "$num_gpus" ]; then
        if [ "$USE_DOCKER" -eq 1 ]; then
            num_gpus=$(docker compose exec index-tts-lora python3 -c "import torch; print(torch.cuda.device_count())" 2>/dev/null)
        else
            num_gpus=$(python3 -c "import torch; print(torch.cuda.device_count())" 2>/dev/null)
        fi

        if [ -z "$num_gpus" ] || [ "$num_gpus" -eq 0 ]; then
            print_error "æ‰¾ä¸åˆ°å¯ç”¨çš„ GPU"
            exit 1
        fi
    fi

    # è‡ªå‹•é¸æ“‡è¨“ç·´æ¨¡å¼
    if [ "$mode" = "auto" ]; then
        if [ "$num_gpus" -gt 1 ]; then
            mode="ddp"
            print_info "ğŸš€ æª¢æ¸¬åˆ° $num_gpus å€‹ GPUï¼Œè‡ªå‹•ä½¿ç”¨ DDP è¨“ç·´"
        else
            mode="dp"
            print_info "ğŸ“Œ æª¢æ¸¬åˆ° $num_gpus å€‹ GPUï¼Œä½¿ç”¨ DataParallel è¨“ç·´"
        fi
    fi

    if [ "$mode" = "ddp" ]; then
        print_info "ä½¿ç”¨ DDP è¨“ç·´ï¼ŒGPU æ•¸é‡: $num_gpus"

        export NCCL_ASYNC_ERROR_HANDLING=1
        export NCCL_BLOCKING_WAIT=1
        export NCCL_DEBUG=INFO
        export RUN_NAME

        if [ "$USE_DOCKER" -eq 1 ]; then
            # ä½¿ç”¨ docker exec ä¸¦åŒæ™‚å°å‘å®¹å™¨çš„ stdoutï¼ˆæœƒå‡ºç¾åœ¨ docker logsï¼‰
            docker compose exec -T index-tts-lora bash -c "
                export RUN_NAME='$RUN_NAME'
                export RUN_LOG_DIR='/workspace/index-tts-lora/$LOG_DIR'
                python3 -m torch.distributed.run \
                --nproc_per_node=$num_gpus \
                train_ddp.py 2>&1 | tee /workspace/index-tts-lora/$LOG_FILE | tee /proc/1/fd/1
            " 2>&1 | tee "$LOG_FILE"
        else
            RUN_NAME="$RUN_NAME" RUN_LOG_DIR="$LOG_DIR" python3 -m torch.distributed.run \
                --nproc_per_node="$num_gpus" \
                train_ddp.py 2>&1 | tee "$LOG_FILE"
        fi
    else
        print_info "ä½¿ç”¨ DataParallel è¨“ç·´"

        if [ "$USE_DOCKER" -eq 1 ]; then
            docker compose exec -T index-tts-lora bash -c "
                export RUN_NAME='$RUN_NAME'
                export RUN_LOG_DIR='/workspace/index-tts-lora/$LOG_DIR'
                python3 train.py 2>&1 | tee /workspace/index-tts-lora/$LOG_FILE | tee /proc/1/fd/1
            " 2>&1 | tee "$LOG_FILE"
        else
            RUN_NAME="$RUN_NAME" RUN_LOG_DIR="$LOG_DIR" python3 train.py 2>&1 | tee "$LOG_FILE"
        fi
    fi

    # ä¿®å¾© log æª”æ¡ˆæ¬Šé™
    if [ -f "$LOG_FILE" ]; then
        # å–å¾—å®¿ä¸»æ©Ÿç”¨æˆ¶ UID/GID
        HOST_UID=$(stat -c '%u' docker-compose.yml 2>/dev/null || echo "1000")
        HOST_GID=$(stat -c '%g' docker-compose.yml 2>/dev/null || echo "1000")
        chown -R $HOST_UID:$HOST_GID "$LOG_DIR" 2>/dev/null || true
    fi

    # å˜—è©¦ä¿®å¾©è¨“ç·´è¼¸å‡ºç›®éŒ„æ¬Šé™ï¼ˆcheckpoints ç­‰ï¼‰
    if [ -d "finetune_models" ]; then
        HOST_UID=${HOST_UID:-$(id -u)}
        HOST_GID=${HOST_GID:-$(id -g)}
        chown -R $HOST_UID:$HOST_GID finetune_models 2>/dev/null || true
    fi

    if [ $? -eq 0 ]; then
        print_success "è¨“ç·´å®Œæˆï¼"
        print_info "Log æª”æ¡ˆ: $LOG_FILE"
    else
        print_error "è¨“ç·´å¤±æ•—ï¼"
        print_info "Log æª”æ¡ˆ: $LOG_FILE"
        exit 1
    fi
}

show_usage() {
    echo "ç”¨æ³•: $0 <command> [options]"
    echo ""
    echo "Commands:"
    echo "  train [--ddp|--dp] [--gpus N]  é–‹å§‹è¨“ç·´æ¨¡å‹"
    echo "  tensorboard [--port PORT]      å•Ÿå‹• TensorBoard"
    echo "  tensorboard-stop               åœæ­¢ TensorBoard"
    echo ""
    echo "Examples:"
    echo "  $0 train                       è‡ªå‹•é¸æ“‡è¨“ç·´æ¨¡å¼"
    echo "  $0 train --ddp --gpus 4        ä½¿ç”¨ 4 å€‹ GPU é€²è¡Œ DDP è¨“ç·´"
    echo "  $0 tensorboard                 å•Ÿå‹• TensorBoard (é è¨­ port 7859)"
    echo "  $0 tensorboard --port 8080     æŒ‡å®š port å•Ÿå‹• TensorBoard"
    echo "  $0 tensorboard-stop            åœæ­¢ TensorBoard"
}

# ç›´æ¥åŸ·è¡Œæ™‚èª¿ç”¨
if [ "${BASH_SOURCE[0]}" == "${0}" ]; then
    case "${1:-}" in
        train)
            shift
            train_model "$@"
            ;;
        tensorboard)
            shift
            port="7859"
            logdir="logs"
            while [ $# -gt 0 ]; do
                case "$1" in
                    --port)
                        shift
                        port="$1"
                        ;;
                    --logdir)
                        shift
                        logdir="$1"
                        ;;
                    *)
                        print_warning "æœªçŸ¥åƒæ•¸: $1"
                        ;;
                esac
                shift
            done
            start_tensorboard "$port" "$logdir"
            ;;
        tensorboard-stop)
            stop_tensorboard
            ;;
        -h|--help|help)
            show_usage
            ;;
        "")
            # é è¨­è¡Œç‚ºï¼šé–‹å§‹è¨“ç·´
            train_model "$@"
            ;;
        *)
            print_error "æœªçŸ¥æŒ‡ä»¤: $1"
            show_usage
            exit 1
            ;;
    esac
fi
