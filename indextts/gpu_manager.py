#!/usr/bin/env python3
"""
GPU ç®¡ç†å™¨ - è‡ªå‹•åµæ¸¬èˆ‡åˆ†é… GPU è³‡æºã€‚

åŠŸèƒ½ï¼š
- è‡ªå‹•åµæ¸¬å¯ç”¨ GPU æ•¸é‡èˆ‡ç‹€æ…‹
- æ™ºæ…§åˆ†é… GPU çµ¦ä¸åŒçš„ Worker
- ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
- è² è¼‰å¹³è¡¡
"""

import os
import logging
from typing import List, Optional, Dict
from dataclasses import dataclass

try:
    import torch
except ImportError:
    print("[éŒ¯èª¤] è«‹å®‰è£ PyTorch: pip install torch")
    raise


@dataclass
class GPUInfo:
    """GPU è³‡è¨Šé¡åˆ¥"""
    index: int
    name: str
    total_memory: float  # GB
    available_memory: float  # GB
    utilization: float  # 0-100%


class GPUManager:
    """GPU è³‡æºç®¡ç†å™¨"""

    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.gpu_count = 0
        self.gpu_info: List[GPUInfo] = []
        self.enabled_gpus: List[int] = []

        self._detect_gpus()

    def _detect_gpus(self):
        """åµæ¸¬å¯ç”¨çš„ GPU"""
        if not torch.cuda.is_available():
            self.logger.warning("[è­¦å‘Š] æœªåµæ¸¬åˆ° CUDA GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
            return

        cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', None)
        if cuda_visible:
            self.logger.info(f"[è³‡è¨Š] CUDA_VISIBLE_DEVICES è¨­å®š: {cuda_visible}")

        self.gpu_count = torch.cuda.device_count()
        self.logger.info(f"[è³‡è¨Š] åµæ¸¬åˆ° {self.gpu_count} å€‹ GPU")

        for i in range(self.gpu_count):
            try:
                props = torch.cuda.get_device_properties(i)
                name = props.name
                total_memory = props.total_memory / (1024**3)

                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                available = total_memory - reserved

                gpu_info = GPUInfo(
                    index=i,
                    name=name,
                    total_memory=total_memory,
                    available_memory=available,
                    utilization=0.0
                )

                self.gpu_info.append(gpu_info)
                self.enabled_gpus.append(i)

                self.logger.info(
                    f"  GPU {i}: {name} | "
                    f"è¨˜æ†¶é«”: {total_memory:.1f}GB (å¯ç”¨: {available:.1f}GB)"
                )

            except Exception as e:
                self.logger.warning(f"[è­¦å‘Š] ç„¡æ³•è®€å– GPU {i} è³‡è¨Š: {e}")

    def get_gpu_count(self) -> int:
        """ç²å–å¯ç”¨ GPU æ•¸é‡"""
        return self.gpu_count

    def get_gpu_info(self, gpu_id: int) -> Optional[GPUInfo]:
        """ç²å–æŒ‡å®š GPU çš„è³‡è¨Š"""
        if 0 <= gpu_id < len(self.gpu_info):
            return self.gpu_info[gpu_id]
        return None

    def get_all_gpu_info(self) -> List[GPUInfo]:
        """ç²å–æ‰€æœ‰ GPU è³‡è¨Š"""
        return self.gpu_info.copy()

    def assign_gpu_to_worker(self, worker_id: int) -> int:
        """
        ç‚º Worker åˆ†é… GPUã€‚

        ç­–ç•¥ï¼šè¼ªè©¢åˆ†é… (Round-Robin)ã€‚

        Args:
            worker_id: Worker ç·¨è™Ÿã€‚

        Returns:
            åˆ†é…çš„ GPU IDã€‚
        """
        if self.gpu_count == 0:
            return -1

        gpu_id = self.enabled_gpus[worker_id % self.gpu_count]
        self.logger.debug(f"[åˆ†é…] Worker {worker_id} -> GPU {gpu_id}")
        return gpu_id

    def get_optimal_worker_count(self,
                                  memory_per_worker: float = 4.0,
                                  max_workers: Optional[int] = None) -> int:
        """
        è¨ˆç®—æœ€ä½³ Worker æ•¸é‡ã€‚

        Args:
            memory_per_worker: æ¯å€‹ Worker é è¨ˆä½¿ç”¨çš„è¨˜æ†¶é«” (GB)ã€‚
            max_workers: æœ€å¤§ Worker æ•¸é‡é™åˆ¶ã€‚

        Returns:
            å»ºè­°çš„ Worker æ•¸é‡ã€‚
        """
        if self.gpu_count == 0:
            import multiprocessing
            cpu_count = multiprocessing.cpu_count()
            workers = max(1, cpu_count // 2)
            self.logger.info(f"[å»ºè­°] CPU æ¨¡å¼ï¼šå»ºè­° {workers} å€‹ Worker")
            return workers

        workers_per_gpu = []
        for gpu in self.gpu_info:
            available_workers = max(1, int(gpu.available_memory / memory_per_worker))
            workers_per_gpu.append(available_workers)

        total_workers = sum(workers_per_gpu)

        if max_workers is not None:
            total_workers = min(total_workers, max_workers)

        self.logger.info(f"[åˆ†æ] GPU è³‡æºåˆ†æï¼š")
        for i, (gpu, workers) in enumerate(zip(self.gpu_info, workers_per_gpu)):
            self.logger.info(
                f"  GPU {i} ({gpu.name}): "
                f"{gpu.available_memory:.1f}GB å¯ç”¨ -> {workers} Workers"
            )
        self.logger.info(f"[å»ºè­°] ç¸½ Worker æ•¸: {total_workers}")

        return total_workers

    def get_device_string(self, gpu_id: int) -> str:
        """
        ç²å– PyTorch Device å­—ä¸²ã€‚

        Args:
            gpu_id: GPU ID (-1 è¡¨ç¤º CPU)ã€‚

        Returns:
            Device å­—ä¸² (ä¾‹å¦‚: "cuda:0", "cpu")ã€‚
        """
        if gpu_id < 0 or self.gpu_count == 0:
            return "cpu"
        return f"cuda:{gpu_id}"

    def set_cuda_visible_devices(self, gpu_ids: List[int]):
        """
        è¨­å®š CUDA_VISIBLE_DEVICES ç’°å¢ƒè®Šæ•¸ã€‚

        æ³¨æ„ï¼šå¿…é ˆåœ¨åˆå§‹åŒ– CUDA å‰èª¿ç”¨æ‰æœ‰æ•ˆã€‚

        Args:
            gpu_ids: è¦ä½¿ç”¨çš„ GPU ID åˆ—è¡¨ã€‚
        """
        gpu_str = ",".join(map(str, gpu_ids))
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_str
        self.logger.info(f"[è¨­å®š] CUDA_VISIBLE_DEVICES={gpu_str}")

    def print_summary(self):
        """åˆ—å° GPU ç‹€æ…‹æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ® GPU è³‡æºæ‘˜è¦")
        print("="*60)

        if self.gpu_count == 0:
            print("[è­¦å‘Š] æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
            return

        print(f"ğŸ“Š å¯ç”¨ GPU æ•¸é‡: {self.gpu_count}")
        print()

        for gpu in self.gpu_info:
            print(f"GPU {gpu.index}: {gpu.name}")
            print(f"  â””â”€ ç¸½è¨˜æ†¶é«”: {gpu.total_memory:.1f} GB")
            print(f"  â””â”€ å¯ç”¨è¨˜æ†¶é«”: {gpu.available_memory:.1f} GB")
            print()

        print("="*60 + "\n")


def get_global_gpu_manager() -> GPUManager:
    """ç²å–å…¨åŸŸ GPU ç®¡ç†å™¨å¯¦ä¾‹ (å–®ä¾‹æ¨¡å¼)"""
    if not hasattr(get_global_gpu_manager, '_instance'):
        get_global_gpu_manager._instance = GPUManager()
    return get_global_gpu_manager._instance


def get_available_gpu_count() -> int:
    """å¿«é€Ÿç²å–å¯ç”¨ GPU æ•¸é‡"""
    manager = get_global_gpu_manager()
    return manager.get_gpu_count()


def assign_worker_gpu(worker_id: int) -> str:
    """å¿«é€Ÿç‚º Worker åˆ†é… GPU ä¸¦è¿”å› Device å­—ä¸²"""
    manager = get_global_gpu_manager()
    gpu_id = manager.assign_gpu_to_worker(worker_id)
    return manager.get_device_string(gpu_id)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    manager = GPUManager()
    manager.print_summary()

    if manager.get_gpu_count() > 0:
        print("ğŸ§ª æ¸¬è©¦ Worker åˆ†é…ï¼š")
        for i in range(10):
            device = assign_worker_gpu(i)
            print(f"  Worker {i} -> {device}")

        print()
        optimal = manager.get_optimal_worker_count(memory_per_worker=4.0)
        print(f"ğŸ’¡ å»ºè­° Worker æ•¸é‡: {optimal}")
