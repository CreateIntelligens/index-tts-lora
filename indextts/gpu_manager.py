#!/usr/bin/env python3
"""
GPU ç®¡ç†å™¨ - è‡ªå‹•åµæ¸¬å’Œåˆ†é…å¤š GPU è³‡æº

åŠŸèƒ½ï¼š
- è‡ªå‹•åµæ¸¬å¯ç”¨ GPU æ•¸é‡å’Œç‹€æ…‹
- æ™ºèƒ½åˆ†é… GPU çµ¦ä¸åŒ worker
- è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
- è² è¼‰å‡è¡¡

Author: TTS ETL Pipeline
Version: 1.0
"""

import os
import logging
from typing import List, Optional, Dict
from dataclasses import dataclass

try:
    import torch
except ImportError:
    print("âŒ è«‹å®‰è£ PyTorch: pip install torch")
    raise


@dataclass
class GPUInfo:
    """GPU è³‡è¨Š"""
    index: int
    name: str
    total_memory: float  # GB
    available_memory: float  # GB
    utilization: float  # 0-100%


class GPUManager:
    """GPU è³‡æºç®¡ç†å™¨"""

    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.gpu_count = 0
        self.gpu_info: List[GPUInfo] = []
        self.enabled_gpus: List[int] = []

        self._detect_gpus()

    def _detect_gpus(self):
        """åµæ¸¬å¯ç”¨çš„ GPU"""
        if not torch.cuda.is_available():
            self.logger.warning("âš ï¸  æœªåµæ¸¬åˆ° CUDA GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
            return

        # æª¢æŸ¥æ˜¯å¦æœ‰ CUDA_VISIBLE_DEVICES é™åˆ¶
        cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', None)
        if cuda_visible:
            self.logger.info(f"ğŸ”§ CUDA_VISIBLE_DEVICES è¨­å®š: {cuda_visible}")

        self.gpu_count = torch.cuda.device_count()
        self.logger.info(f"ğŸ® åµæ¸¬åˆ° {self.gpu_count} å€‹ GPU")

        # æ”¶é›†æ¯å€‹ GPU çš„è³‡è¨Š
        for i in range(self.gpu_count):
            try:
                props = torch.cuda.get_device_properties(i)
                name = props.name
                total_memory = props.total_memory / (1024**3)  # è½‰æ›ç‚º GB

                # å˜—è©¦ç²å–ç•¶å‰å¯ç”¨è¨˜æ†¶é«”
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                available = total_memory - reserved

                gpu_info = GPUInfo(
                    index=i,
                    name=name,
                    total_memory=total_memory,
                    available_memory=available,
                    utilization=0.0  # PyTorch æ²’æœ‰ç›´æ¥çš„åˆ©ç”¨ç‡ API
                )

                self.gpu_info.append(gpu_info)
                self.enabled_gpus.append(i)

                self.logger.info(
                    f"  GPU {i}: {name} | "
                    f"è¨˜æ†¶é«”: {total_memory:.1f}GB (å¯ç”¨: {available:.1f}GB)"
                )

            except Exception as e:
                self.logger.warning(f"âš ï¸  ç„¡æ³•è®€å– GPU {i} è³‡è¨Š: {e}")

    def get_gpu_count(self) -> int:
        """ç²å–å¯ç”¨ GPU æ•¸é‡"""
        return self.gpu_count

    def get_gpu_info(self, gpu_id: int) -> Optional[GPUInfo]:
        """ç²å–æŒ‡å®š GPU çš„è³‡è¨Š"""
        if 0 <= gpu_id < len(self.gpu_info):
            return self.gpu_info[gpu_id]
        return None

    def get_all_gpu_info(self) -> List[GPUInfo]:
        """ç²å–æ‰€æœ‰ GPU è³‡è¨Š"""
        return self.gpu_info.copy()

    def assign_gpu_to_worker(self, worker_id: int) -> int:
        """
        ç‚º worker åˆ†é… GPU

        ç­–ç•¥ï¼šè¼ªè©¢åˆ†é… (round-robin)

        Args:
            worker_id: Worker ç·¨è™Ÿ

        Returns:
            åˆ†é…çš„ GPU ID
        """
        if self.gpu_count == 0:
            return -1  # ç„¡ GPU å¯ç”¨

        gpu_id = self.enabled_gpus[worker_id % self.gpu_count]
        self.logger.debug(f"ğŸ”§ Worker {worker_id} â†’ GPU {gpu_id}")
        return gpu_id

    def get_optimal_worker_count(self,
                                  memory_per_worker: float = 4.0,
                                  max_workers: Optional[int] = None) -> int:
        """
        è¨ˆç®—æœ€ä½³ worker æ•¸é‡

        Args:
            memory_per_worker: æ¯å€‹ worker é è¨ˆä½¿ç”¨çš„è¨˜æ†¶é«” (GB)
            max_workers: æœ€å¤§ worker æ•¸é‡é™åˆ¶

        Returns:
            å»ºè­°çš„ worker æ•¸é‡
        """
        if self.gpu_count == 0:
            # CPU æ¨¡å¼ï¼šåŸºæ–¼ CPU æ ¸å¿ƒæ•¸
            import multiprocessing
            cpu_count = multiprocessing.cpu_count()
            workers = max(1, cpu_count // 2)
            self.logger.info(f"ğŸ’¡ CPU æ¨¡å¼ï¼šå»ºè­° {workers} å€‹ worker")
            return workers

        # è¨ˆç®—æ¯å€‹ GPU å¯ä»¥æ”¯æ´çš„ worker æ•¸
        workers_per_gpu = []
        for gpu in self.gpu_info:
            available_workers = max(1, int(gpu.available_memory / memory_per_worker))
            workers_per_gpu.append(available_workers)

        # ç¸½ worker æ•¸ = æ¯å€‹ GPU çš„ worker æ•¸ç¸½å’Œ
        total_workers = sum(workers_per_gpu)

        # æ‡‰ç”¨æœ€å¤§é™åˆ¶
        if max_workers is not None:
            total_workers = min(total_workers, max_workers)

        self.logger.info(f"ğŸ“Š GPU è³‡æºåˆ†æï¼š")
        for i, (gpu, workers) in enumerate(zip(self.gpu_info, workers_per_gpu)):
            self.logger.info(
                f"  GPU {i} ({gpu.name}): "
                f"{gpu.available_memory:.1f}GB å¯ç”¨ â†’ {workers} workers"
            )
        self.logger.info(f"ğŸ’¡ å»ºè­°ç¸½ worker æ•¸: {total_workers}")

        return total_workers

    def get_device_string(self, gpu_id: int) -> str:
        """
        ç²å– PyTorch device å­—ä¸²

        Args:
            gpu_id: GPU ID (-1 è¡¨ç¤º CPU)

        Returns:
            device å­—ä¸² (ä¾‹å¦‚: "cuda:0", "cpu")
        """
        if gpu_id < 0 or self.gpu_count == 0:
            return "cpu"
        return f"cuda:{gpu_id}"

    def set_cuda_visible_devices(self, gpu_ids: List[int]):
        """
        è¨­å®š CUDA_VISIBLE_DEVICES ç’°å¢ƒè®Šæ•¸

        æ³¨æ„ï¼šå¿…é ˆåœ¨åˆå§‹åŒ– CUDA å‰èª¿ç”¨æ‰æœ‰æ•ˆ

        Args:
            gpu_ids: è¦ä½¿ç”¨çš„ GPU ID åˆ—è¡¨
        """
        gpu_str = ",".join(map(str, gpu_ids))
        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_str
        self.logger.info(f"ğŸ”§ è¨­å®š CUDA_VISIBLE_DEVICES={gpu_str}")

    def print_summary(self):
        """æ‰“å° GPU ç‹€æ…‹æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ® GPU è³‡æºæ‘˜è¦")
        print("="*60)

        if self.gpu_count == 0:
            print("âš ï¸  æœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
            return

        print(f"ğŸ“Š å¯ç”¨ GPU æ•¸é‡: {self.gpu_count}")
        print()

        for gpu in self.gpu_info:
            print(f"GPU {gpu.index}: {gpu.name}")
            print(f"  â””â”€ ç¸½è¨˜æ†¶é«”: {gpu.total_memory:.1f} GB")
            print(f"  â””â”€ å¯ç”¨è¨˜æ†¶é«”: {gpu.available_memory:.1f} GB")
            print()

        print("="*60 + "\n")


def get_global_gpu_manager() -> GPUManager:
    """ç²å–å…¨å±€ GPU ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    if not hasattr(get_global_gpu_manager, '_instance'):
        get_global_gpu_manager._instance = GPUManager()
    return get_global_gpu_manager._instance


# ä¾¿æ·å‡½æ•¸
def get_available_gpu_count() -> int:
    """å¿«é€Ÿç²å–å¯ç”¨ GPU æ•¸é‡"""
    manager = get_global_gpu_manager()
    return manager.get_gpu_count()


def assign_worker_gpu(worker_id: int) -> str:
    """å¿«é€Ÿç‚º worker åˆ†é… GPU ä¸¦è¿”å› device å­—ä¸²"""
    manager = get_global_gpu_manager()
    gpu_id = manager.assign_gpu_to_worker(worker_id)
    return manager.get_device_string(gpu_id)


if __name__ == "__main__":
    # æ¸¬è©¦ GPU ç®¡ç†å™¨
    logging.basicConfig(level=logging.INFO)

    manager = GPUManager()
    manager.print_summary()

    # æ¸¬è©¦ worker åˆ†é…
    if manager.get_gpu_count() > 0:
        print("ğŸ§ª æ¸¬è©¦ Worker åˆ†é…ï¼š")
        for i in range(10):
            device = assign_worker_gpu(i)
            print(f"  Worker {i} â†’ {device}")

        print()
        optimal = manager.get_optimal_worker_count(memory_per_worker=4.0)
        print(f"ğŸ’¡ å»ºè­° worker æ•¸é‡: {optimal}")
