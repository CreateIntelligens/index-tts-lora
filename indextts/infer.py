import json  # æ–°å¢é€™è¡Œ
import os
import sys
import time
import warnings
from subprocess import CalledProcessError
from typing import Dict, List, Tuple, Optional

import torch
import torch.nn as nn
import torchaudio
import soundfile as sf
from omegaconf import OmegaConf
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import random

import numpy as np

from indextts.BigVGAN.models import BigVGAN as Generator
from indextts.gpt.model import UnifiedVoice
from indextts.utils.checkpoint import load_checkpoint
from indextts.utils.feature_extractors import MelSpectrogramFeatures
from indextts.utils.front import TextNormalizer, TextTokenizer


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def _get_parent_module(model: nn.Module, name: str) -> nn.Module:
    """
    ç²å–æ¨¡å‹ä¸­æŒ‡å®šåç¨±çš„çˆ¶æ¨¡çµ„ã€‚
    
    Args:
        model: æ ¹æ¨¡å‹
        name: å®Œæ•´çš„æ¨¡çµ„è·¯å¾‘åç¨±ï¼ˆå¦‚ 'gpt.h.0.attn.c_attn'ï¼‰
    
    Returns:
        çˆ¶æ¨¡çµ„
    """
    parts = name.split('.')
    parent = model
    for part in parts[:-1]:
        parent = getattr(parent, part)
    return parent


def _quantize_linear_layers_to_int8(model: nn.Module, target_modules: Optional[List[str]] = None, verbose: bool = True) -> int:
    """
    å°‡æ¨¡å‹ä¸­çš„ nn.Linear å±¤æ›¿æ›ç‚º bitsandbytes çš„ Linear8bitLtã€‚
    
    Args:
        model: è¦é‡åŒ–çš„æ¨¡å‹
        target_modules: è¦é‡åŒ–çš„æ¨¡çµ„åç¨±åˆ—è¡¨ã€‚å¦‚æœç‚º Noneï¼Œå‰‡é‡åŒ–æ‰€æœ‰ Linear å±¤ã€‚
                       ä¾‹å¦‚: ['gpt', 'text_head', 'mel_head']
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
    
    Returns:
        æ›¿æ›çš„å±¤æ•¸
    
    Note:
        æ­¤å‡½æ•¸æœƒå°±åœ°ä¿®æ”¹æ¨¡å‹ã€‚
    """
    try:
        import bitsandbytes as bnb
    except ImportError:
        raise ImportError(
            "bitsandbytes æœªå®‰è£ã€‚è«‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£ï¼š\n"
            "pip install bitsandbytes"
        )
    
    replaced_count = 0
    total_params_before = 0
    total_params_after = 0
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ›¿æ›çš„æ¨¡çµ„
    modules_to_replace = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # æª¢æŸ¥æ˜¯å¦åœ¨ç›®æ¨™æ¨¡çµ„åˆ—è¡¨ä¸­
            if target_modules is not None:
                should_replace = any(name.startswith(target) or name == target for target in target_modules)
                if not should_replace:
                    continue
            
            modules_to_replace.append((name, module))
    
    if verbose:
        print(f">> [é‡åŒ–] æ‰¾åˆ° {len(modules_to_replace)} å€‹å¯é‡åŒ–çš„ Linear å±¤")
    
    # æ›¿æ›æ¨¡çµ„
    for name, module in modules_to_replace:
        parent = _get_parent_module(model, name)
        child_name = name.split('.')[-1]
        
        # è¨ˆç®—åƒæ•¸é‡ï¼ˆç”¨æ–¼é¡¯å­˜ä¼°ç®—ï¼‰
        param_count = module.in_features * module.out_features
        total_params_before += param_count * 4  # FP32 = 4 bytes
        total_params_after += param_count * 1   # INT8 = 1 byte
        
        # å‰µå»º 8bit Linear å±¤
        has_bias = module.bias is not None
        quantized_linear = bnb.nn.Linear8bitLt(
            module.in_features,
            module.out_features,
            bias=has_bias,
            has_fp16_weights=False,
            threshold=6.0,  # é›¢ç¾¤å€¼é–¾å€¼
        )
        
        # è¤‡è£½æ¬Šé‡ï¼ˆbitsandbytes æœƒè‡ªå‹•é‡åŒ–ï¼‰
        quantized_linear.weight = bnb.nn.Int8Params(
            module.weight.data.contiguous(),
            requires_grad=False
        )
        if has_bias:
            quantized_linear.bias = nn.Parameter(module.bias.data.clone())
        
        # æ›¿æ›æ¨¡çµ„
        setattr(parent, child_name, quantized_linear)
        replaced_count += 1
        
        if verbose and replaced_count <= 5:
            print(f">>   - é‡åŒ–: {name} ({module.in_features}x{module.out_features})")
    
    if verbose and replaced_count > 5:
        print(f">>   - ... é‚„æœ‰ {replaced_count - 5} å€‹å±¤")
    
    if verbose:
        mem_before_mb = total_params_before / (1024 * 1024)
        mem_after_mb = total_params_after / (1024 * 1024)
        savings_pct = (1 - total_params_after / total_params_before) * 100 if total_params_before > 0 else 0
        print(f">> [é‡åŒ–] æ¬Šé‡è¨˜æ†¶é«”: {mem_before_mb:.1f}MB â†’ {mem_after_mb:.1f}MB (ç¯€çœ {savings_pct:.0f}%)")
    
    return replaced_count


def _quantize_linear_layers_to_int4(model: nn.Module, target_modules: Optional[List[str]] = None, verbose: bool = True, compute_dtype: torch.dtype = torch.bfloat16) -> int:
    """
    å°‡æ¨¡å‹ä¸­çš„ nn.Linear å±¤æ›¿æ›ç‚º bitsandbytes çš„ Linear4bit (NF4)ã€‚

    Args:
        model: è¦é‡åŒ–çš„æ¨¡å‹
        target_modules: è¦é‡åŒ–çš„æ¨¡çµ„åç¨±åˆ—è¡¨
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        compute_dtype: é‡åŒ–å±¤çš„é‹ç®—ç²¾åº¦ï¼ˆé è¨­ BF16ï¼‰

    Returns:
        æ›¿æ›çš„å±¤æ•¸
    """
    try:
        import bitsandbytes as bnb
    except ImportError:
        raise ImportError(
            "bitsandbytes æœªå®‰è£ã€‚è«‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£ï¼š\n"
            "pip install bitsandbytes"
        )
    
    replaced_count = 0
    total_params_before = 0
    total_params_after = 0
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ›¿æ›çš„æ¨¡çµ„
    modules_to_replace = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            if target_modules is not None:
                should_replace = any(name.startswith(target) or name == target for target in target_modules)
                if not should_replace:
                    continue
            modules_to_replace.append((name, module))
    
    if verbose:
        print(f">> [é‡åŒ–] æ‰¾åˆ° {len(modules_to_replace)} å€‹å¯é‡åŒ–çš„ Linear å±¤")
    
    # æ›¿æ›æ¨¡çµ„
    for name, module in modules_to_replace:
        parent = _get_parent_module(model, name)
        child_name = name.split('.')[-1]
        
        # è¨ˆç®—åƒæ•¸é‡
        param_count = module.in_features * module.out_features
        total_params_before += param_count * 4   # FP32 = 4 bytes
        total_params_after += param_count * 0.5  # INT4 = 0.5 byte
        
        has_bias = module.bias is not None
        quantized_linear = bnb.nn.Linear4bit(
            module.in_features,
            module.out_features,
            bias=has_bias,
            compute_dtype=compute_dtype,  # ä½¿ç”¨é…ç½®çš„é‹ç®—ç²¾åº¦
            quant_type='nf4',  # ä½¿ç”¨ NF4 é‡åŒ–
        )
        
        # è¤‡è£½æ¬Šé‡
        quantized_linear.weight = bnb.nn.Params4bit(
            module.weight.data.contiguous(),
            requires_grad=False,
            quant_type='nf4'
        )
        if has_bias:
            quantized_linear.bias = nn.Parameter(module.bias.data.clone())
        
        setattr(parent, child_name, quantized_linear)
        replaced_count += 1
        
        if verbose and replaced_count <= 5:
            print(f">>   - é‡åŒ–: {name} ({module.in_features}x{module.out_features})")
    
    if verbose and replaced_count > 5:
        print(f">>   - ... é‚„æœ‰ {replaced_count - 5} å€‹å±¤")
    
    if verbose:
        mem_before_mb = total_params_before / (1024 * 1024)
        mem_after_mb = total_params_after / (1024 * 1024)
        savings_pct = (1 - total_params_after / total_params_before) * 100 if total_params_before > 0 else 0
        print(f">> [é‡åŒ–] æ¬Šé‡è¨˜æ†¶é«”: {mem_before_mb:.1f}MB â†’ {mem_after_mb:.1f}MB (ç¯€çœ {savings_pct:.0f}%)")
    
    return replaced_count

class IndexTTS:
    def __init__(
        self, cfg_path="checkpoints/config.yaml", model_dir="checkpoints", is_fp16=True, device=None, use_cuda_kernel=None,
        speaker_info_path=None,  # æ–°å¢ï¼šèªªè©±äººè³‡è¨Šæª”æ¡ˆè·¯å¾‘
        precision_config=None,  # æ–°å¢ï¼šç´°ç²’åº¦æ··åˆç²¾åº¦é…ç½®
    ):
        """
        Args:
            cfg_path (str): path to the config file.
            model_dir (str): path to the model directory.
            is_fp16 (bool): whether to use fp16 (deprecated, use precision_config instead).
            device (str): device to use (e.g., 'cuda:0', 'cpu'). If None, it will be set automatically based on the availability of CUDA or MPS.
            use_cuda_kernel (None | bool): whether to use BigVGan custom fused activation CUDA kernel, only for CUDA device.
            precision_config (dict): ç´°ç²’åº¦æ··åˆç²¾åº¦é…ç½®ï¼Œä¾‹å¦‚: {'gpt': 'bf16', 'vocoder': 'bf16'}
        """
        if device is not None:
            self.device = device
            self.is_fp16 = False if device == "cpu" else is_fp16
            self.use_cuda_kernel = use_cuda_kernel is not None and use_cuda_kernel and device.startswith("cuda")
        elif torch.cuda.is_available():
            self.device = "cuda:0"
            self.is_fp16 = is_fp16
            self.use_cuda_kernel = use_cuda_kernel is None or use_cuda_kernel
        elif hasattr(torch, "mps") and torch.backends.mps.is_available():
            self.device = "mps"
            self.is_fp16 = False # Use float16 on MPS is overhead than float32
            self.use_cuda_kernel = False
        else:
            self.device = "cpu"
            self.is_fp16 = False
            self.use_cuda_kernel = False
            print(">> Be patient, it may take a while to run in CPU mode.")

        self.cfg = OmegaConf.load(cfg_path)
        self.model_dir = model_dir

        # è™•ç†æ··åˆç²¾åº¦é…ç½®
        # å„ªå…ˆé †åºï¼š1. precision_config åƒæ•¸ -> 2. config_inference.yaml -> 3. config.yaml çš„ inference å€å¡Š -> 4. is_fp16ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        config_source = None
        if precision_config is None:
            # å…ˆå˜—è©¦è®€å–å°ˆé–€çš„æ¨ç†é…ç½®æª”
            inference_config_path = os.path.join(model_dir, "config_inference.yaml")
            if os.path.exists(inference_config_path):
                inference_cfg = OmegaConf.load(inference_config_path)
                if hasattr(inference_cfg, 'inference'):
                    precision_config = inference_cfg.inference
                    config_source = f"config_inference.yaml"
            # å›é€€åˆ°åŸå§‹ config.yaml çš„ inference å€å¡Š
            elif hasattr(self.cfg, 'inference'):
                precision_config = self.cfg.inference
                config_source = "config.yaml [inference]"
        else:
            config_source = "ç¨‹å¼ç¢¼åƒæ•¸ (precision_config)"

        # è§£æç²¾åº¦é…ç½®
        def resolve_dtype(precision_str):
            if precision_str in ["bf16", "bfloat16"]:
                return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            elif precision_str in ["fp16", "float16"]:
                return torch.float16
            elif precision_str in ["fp8"]:
                return torch.float8_e4m3fn if hasattr(torch, 'float8_e4m3fn') else torch.bfloat16
            else:  # fp32, no, None
                return torch.float32

        if precision_config and isinstance(precision_config, dict):
            # è®€å–æ¨ç†é…ç½®ï¼ˆinference.gpt, inference.vocoder, inference.quantizationï¼‰
            gpt_precision = precision_config.get('gpt', 'bf16')
            vocoder_precision = precision_config.get('vocoder', 'bf16')

            quant_cfg = precision_config.get('quantization', {})
            quant_enabled = quant_cfg.get('enabled', False)

            # è¨­å®šç²¾åº¦å’Œé‡åŒ–
            if quant_enabled:
                # é€²éšæ¨¡å¼ï¼šweight_dtype + compute_dtype
                weight_dtype = quant_cfg.get('weight_dtype', 'int8')
                compute_dtype = quant_cfg.get('compute_dtype', 'bf16')

                self.gpt_weight_dtype = weight_dtype
                self.gpt_compute_dtype = resolve_dtype(compute_dtype)
                self.use_quantization = True
                self.load_in_8bit = (weight_dtype == 'int8')
                self.load_in_4bit = (weight_dtype == 'int4')

                print(f">> ä½¿ç”¨é‡åŒ–æ¨ç† (é€²éšæ¨¡å¼) - é…ç½®ä¾†æº: {config_source}")
                print(f"   - æ¬Šé‡å­˜å„²: {weight_dtype.upper()} (çœ {'75%' if weight_dtype == 'int8' else '87.5%'} é¡¯å­˜)")
                print(f"   - é‹ç®—ç²¾åº¦: {self.gpt_compute_dtype}")
                print(f"   - Vocoder: {vocoder_precision}")

            elif gpt_precision == 'int8':
                # ç°¡å–®æ¨¡å¼ï¼šint8 (æ¬Šé‡ INT8 + é‹ç®— BF16)
                self.gpt_weight_dtype = 'int8'
                self.gpt_compute_dtype = torch.bfloat16
                self.use_quantization = True
                self.load_in_8bit = True
                self.load_in_4bit = False
                print(f">> ä½¿ç”¨ INT8 é‡åŒ–æ¨ç† - é…ç½®ä¾†æº: {config_source}")
                print(f"   æ¬Šé‡=INT8, é‹ç®—=BF16, Vocoder={vocoder_precision}")

            elif gpt_precision == 'int4':
                # ç°¡å–®æ¨¡å¼ï¼šint4 (æ¬Šé‡ INT4 + é‹ç®— BF16)
                self.gpt_weight_dtype = 'int4'
                self.gpt_compute_dtype = torch.bfloat16
                self.use_quantization = True
                self.load_in_8bit = False
                self.load_in_4bit = True
                print(f">> ä½¿ç”¨ INT4 é‡åŒ–æ¨ç† - é…ç½®ä¾†æº: {config_source}")
                print(f"   æ¬Šé‡=INT4, é‹ç®—=BF16, Vocoder={vocoder_precision}")

            else:
                # æ¨™æº–æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æŒ‡å®šç²¾åº¦
                self.gpt_dtype = resolve_dtype(gpt_precision)
                self.use_quantization = False
                self.load_in_8bit = False
                self.load_in_4bit = False
                print(f">> ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç† - é…ç½®ä¾†æº: {config_source}")
                print(f"   GPT={self.gpt_dtype}, Vocoder={vocoder_precision}")

            self.vocoder_dtype = resolve_dtype(vocoder_precision)
            self.dvae_dtype = self.gpt_dtype if not self.use_quantization and isinstance(self.gpt_dtype, torch.dtype) else torch.bfloat16
        else:
            # å‘å¾Œå…¼å®¹ï¼šä½¿ç”¨ is_fp16ï¼ˆè‡ªå‹•é¸æ“‡ BF16 æˆ– FP16ï¼‰
            if self.is_fp16:
                # å„ªå…ˆä½¿ç”¨ BF16ï¼ˆæ•¸å€¼ç©©å®šæ€§æ›´å¥½ï¼‰ï¼Œä¸æ”¯æ´æ‰ç”¨ FP16
                if torch.cuda.is_bf16_supported():
                    self.gpt_dtype = torch.bfloat16
                    self.vocoder_dtype = torch.float32 # BigVGAN åœ¨ BF16 ä¸‹å¯èƒ½ä¸ç©©å®šï¼Œé è¨­å›é€€åˆ° FP32
                    self.dvae_dtype = torch.bfloat16
                    print(">> ä½¿ç”¨ BF16 æ¨ç† (GPT) / FP32 (Vocoder) - é…ç½®ä¾†æº: is_fp16 åƒæ•¸ï¼ˆå‘å¾Œå…¼å®¹æ¨¡å¼ï¼‰")
                else:
                    self.gpt_dtype = torch.float16
                    self.vocoder_dtype = torch.float32 # BigVGAN åœ¨ FP16 ä¸‹å¯èƒ½ä¸ç©©å®šï¼Œé è¨­å›é€€åˆ° FP32
                    self.dvae_dtype = torch.float16
                    print(">> ä½¿ç”¨ FP16 æ¨ç† (GPT) / FP32 (Vocoder) - é…ç½®ä¾†æº: is_fp16 åƒæ•¸ï¼ˆå‘å¾Œå…¼å®¹æ¨¡å¼ï¼‰")
                print("   å»ºè­°: ä½¿ç”¨ config_inference.yaml æˆ– config.yaml [inference] é€²è¡Œç²¾åº¦é…ç½®")
            else:
                self.gpt_dtype = torch.float32
                self.vocoder_dtype = torch.float32
                self.dvae_dtype = torch.float32
                print(">> ä½¿ç”¨ FP32 æ¨ç† - é…ç½®ä¾†æº: é è¨­å€¼ï¼ˆå‘å¾Œå…¼å®¹æ¨¡å¼ï¼‰")
                print("   å»ºè­°: ä½¿ç”¨ config_inference.yaml æˆ– config.yaml [inference] é€²è¡Œç²¾åº¦é…ç½®")

            # å‘å¾Œå…¼å®¹æ¨¡å¼ä¸ä½¿ç”¨é‡åŒ–
            self.use_quantization = False
            self.load_in_8bit = False
            self.load_in_4bit = False

        # å‘å¾Œå…¼å®¹
        self.dtype = self.gpt_dtype if self.gpt_dtype != torch.float32 else None
        self.stop_mel_token = self.cfg.gpt.stop_mel_token

        # Comment-off to load the VQ-VAE model for debugging tokenizer
        #   https://github.com/index-tts/index-tts/issues/34
        #
        # from indextts.vqvae.xtts_dvae import DiscreteVAE
        # self.dvae = DiscreteVAE(**self.cfg.vqvae)
        # self.dvae_path = os.path.join(self.model_dir, self.cfg.dvae_checkpoint)
        # load_checkpoint(self.dvae, self.dvae_path)
        # self.dvae = self.dvae.to(self.device)
        # if self.is_fp16:
        #     self.dvae.eval().half()
        # else:
        #     self.dvae.eval()
        # print(">> vqvae weights restored from:", self.dvae_path)
        self.gpt_path = os.path.join(self.model_dir, self.cfg.gpt_checkpoint)

        # ä½¿ç”¨é‡åŒ–è¼‰å…¥
        if self.use_quantization:
            try:
                # è¼‰å…¥æ¨¡å‹ï¼ˆå…ˆä»¥ FP32 è¼‰å…¥ï¼‰
                self.gpt = UnifiedVoice(**self.cfg.gpt)
                load_checkpoint(self.gpt, self.gpt_path)
                self.gpt = self.gpt.to(self.device)
                
                # å®šç¾©è¦é‡åŒ–çš„æ¨¡çµ„ï¼ˆGPT æ ¸å¿ƒéƒ¨åˆ†ï¼‰
                # gpt æ˜¯ HuggingFace GPT2Modelï¼ŒåŒ…å«ä¸»è¦çš„ Transformer å±¤
                target_modules = ['gpt', 'text_head', 'mel_head']
                
                # åŸ·è¡Œå‹•æ…‹é‡åŒ–
                print("=" * 60)
                print(">> ğŸ”§ é–‹å§‹ GPT æ¨¡å‹é‡åŒ–...")
                print("=" * 60)
                
                if self.load_in_8bit:
                    replaced = _quantize_linear_layers_to_int8(self.gpt, target_modules, verbose=True)
                    quant_type = "INT8"
                elif self.load_in_4bit:
                    replaced = _quantize_linear_layers_to_int4(self.gpt, target_modules, verbose=True, compute_dtype=self.gpt_compute_dtype)
                    quant_type = "INT4 (NF4)"
                else:
                    replaced = 0
                    quant_type = "UNKNOWN"
                
                if replaced > 0:
                    print("=" * 60)
                    print(f">> âœ… é‡åŒ–å®Œæˆï¼")
                    print(f">>    - é‡åŒ–é¡å‹: {quant_type}")
                    print(f">>    - é‡åŒ–å±¤æ•¸: {replaced}")
                    print(f">>    - æ¨¡å‹è·¯å¾‘: {self.gpt_path}")
                    print("=" * 60)
                    self.gpt.eval()
                else:
                    print(">> âš ï¸  æœªæ‰¾åˆ°å¯é‡åŒ–çš„å±¤ï¼Œå›é€€åˆ° BF16")
                    self.use_quantization = False
                    self.gpt.eval().to(torch.bfloat16)
                    print(f">> GPT weights restored from: {self.gpt_path} (dtype: BF16)")

            except ImportError:
                print(">> âš ï¸  bitsandbytes æœªå®‰è£ï¼Œå›é€€åˆ° BF16")
                print(">> å®‰è£: pip install bitsandbytes")
                self.use_quantization = False
                self.gpt = UnifiedVoice(**self.cfg.gpt)
                load_checkpoint(self.gpt, self.gpt_path)
                self.gpt = self.gpt.to(self.device)
                self.gpt.eval().to(torch.bfloat16)
                print(f">> GPT weights restored from: {self.gpt_path} (dtype: BF16)")
            except Exception as e:
                print(f">> âš ï¸  é‡åŒ–å¤±æ•—: {e}")
                print(">> å›é€€åˆ° BF16 ç²¾åº¦")
                self.use_quantization = False
                # é‡æ–°è¼‰å…¥æ¨¡å‹
                self.gpt = UnifiedVoice(**self.cfg.gpt)
                load_checkpoint(self.gpt, self.gpt_path)
                self.gpt = self.gpt.to(self.device)
                self.gpt.eval().to(torch.bfloat16)
                print(f">> GPT weights restored from: {self.gpt_path} (dtype: BF16)")
        else:
            # æ¨™æº–ç²¾åº¦è¼‰å…¥
            self.gpt = UnifiedVoice(**self.cfg.gpt)
            load_checkpoint(self.gpt, self.gpt_path)
            self.gpt = self.gpt.to(self.device)

            # ä½¿ç”¨ç´°ç²’åº¦ç²¾åº¦
            if self.gpt_dtype == torch.float16:
                self.gpt.eval().half()
            elif self.gpt_dtype == torch.bfloat16:
                self.gpt.eval().to(torch.bfloat16)
            else:
                self.gpt.eval()
            print(f">> GPT weights restored from: {self.gpt_path} (dtype: {self.gpt_dtype})")
        if self.is_fp16:
            try:
                import deepspeed

                use_deepspeed = True
            except (ImportError, OSError, CalledProcessError) as e:
                use_deepspeed = False
                print(f">> DeepSpeedè¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ°æ¨™æº–æ¨ç†: {e}")
                print("See more details https://www.deepspeed.ai/tutorials/advanced-install/")

            self.gpt.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=True, half=True)
        else:
            self.gpt.post_init_gpt2_config(use_deepspeed=False, kv_cache=True, half=False)

        if self.use_cuda_kernel:
            # preload the CUDA kernel for BigVGAN
            try:
                from indextts.BigVGAN.alias_free_activation.cuda import (
                    load as anti_alias_activation_loader,
                )
                anti_alias_activation_cuda = anti_alias_activation_loader.load()
                print(">> Preload custom CUDA kernel for BigVGAN", anti_alias_activation_cuda)
            except Exception as e:
                print(">> Failed to load custom CUDA kernel for BigVGAN. Falling back to torch.", e, file=sys.stderr)
                print(" Reinstall with `pip install -e . --no-deps --no-build-isolation` to prebuild `anti_alias_activation_cuda` kernel.", file=sys.stderr)
                print(
                    "See more details: https://github.com/index-tts/index-tts/issues/164#issuecomment-2903453206", file=sys.stderr
                )
                self.use_cuda_kernel = False
        self.bigvgan = Generator(self.cfg.bigvgan, use_cuda_kernel=self.use_cuda_kernel)
        self.bigvgan_path = os.path.join(self.model_dir, self.cfg.bigvgan_checkpoint)
        vocoder_dict = torch.load(self.bigvgan_path, map_location="cpu")
        self.bigvgan.load_state_dict(vocoder_dict["generator"])
        self.bigvgan = self.bigvgan.to(self.device)

        # ä½¿ç”¨ç´°ç²’åº¦ç²¾åº¦ï¼ˆä¿æŒ BatchNorm ç‚º FP32ï¼‰
        if self.vocoder_dtype == torch.float16:
            self.bigvgan.half()
            # BatchNorm å±¤å›é€€åˆ° FP32
            for module in self.bigvgan.modules():
                if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):
                    module.float()
        elif self.vocoder_dtype == torch.bfloat16:
            self.bigvgan.to(torch.bfloat16)
            # BatchNorm å±¤å›é€€åˆ° FP32
            for module in self.bigvgan.modules():
                if isinstance(module, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):
                    module.float()

        # remove weight norm on eval mode
        self.bigvgan.remove_weight_norm()
        self.bigvgan.eval()
        print(f">> bigvgan weights restored from: {self.bigvgan_path} (dtype: {self.vocoder_dtype})")
        self.bpe_path = os.path.join(self.model_dir, self.cfg.dataset["bpe_model"])
        self.normalizer = TextNormalizer()
        self.normalizer.load()
        print(">> TextNormalizer loaded")
        self.tokenizer = TextTokenizer(self.bpe_path, self.normalizer)
        print(">> bpe model loaded from:", self.bpe_path)
        # å¿«å–åƒè€ƒéŸ³è¨Šmelï¼š
        self.cache_audio_prompt = None
        self.cache_cond_mel = None
        # é€²åº¦å¼•ç”¨é¡¯ç¤ºï¼ˆå¯é¸ï¼‰
        self.gr_progress = None
        self.model_version = self.cfg.version if hasattr(self.cfg, "version") else None
        
        # åˆå§‹åŒ–å¤šèªªè©±äººæ”¯æ´
        self.speaker_list = []
        if speaker_info_path and os.path.exists(speaker_info_path):
            try:
                with open(speaker_info_path, 'r', encoding='utf-8') as f:
                    speaker_info = json.load(f)
                # speaker_info.json æ˜¯ä¸€å€‹æ•¸çµ„ï¼Œæ¯å€‹å…ƒç´ åŒ…å« speaker æ¬„ä½
                self.speaker_list = [item['speaker'] for item in speaker_info if 'speaker' in item]
                print(f">> Multi-speaker support enabled with {len(self.speaker_list)} speakers: {self.speaker_list}")
            except Exception as e:
                print(f">> Failed to load speaker_info from {speaker_info_path}: {e}")
                self.speaker_list = []
        else:
            print(">> Single-speaker mode (no speaker_info_path provided)")

        # é©—è­‰æ¨¡å‹ç²¾åº¦
        self._verify_model_precision()

    def _verify_model_precision(self):
        """
        é©—è­‰æ¨¡å‹å¯¦éš›è¼‰å…¥çš„ç²¾åº¦æ˜¯å¦ç¬¦åˆé æœŸã€‚
        é€™æœ‰åŠ©æ–¼åŠæ—©ç™¼ç¾ç²¾åº¦é…ç½®éŒ¯èª¤ã€‚
        """
        print("=" * 60)
        print(">> ğŸ” é©—è­‰æ¨¡å‹ç²¾åº¦...")

        # é©—è­‰ GPT æ¨¡å‹ç²¾åº¦
        try:
            # ç²å– GPT æ¨¡å‹çš„ç¬¬ä¸€å€‹åƒæ•¸çš„ç²¾åº¦
            gpt_actual_dtype = next(self.gpt.parameters()).dtype

            if self.use_quantization:
                # é‡åŒ–æ¨¡å¼ï¼šæª¢æŸ¥é‹ç®—ç²¾åº¦ï¼ˆæ¬Šé‡å¯èƒ½æ˜¯ INT8/INT4ï¼‰
                expected_dtype = self.gpt_compute_dtype
                print(f">> GPT æ¨¡å‹ (é‡åŒ–æ¨¡å¼):")
                print(f"   - é æœŸé‹ç®—ç²¾åº¦: {expected_dtype}")
                print(f"   - å¯¦éš›åƒæ•¸ç²¾åº¦: {gpt_actual_dtype}")
                # æ³¨æ„ï¼šé‡åŒ–å¾ŒæŸäº›å±¤å¯èƒ½æ˜¯é‡åŒ–é¡å‹ï¼Œé€™è£¡åªæ˜¯æª¢æŸ¥éé‡åŒ–åƒæ•¸
                if hasattr(gpt_actual_dtype, '__name__'):
                    dtype_name = gpt_actual_dtype.__name__ if hasattr(gpt_actual_dtype, '__name__') else str(gpt_actual_dtype)
                    if 'int' in dtype_name.lower() or 'Int' in str(type(gpt_actual_dtype)):
                        print(f"   âœ… é‡åŒ–åƒæ•¸åµæ¸¬åˆ°: {gpt_actual_dtype}")
                    else:
                        print(f"   âœ… éé‡åŒ–åƒæ•¸ç²¾åº¦: {gpt_actual_dtype}")
            else:
                # æ¨™æº–ç²¾åº¦æ¨¡å¼
                expected_dtype = self.gpt_dtype
                print(f">> GPT æ¨¡å‹:")
                print(f"   - é æœŸç²¾åº¦: {expected_dtype}")
                print(f"   - å¯¦éš›ç²¾åº¦: {gpt_actual_dtype}")

                if gpt_actual_dtype != expected_dtype:
                    print(f"   âš ï¸  è­¦å‘Šï¼šç²¾åº¦ä¸ç¬¦ï¼è«‹æª¢æŸ¥æ¨¡å‹è¼‰å…¥æµç¨‹")
                else:
                    print(f"   âœ… ç²¾åº¦é©—è­‰é€šé")
        except Exception as e:
            print(f"   âš ï¸  GPT ç²¾åº¦é©—è­‰å¤±æ•—: {e}")

        # é©—è­‰ BigVGAN æ¨¡å‹ç²¾åº¦
        try:
            vocoder_actual_dtype = next(self.bigvgan.parameters()).dtype
            expected_vocoder_dtype = self.vocoder_dtype

            print(f">> BigVGAN è²ç¢¼å™¨:")
            print(f"   - é æœŸç²¾åº¦: {expected_vocoder_dtype}")
            print(f"   - å¯¦éš›ç²¾åº¦: {vocoder_actual_dtype}")

            if vocoder_actual_dtype != expected_vocoder_dtype:
                print(f"   âš ï¸  è­¦å‘Šï¼šç²¾åº¦ä¸ç¬¦ï¼è«‹æª¢æŸ¥æ¨¡å‹è¼‰å…¥æµç¨‹")
            else:
                print(f"   âœ… ç²¾åº¦é©—è­‰é€šé")
        except Exception as e:
            print(f"   âš ï¸  Vocoder ç²¾åº¦é©—è­‰å¤±æ•—: {e}")

        print("=" * 60)

    def remove_long_silence(self, codes: torch.Tensor, silent_token=52, max_consecutive=30):
        """
        Shrink special tokens (silent_token and stop_mel_token) in codes
        codes: [B, T]
        """
        code_lens = []
        codes_list = []
        device = codes.device
        dtype = codes.dtype
        isfix = False
        for i in range(0, codes.shape[0]):
            code = codes[i]
            if not torch.any(code == self.stop_mel_token).item():
                len_ = code.size(0)
            else:
                stop_mel_idx = (code == self.stop_mel_token).nonzero(as_tuple=False)
                len_ = stop_mel_idx[0].item() if len(stop_mel_idx) > 0 else code.size(0)

            count = torch.sum(code == silent_token).item()
            if count > max_consecutive:
                # code = code.cpu().tolist()
                ncode_idx = []
                n = 0
                for k in range(len_):
                    assert code[k] != self.stop_mel_token, f"stop_mel_token {self.stop_mel_token} should be shrinked here"
                    if code[k] != silent_token:
                        ncode_idx.append(k)
                        n = 0
                    elif code[k] == silent_token and n < 10:
                        ncode_idx.append(k)
                        n += 1
                    # if (k == 0 and code[k] == 52) or (code[k] == 52 and code[k-1] == 52):
                    #    n += 1
                # new code
                len_ = len(ncode_idx)
                codes_list.append(code[ncode_idx])
                isfix = True
            else:
                # shrink to len_
                codes_list.append(code[:len_])
            code_lens.append(len_)
        if isfix:
            if len(codes_list) > 1:
                codes = pad_sequence(codes_list, batch_first=True, padding_value=self.stop_mel_token)
            else:
                codes = codes_list[0].unsqueeze(0)
        else:
            # unchanged
            pass
        # clip codes to max length
        max_len = max(code_lens)
        if max_len < codes.shape[1]:
            codes = codes[:, :max_len]
        code_lens = torch.tensor(code_lens, dtype=torch.long, device=device)
        return codes, code_lens

    def bucket_sentences(self, sentences, bucket_max_size=4) -> List[List[Dict]]:
        """
        Sentence data bucketing.
        if ``bucket_max_size=1``, return all sentences in one bucket.
        """
        outputs: List[Dict] = []
        for idx, sent in enumerate(sentences):
            outputs.append({"idx": idx, "sent": sent, "len": len(sent)})
       
        if len(outputs) > bucket_max_size:
            # split sentences into buckets by sentence length
            buckets: List[List[Dict]] = []
            factor = 1.5
            last_bucket = None
            last_bucket_sent_len_median = 0

            for sent in sorted(outputs, key=lambda x: x["len"]):
                current_sent_len = sent["len"]
                if current_sent_len == 0:
                    print(">> skip empty sentence")
                    continue
                if last_bucket is None \
                        or current_sent_len >= int(last_bucket_sent_len_median * factor) \
                        or len(last_bucket) >= bucket_max_size:
                    # new bucket
                    buckets.append([sent])
                    last_bucket = buckets[-1]
                    last_bucket_sent_len_median = current_sent_len
                else:
                    # current bucket can hold more sentences
                    last_bucket.append(sent) # sorted
                    mid = len(last_bucket) // 2
                    last_bucket_sent_len_median = last_bucket[mid]["len"]
            last_bucket=None
            # merge all buckets with size 1
            out_buckets: List[List[Dict]] = []
            only_ones: List[Dict] = []
            for b in buckets:
                if len(b) == 1:
                    only_ones.append(b[0])
                else:
                    out_buckets.append(b)
            if len(only_ones) > 0:
                # merge into previous buckets if possible
                # print("only_ones:", [(o["idx"], o["len"]) for o in only_ones])
                for i in range(len(out_buckets)):
                    b = out_buckets[i]
                    if len(b) < bucket_max_size:
                        b.append(only_ones.pop(0))
                        if len(only_ones) == 0:
                            break
                # combined all remaining sized 1 buckets
                if len(only_ones) > 0:
                    out_buckets.extend([only_ones[i:i+bucket_max_size] for i in range(0, len(only_ones), bucket_max_size)])
            return out_buckets
        return [outputs]

    def pad_tokens_cat(self, tokens: List[torch.Tensor]) -> torch.Tensor:
        if self.model_version and self.model_version >= 1.5:
            # 1.5ç‰ˆæœ¬ä»¥ä¸Šï¼Œä½¿ç”¨ stop_text_token å³å´å¡«å……
            # [1, N] -> [N,]
            tokens = [t.squeeze(0) for t in tokens]
            # æ‰‹å‹•å¯¦ç¾ right paddingï¼ˆPyTorch pad_sequence ä¸æ”¯æ´ padding_sideï¼‰
            max_len = max(t.size(0) for t in tokens)
            outputs = []
            for t in tokens:
                pad_len = max_len - t.size(0)
                if pad_len > 0:
                    # åœ¨å³å´å¡«å…… stop_text_token
                    padded = torch.cat([t, torch.full((pad_len,), self.cfg.gpt.stop_text_token, dtype=t.dtype, device=t.device)])
                else:
                    padded = t
                outputs.append(padded)
            return torch.stack(outputs)  # [batch_size, max_len]
        max_len = max(t.size(1) for t in tokens)
        outputs = []
        for tensor in tokens:
            pad_len = max_len - tensor.size(1)
            if pad_len > 0:
                n = min(8, pad_len)
                tensor = torch.nn.functional.pad(tensor, (0, n), value=self.cfg.gpt.stop_text_token)
                tensor = torch.nn.functional.pad(tensor, (0, pad_len - n), value=self.cfg.gpt.start_text_token)
            tensor = tensor[:, :max_len]
            outputs.append(tensor)
        tokens = torch.cat(outputs, dim=0)
        return tokens

    def torch_empty_cache(self):
        try:
            if "cuda" in str(self.device):
                torch.cuda.empty_cache()
            elif "mps" in str(self.device):
                torch.mps.empty_cache()
        except Exception as e:
            pass

    def _set_gr_progress(self, value, desc):
        if self.gr_progress is not None:
            self.gr_progress(value, desc=desc)

    # å¿«é€Ÿæ¨ç†ï¼šå°æ–¼â€œå¤šå¥é•·æ–‡å­—â€ï¼Œå¯å¯¦ç¾è‡³å°‘ 2~10 å€ä»¥ä¸Šçš„é€Ÿåº¦æå‡~ ï¼ˆFirst modified by sunnyboxs 2025-04-16ï¼‰
    def infer_fast(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=100, sentences_bucket_max_size=4, **generation_kwargs):
        """
        Args:
            ``max_text_tokens_per_sentence``: åˆ†å¥çš„æœ€å¤§tokenæ•¸ï¼Œé è¨­``100``ï¼Œå¯ä»¥æ ¹æ“šGPUç¡¬é«”æƒ…æ³èª¿æ•´
                - è¶Šå°ï¼Œbatch è¶Šå¤šï¼Œæ¨ç†é€Ÿåº¦è¶Š*å¿«*ï¼Œä½”ç”¨è¨˜æ†¶é«”æ›´å¤šï¼Œå¯èƒ½å½±éŸ¿è³ªé‡
                - è¶Šå¤§ï¼Œbatch è¶Šå°‘ï¼Œæ¨ç†é€Ÿåº¦è¶Š*æ…¢*ï¼Œä½”ç”¨è¨˜æ†¶é«”å’Œè³ªé‡æ›´æ¥è¿‘æ–¼éå¿«é€Ÿæ¨ç†
            ``sentences_bucket_max_size``: åˆ†å¥åˆ†æ¡¶çš„æœ€å¤§å®¹é‡ï¼Œé è¨­``4``ï¼Œå¯ä»¥æ ¹æ“šGPUè¨˜æ†¶é«”èª¿æ•´
                - è¶Šå¤§ï¼Œbucketæ•¸é‡è¶Šå°‘ï¼Œbatchè¶Šå¤šï¼Œæ¨ç†é€Ÿåº¦è¶Š*å¿«*ï¼Œä½”ç”¨è¨˜æ†¶é«”æ›´å¤šï¼Œå¯èƒ½å½±éŸ¿è³ªé‡
                - è¶Šå°ï¼Œbucketæ•¸é‡è¶Šå¤šï¼Œbatchè¶Šå°‘ï¼Œæ¨ç†é€Ÿåº¦è¶Š*æ…¢*ï¼Œä½”ç”¨è¨˜æ†¶é«”å’Œè³ªé‡æ›´æ¥è¿‘æ–¼éå¿«é€Ÿæ¨ç†
        """
        print(">> start fast inference...")
        
        self._set_gr_progress(0, "start fast inference...")
        if verbose:
            print(f"origin text:{text}")
        start_time = time.perf_counter()

        # å¦‚æœåƒè€ƒéŸ³è¨Šæ”¹è®Šäº†ï¼Œæ‰éœ€è¦é‡æ–°ç”Ÿæˆ cond_mel, æå‡é€Ÿåº¦
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            audio, sr = sf.read(audio_prompt)
            audio = torch.from_numpy(audio.T if audio.ndim > 1 else audio.reshape(1, -1)).float()
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            pass

        auto_conditioning = cond_mel
        cond_mel_lengths = torch.tensor([cond_mel_frame], device=self.device)

        # text_tokens
        text_tokens_list = self.tokenizer.tokenize(text)

        sentences = self.tokenizer.split_sentences(text_tokens_list, max_tokens_per_sentence=max_text_tokens_per_sentence)
        if verbose:
            print(">> text token count:", len(text_tokens_list))
            print("   splited sentences count:", len(sentences))
            print("   max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        # lang = "EN"
        # lang = "ZH"
        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        bigvgan_time = 0

        # text processing
        all_text_tokens: List[List[torch.Tensor]] = []
        self._set_gr_progress(0.1, "text processing...")
        bucket_max_size = sentences_bucket_max_size if self.device != "cpu" else 1
        all_sentences = self.bucket_sentences(sentences, bucket_max_size=bucket_max_size)
        bucket_count = len(all_sentences)
        if verbose:
            print(">> sentences bucket_count:", bucket_count,
                  "bucket sizes:", [(len(s), [t["idx"] for t in s]) for s in all_sentences],
                  "bucket_max_size:", bucket_max_size)
        for sentences in all_sentences:
            temp_tokens: List[torch.Tensor] = []
            all_text_tokens.append(temp_tokens)
            for item in sentences:
                sent = item["sent"]
                text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
                text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
                if verbose:
                    print(text_tokens)
                    print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                    # debug tokenizer
                    text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                    print("text_token_syms is same as sentence tokens", text_token_syms == sent) 
                temp_tokens.append(text_tokens)
        
            
        # Sequential processing of bucketing data
        all_batch_num = sum(len(s) for s in all_sentences)
        all_batch_codes = []
        processed_num = 0
        for item_tokens in all_text_tokens:
            batch_num = len(item_tokens)
            if batch_num > 1:
                batch_text_tokens = self.pad_tokens_cat(item_tokens)
            else:
                batch_text_tokens = item_tokens[0]
            processed_num += batch_num
            # gpt speech
            self._set_gr_progress(0.2 + 0.3 * processed_num/all_batch_num, f"gpt inference speech... {processed_num}/{all_batch_num}")
            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.amp.autocast(batch_text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    temp_codes = self.gpt.inference_speech(auto_conditioning, batch_text_tokens,
                                        cond_mel_lengths=cond_mel_lengths,
                                        # text_lengths=text_len,
                                        do_sample=do_sample,
                                        top_p=top_p,
                                        top_k=top_k,
                                        temperature=temperature,
                                        num_return_sequences=autoregressive_batch_size,
                                        length_penalty=length_penalty,
                                        num_beams=num_beams,
                                        repetition_penalty=repetition_penalty,
                                        max_generate_length=max_mel_tokens,
                                        **generation_kwargs)
                    all_batch_codes.append(temp_codes)
            gpt_gen_time += time.perf_counter() - m_start_time

        # gpt latent
        self._set_gr_progress(0.5, "gpt inference latents...")
        all_idxs = []
        all_latents = []
        has_warned = False
        for batch_codes, batch_tokens, batch_sentences in zip(all_batch_codes, all_text_tokens, all_sentences):
            for i in range(batch_codes.shape[0]):
                codes = batch_codes[i]  # [x]
                if not has_warned and codes[-1] != self.stop_mel_token:
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True
                codes = codes.unsqueeze(0)  # [x] -> [1, x]
                if verbose:
                    print("codes:", codes.shape)
                    print(codes)
                codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
                if verbose:
                    print("fix codes:", codes.shape)
                    print(codes)
                    print("code_lens:", code_lens)
                text_tokens = batch_tokens[i]
                all_idxs.append(batch_sentences[i]["idx"])
                m_start_time = time.perf_counter()
                with torch.no_grad():
                    with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                        latent = \
                            self.gpt(auto_conditioning, text_tokens,
                                        torch.tensor([text_tokens.shape[-1]], device=text_tokens.device), codes,
                                        code_lens*self.gpt.mel_length_compression,
                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),
                                        return_latent=True, clip_inputs=False)
                        gpt_forward_time += time.perf_counter() - m_start_time
                        all_latents.append(latent)
        del all_batch_codes, all_text_tokens, all_sentences
        # bigvgan chunk
        chunk_size = 2
        all_latents = [all_latents[all_idxs.index(i)] for i in range(len(all_latents))]
        if verbose:
            print(">> all_latents:", len(all_latents))
            print("  latents length:", [l.shape[1] for l in all_latents])
        chunk_latents = [all_latents[i : i + chunk_size] for i in range(0, len(all_latents), chunk_size)]
        chunk_length = len(chunk_latents)
        latent_length = len(all_latents)

        # bigvgan chunk decode
        self._set_gr_progress(0.7, "bigvgan decode...")
        tqdm_progress = tqdm(total=latent_length, desc="bigvgan")
        for items in chunk_latents:
            tqdm_progress.update(len(items))
            latent = torch.cat(items, dim=1)
            with torch.no_grad():
                # Determine autocast settings for vocoder
                vocoder_autocast_enabled = self.vocoder_dtype != torch.float32
                vocoder_autocast_dtype = self.vocoder_dtype if vocoder_autocast_enabled else None

                # Explicitly cast inputs if vocoder is FP32
                if not vocoder_autocast_enabled:
                    latent = latent.float()
                    cond_input = auto_conditioning.transpose(1, 2).float()
                else:
                    cond_input = auto_conditioning.transpose(1, 2)

                with torch.amp.autocast(latent.device.type, enabled=vocoder_autocast_enabled, dtype=vocoder_autocast_dtype):
                    m_start_time = time.perf_counter()
                    wav, _ = self.bigvgan(latent, cond_input)
                    bigvgan_time += time.perf_counter() - m_start_time
                    wav = wav.squeeze(1)
                    pass
            wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
            wavs.append(wav.cpu()) # to cpu before saving

        # clear cache
        tqdm_progress.close()  # ç¢ºä¿é€²åº¦æ¢è¢«é—œé–‰
        del all_latents, chunk_latents
        end_time = time.perf_counter()
        self.torch_empty_cache()

        # wav audio output
        self._set_gr_progress(0.9, "save audio...")
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        print(f">> Reference audio length: {cond_mel_frame * 256 / sampling_rate:.2f} seconds")
        print(f">> gpt_gen_time: {gpt_gen_time:.2f} seconds")
        print(f">> gpt_forward_time: {gpt_forward_time:.2f} seconds")
        print(f">> bigvgan_time: {bigvgan_time:.2f} seconds")
        print(f">> Total fast inference time: {end_time - start_time:.2f} seconds")
        print(f">> Generated audio length: {wav_length:.2f} seconds")
        print(f">> [fast] bigvgan chunk_length: {chunk_length}")
        print(f">> [fast] batch_num: {all_batch_num} bucket_max_size: {bucket_max_size}", f"bucket_count: {bucket_count}" if bucket_max_size > 1 else "")
        print(f">> [fast] RTF: {(end_time - start_time) / wav_length:.4f}")

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # ç›´æ¥å„²å­˜éŸ³è¨Šåˆ°æŒ‡å®šè·¯å¾‘ä¸­
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            # ç›´æ¥ä½¿ç”¨ soundfile é¿å… torchaudio.save çš„ torchcodec ä¾è³´å•é¡Œ
            # å…ˆè½‰æˆ int16ï¼Œå†è½‰ numpyï¼ˆsoundfile æœƒæ­£ç¢ºè™•ç† int16ï¼‰
            wav_int16 = wav.squeeze(0).to(torch.float32).numpy().astype('int16')
            sf.write(output_path, wav_int16, sampling_rate, subtype='PCM_16')
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # è¿”å›ä»¥ç¬¦åˆGradioçš„æ ¼å¼è¦æ±‚
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)

    # åŸå§‹æ¨ç†æ¨¡å¼
    def infer(self, audio_prompt, text, output_path, verbose=False, max_text_tokens_per_sentence=120, speaker_id=None, **generation_kwargs):
        # é©—è­‰speaker_id
        if speaker_id is not None:
            if not hasattr(self, 'speaker_list') or not self.speaker_list:
                raise ValueError("Multi-speaker support not enabled. Please initialize with speaker_info_path.")
            if speaker_id not in self.speaker_list:
                raise ValueError(f"Invalid speaker_id: {speaker_id}. Available speakers: {self.speaker_list}")
        
        if verbose:
            print(f"origin text:{text}")
            if speaker_id:
                print(f"using speaker: {speaker_id}")
        start_time = time.perf_counter()

        # å¦‚æœåƒè€ƒéŸ³è¨Šæ”¹è®Šäº†ï¼Œæ‰éœ€è¦é‡æ–°ç”Ÿæˆ cond_mel, æå‡é€Ÿåº¦
        if self.cache_cond_mel is None or self.cache_audio_prompt != audio_prompt:
            audio, sr = sf.read(audio_prompt)
            audio = torch.from_numpy(audio.T if audio.ndim > 1 else audio.reshape(1, -1)).float()
            audio = torch.mean(audio, dim=0, keepdim=True)
            if audio.shape[0] > 1:
                audio = audio[0].unsqueeze(0)
            audio = torchaudio.transforms.Resample(sr, 24000)(audio)
            cond_mel = MelSpectrogramFeatures()(audio).to(self.device)
            cond_mel_frame = cond_mel.shape[-1]
            if verbose:
                print(f"cond_mel shape: {cond_mel.shape}", "dtype:", cond_mel.dtype)

            self.cache_audio_prompt = audio_prompt
            self.cache_cond_mel = cond_mel
        else:
            cond_mel = self.cache_cond_mel
            cond_mel_frame = cond_mel.shape[-1]
            pass

        self._set_gr_progress(0.1, "text processing...")
        auto_conditioning = cond_mel
        text_tokens_list = self.tokenizer.tokenize(text)
        sentences = self.tokenizer.split_sentences(text_tokens_list, max_text_tokens_per_sentence)
        if verbose:
            print("text token count:", len(text_tokens_list))
            print("sentences count:", len(sentences))
            print("max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 1.0)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 600)
        sampling_rate = 24000
        # lang = "EN"
        # lang = "ZH"
        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        bigvgan_time = 0
        progress = 0
        has_warned = False
        for sent in sentences:
            text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
            # text_tokens = F.pad(text_tokens, (0, 1))  # This may not be necessary.
            # text_tokens = F.pad(text_tokens, (1, 0), value=0)
            # text_tokens = F.pad(text_tokens, (0, 1), value=1)
            if verbose:
                print(text_tokens)
                print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                # debug tokenizer
                text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                print("text_token_syms is same as sentence tokens", text_token_syms == sent)

            # text_len = torch.IntTensor([text_tokens.size(1)], device=text_tokens.device)
            # print(text_len)
            progress += 1
            self._set_gr_progress(0.2 + 0.4 * (progress-1) / len(sentences), f"gpt inference latent... {progress}/{len(sentences)}")
            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    codes = self.gpt.inference_speech(auto_conditioning, text_tokens,
                                                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]],
                                                                                      device=text_tokens.device),
                                                        speaker_ids=[speaker_id] if speaker_id else None,  # æ–°å¢é€™è¡Œ
                                                        do_sample=do_sample,
                                                        top_p=top_p,
                                                        top_k=top_k,
                                                        temperature=temperature,
                                                        num_return_sequences=autoregressive_batch_size,
                                                        length_penalty=length_penalty,
                                                        num_beams=num_beams,
                                                        repetition_penalty=repetition_penalty,
                                                        # ç§»é™¤ speaker_id=speaker_id é€™ä¸€è¡Œ
                                                        )
                gpt_gen_time += time.perf_counter() - m_start_time
                if not has_warned and (codes[:, -1] != self.stop_mel_token).any():
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Input text tokens: {text_tokens.shape[1]}. "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True

                code_lens = torch.tensor([codes.shape[-1]], device=codes.device, dtype=codes.dtype)
                if verbose:
                    print(codes, type(codes))
                    print(f"codes shape: {codes.shape}, codes type: {codes.dtype}")
                    print(f"code len: {code_lens}")

                # remove ultra-long silence if exits
                # temporarily fix the long silence bug.
                codes, code_lens = self.remove_long_silence(codes, silent_token=52, max_consecutive=30)
                if verbose:
                    print(codes, type(codes))
                    print(f"fix codes shape: {codes.shape}, codes type: {codes.dtype}")
                    print(f"code len: {code_lens}")
                self._set_gr_progress(0.2 + 0.4 * progress / len(sentences), f"gpt inference speech... {progress}/{len(sentences)}")
                m_start_time = time.perf_counter()
                # latent, text_lens_out, code_lens_out = \
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    latent = self.gpt(
                        speech_conditioning_latent=auto_conditioning,  # ä¿®æ­£ï¼šmel_spec -> speech_conditioning_latent
                        text_inputs=text_tokens,                        # ä¿®æ­£ï¼štext_ids -> text_inputs
                        text_lengths=torch.tensor([text_tokens.shape[-1]], device=text_tokens.device),
                        mel_codes=codes,
                        wav_lengths=code_lens*self.gpt.mel_length_compression,  # ä¿®æ­£ï¼šcodes_lengths -> wav_lengths
                        cond_mel_lengths=torch.tensor([auto_conditioning.shape[-1]], device=text_tokens.device),  # ä¿®æ­£ï¼šmel_lengths -> cond_mel_lengths
                        speaker_ids=[speaker_id] if speaker_id else None,
                        return_latent=True
                    )
                    gpt_forward_time += time.perf_counter() - m_start_time

                # Determine autocast settings for vocoder
                vocoder_autocast_enabled = self.vocoder_dtype != torch.float32
                vocoder_autocast_dtype = self.vocoder_dtype if vocoder_autocast_enabled else None

                # Explicitly cast inputs if vocoder is FP32
                if not vocoder_autocast_enabled:
                    latent = latent.float()
                    cond_input = auto_conditioning.transpose(1, 2).float()
                else:
                    cond_input = auto_conditioning.transpose(1, 2)

                with torch.amp.autocast(text_tokens.device.type, enabled=vocoder_autocast_enabled, dtype=vocoder_autocast_dtype):
                    m_start_time = time.perf_counter()
                    wav, _ = self.bigvgan(latent, cond_input)
                    bigvgan_time += time.perf_counter() - m_start_time
                    wav = wav.squeeze(1)

                wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
                if verbose:
                    print(f"wav shape: {wav.shape}", "min:", wav.min(), "max:", wav.max())
                # wavs.append(wav[:, :-512])
                wavs.append(wav.cpu())  # to cpu before saving
        end_time = time.perf_counter()
        self._set_gr_progress(0.9, "save audio...")
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        print(f">> Reference audio length: {cond_mel_frame * 256 / sampling_rate:.2f} seconds")
        print(f">> gpt_gen_time: {gpt_gen_time:.2f} seconds")
        print(f">> gpt_forward_time: {gpt_forward_time:.2f} seconds")
        print(f">> bigvgan_time: {bigvgan_time:.2f} seconds")
        print(f">> Total inference time: {end_time - start_time:.2f} seconds")
        print(f">> Generated audio length: {wav_length:.2f} seconds")
        print(f">> RTF: {(end_time - start_time) / wav_length:.4f}")

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # ç›´æ¥å„²å­˜éŸ³è¨Šåˆ°æŒ‡å®šè·¯å¾‘ä¸­
            if os.path.isfile(output_path):
                os.remove(output_path)
                print(">> remove old wav file:", output_path)
            if os.path.dirname(output_path) != "":
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
            # ç›´æ¥ä½¿ç”¨ soundfile é¿å… torchaudio.save çš„ torchcodec ä¾è³´å•é¡Œ
            # å…ˆè½‰æˆ int16ï¼Œå†è½‰ numpyï¼ˆsoundfile æœƒæ­£ç¢ºè™•ç† int16ï¼‰
            wav_int16 = wav.squeeze(0).to(torch.float32).numpy().astype('int16')
            sf.write(output_path, wav_int16, sampling_rate, subtype='PCM_16')
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # è¿”å›ä»¥ç¬¦åˆGradioçš„æ ¼å¼è¦æ±‚
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)


if __name__ == "__main__":
    set_seed(1234)
    
    # æŒ‡å®šèªªè©±äººè³‡è¨Šæª”æ¡ˆ
    speaker_info_path = "finetune_data/processed_data/speaker_info.json"

    ifile = sys.argv[1]
    target_txt_list = []
    with open(ifile, 'r') as f:
        for line in f:
            line = line.strip()
            uid, prompt_txt, prompt_wav, target_txt = line.split('|')
            target_txt_list.append((uid, target_txt))
    
    # åˆå§‹åŒ–TTSï¼Œè¼‰å…¥å¤šèªªè©±äººæ”¯æ´
    tts = IndexTTS(
        cfg_path="checkpoints/config.yaml", 
        model_dir="checkpoints", 
        is_fp16=True, 
        use_cuda_kernel=False,
        speaker_info_path=speaker_info_path  # æ–°å¢å¼•æ•¸
    )

    prompts = [
        ("kaishu_30min", "/path/to/prompt.wav"),
        ]
    
    for speaker_id, prompt_wav in prompts:

        output_dir = f"result/{speaker_id}_{os.path.basename(ifile).rstrip('.lst')}_{time.strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(output_dir, exist_ok=True)

        # ä½¿ç”¨ä¸åŒèªªè©±äººé€²è¡Œæ¨ç†
        for i, (uid, target_txt) in enumerate(target_txt_list):

            output_wav_path = f"{output_dir}/{uid}.wav"
            tts.infer(
                audio_prompt=prompt_wav, 
                text=target_txt, 
                output_path=output_wav_path, 
                verbose=True,
                speaker_id=speaker_id  # æ–°å¢å¼•æ•¸
            )
