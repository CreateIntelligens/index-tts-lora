#!/usr/bin/env python3
"""
éŸ³è¨Šç‰¹å¾µæå–å·¥å…·

è©²æŒ‡ä»¤ç¢¼ç”¨æ–¼è™•ç†éŸ³è¨Šè³‡æ–™ï¼Œæå–æ¢…çˆ¾é »è­œç‰¹å¾µã€é›¢æ•£ç¨‹å¼ç¢¼æœ¬ç´¢å¼•å’Œcondition latentï¼Œ
ä¸¦è¨ˆç®—medoidæ¨£æœ¬ç”¨æ–¼èªéŸ³åˆæˆæ¨¡å‹çš„è¨“ç·´ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. éŸ³è¨Šè³‡æ–™é è™•ç†å’Œç‰¹å¾µæå–
2. é›¢æ•£è®Šåˆ†è‡ªç·¨ç¢¼å™¨(DVAE)ç·¨ç¢¼
3. Condition latentæå–
4. Medoidæ¨£æœ¬è¨ˆç®—
5. è¨“ç·´/é©—è­‰é›†åˆ†å‰²
"""

import argparse
import gc
import json
import os
import sys
import threading
from datetime import datetime
from pathlib import Path
from queue import Empty, Queue
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
import soundfile as sf
from loguru import logger
from omegaconf import OmegaConf
from tqdm import tqdm

# ç¢ºä¿å°ˆæ¡ˆæ ¹ç›®éŒ„åœ¨ Python è·¯å¾‘ä¸­
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from indextts.utils.feature_extractors import MelSpectrogramFeatures
from indextts.vqvae.xtts_dvae import DiscreteVAE

# å¸¸é‡å®šç¾©
DEFAULT_OUTPUT_DIR = "finetune_data/processed_data/"
DEFAULT_MODEL_PATH = "checkpoints/gpt.pth.open_source"
FINETUNE_MODEL_DIR = "finetune_models"
CONFIG_FILENAME = "config.yaml"
METADATA_FILENAME = "metadata.jsonl"
TRAIN_SPLIT_RATIO = 0.9
CONDITION_LATENT_DIM = 32


class AudioDataset(torch.utils.data.Dataset):
    """éŸ³é »æ•¸æ“šé›†ï¼Œç”¨æ–¼æ‰¹æ¬¡åŠ è¼‰å’Œè™•ç†"""

    def __init__(self, audio_list_path: str):
        """
        Args:
            audio_list_path: audio_list.txt æ–‡ä»¶è·¯å¾‘
        """
        self.samples = []

        # è§£æ audio_list.txt
        with open(audio_list_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    parts = line.split('\t')
                    if len(parts) == 2:
                        wav_path, text = parts
                        if os.path.exists(wav_path):
                            self.samples.append({
                                'wav_path': wav_path,
                                'text': text
                            })
                except Exception as e:
                    logger.warning(f"è§£æå¤±æ•—: {line}, {e}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Returns:
            dict: {
                'wav_path': str,
                'text': str,
                'audio': torch.Tensor or None,
                'sr': int or None,
                'success': bool
            }
        """
        sample = self.samples[idx]

        try:
            audio, sr = sf.read(sample["wav_path"])
            audio = torch.from_numpy(audio.T if audio.ndim > 1 else audio.reshape(1, -1)).float()
            return {
                'wav_path': sample['wav_path'],
                'text': sample['text'],
                'audio': audio,
                'sr': sr,
                'success': True
            }
        except Exception as e:
            logger.warning(f"åŠ è¼‰å¤±æ•—: {sample['wav_path']}, {e}")
            return {
                'wav_path': sample['wav_path'],
                'text': sample['text'],
                'audio': None,
                'sr': None,
                'success': False
            }


def collate_batch(batch):
    """
    æ•´ç†æ‰¹æ¬¡æ•¸æ“š

    Args:
        batch: List[dict] from Dataset.__getitem__

    Returns:
        dict: {
            'wav_paths': List[str],
            'texts': List[str],
            'audios': List[torch.Tensor],
            'srs': List[int],
            'valid_indices': List[int]
        }
    """
    wav_paths = []
    texts = []
    audios = []
    srs = []
    valid_indices = []

    for i, item in enumerate(batch):
        if item['success']:
            wav_paths.append(item['wav_path'])
            texts.append(item['text'])
            audios.append(item['audio'])
            srs.append(item['sr'])
            valid_indices.append(i)

    return {
        'wav_paths': wav_paths,
        'texts': texts,
        'audios': audios,
        'srs': srs,
        'valid_indices': valid_indices
    }


def get_host_uid_gid() -> Tuple[int, int]:
    """å˜—è©¦æ¨æ–·å®¿ä¸»æ©Ÿçš„ UID/GID ä»¥ä¿®å¾©æª”æ¡ˆæ¬Šé™"""
    check_files = [
        'docker-compose.yml',
        'Dockerfile',
        'run.sh',
        'webui.py',
        'train.py',
    ]

    for filename in check_files:
        filepath = Path(filename)
        if filepath.exists():
            stat_info = filepath.stat()
            uid = stat_info.st_uid
            gid = stat_info.st_gid
            if uid != 0:
                return uid, gid

    return 1000, 1000


def fix_permissions(path: Union[str, Path]) -> None:
    """å°‡æª”æ¡ˆæˆ–ç›®éŒ„æ¬Šé™èª¿æ•´ç‚ºå®¿ä¸»ä½¿ç”¨è€…"""
    target = Path(path)
    try:
        uid, gid = get_host_uid_gid()
        os.chown(target, uid, gid)
    except PermissionError:
        logger.warning(f"ç„¡æ³•èª¿æ•´æ¬Šé™ (PermissionError): {target}")
    except FileNotFoundError:
        logger.warning(f"ç„¡æ³•èª¿æ•´æ¬Šé™ (éºå¤±): {target}")
    except Exception as exc:
        logger.warning(f"ç„¡æ³•èª¿æ•´æ¬Šé™: {target} -> {exc}")


class AudioProcessor:
    """éŸ³è¨Šè™•ç†å™¨é¡ï¼Œå°è£éŸ³è¨Šç‰¹å¾µæå–ç›¸é—œåŠŸèƒ½"""
    
    def __init__(self, dvae: DiscreteVAE, mel_config: Dict, device: str = 'cuda'):
        self.dvae = dvae
        self.mel_config = mel_config
        self.device = device
        self.mel_feature = MelSpectrogramFeatures(**mel_config).to(self.device)
        self.sample_rate = self.mel_config['sample_rate']
        self._resamplers: Dict[int, torchaudio.transforms.Resample] = {}

    def _get_resampler(self, source_sr: int) -> torchaudio.transforms.Resample:
        """å–å¾—å°æ‡‰è¼¸å…¥å–æ¨£ç‡çš„é‡å–æ¨£å™¨ä¸¦æ¬åˆ°æ­£ç¢ºè£ç½®"""
        if source_sr not in self._resamplers:
            resampler = torchaudio.transforms.Resample(
                orig_freq=source_sr,
                new_freq=self.sample_rate
            ).to(self.device)
            self._resamplers[source_sr] = resampler
        return self._resamplers[source_sr]
    
    @torch.no_grad()
    def process_audio_data(
        self,
        audio: Union[torch.Tensor, np.ndarray],
        sr: int
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è™•ç†éŸ³è¨Šè³‡æ–™ï¼ŒåŒ…æ‹¬æå–æ¢…çˆ¾é »è­œç‰¹å¾µã€ç²å–é›¢æ•£ç¨‹å¼ç¢¼æœ¬ç´¢å¼•ç­‰ã€‚

        Args:
            audio: è¼¸å…¥çš„éŸ³è¨Šè³‡æ–™
            sr: éŸ³è¨Šçš„å–æ¨£ç‡

        Returns:
            è™•ç†å¾Œçš„éŸ³è¨Šè³‡æ–™ã€æ¢…çˆ¾é »è­œç‰¹å¾µå’Œé›¢æ•£ç¨‹å¼ç¢¼æœ¬ç´¢å¼•
        """
        # è³‡æ–™å‹åˆ¥è½‰æ›
        if isinstance(audio, np.ndarray):
            audio = torch.from_numpy(audio)

        audio = audio.to(self.device)
        
        # é‡å–æ¨£
        if sr != self.sample_rate:
            resampler = self._get_resampler(sr)
            audio = resampler(audio)
        
        # æå–æ¢…çˆ¾é »è­œç‰¹å¾µ
        mel = self.mel_feature(audio)
        
        # ç²å–é›¢æ•£ç¨‹å¼ç¢¼æœ¬ç´¢å¼•
        codes = self.dvae.get_codebook_indices(mel)
        
        # è™•ç†éŸ³è¨Šç¶­åº¦
        if audio.ndim > 1 and audio.shape[0] == 1:
            audio = audio.squeeze(0)
        
        return audio, mel, codes


class ConditionExtractor:
    """Condition latentæå–å™¨é¡"""
    
    def __init__(self, model, device: str = 'cuda'):
        self.model = model
        self.device = device
    
    def extract_condition_latent(
        self, 
        mel: torch.Tensor
    ) -> np.ndarray:
        """
        å¾æ¢…çˆ¾é »è­œä¸­æå–condition latent
        
        Args:
            mel: æ¢…çˆ¾é »è­œç‰¹å¾µ (1, mel_dim, T) æˆ– (mel_dim, T)
        
        Returns:
            condition latent (32, dim)
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹ä¸èƒ½ç‚ºNoneï¼Œéœ€è¦å‚³å…¥å·²è¼‰å…¥çš„UnifiedVoiceæ¨¡å‹")
        
        # ç¢ºä¿melè³‡æ–™æ ¼å¼æ­£ç¢º
        if isinstance(mel, np.ndarray):
            mel = torch.from_numpy(mel)
        
        # ç¢ºä¿melæ˜¯3ç¶­çš„ (1, mel_dim, T)
        if mel.ndim == 2:
            mel = mel.unsqueeze(0)
        elif mel.ndim == 3 and mel.shape[0] == 1:
            pass
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„melå½¢ç‹€: {mel.shape}")
        
        mel = mel.to(self.device)
        mel_length = torch.tensor([mel.shape[-1]], device=self.device)
        
        # ä½¿ç”¨UnifiedVoiceæ¨¡å‹æå–condition latent
        with torch.no_grad():
            condition = self.model.get_conditioning(mel, mel_length)
            condition = condition.squeeze(0).cpu().float().numpy()

        return condition

    def extract_condition_latent_batch(self, mels: List[torch.Tensor]) -> List[np.ndarray]:
        """
        æ‰¹æ¬¡æå– condition latent

        Args:
            mels: List of mel spectrograms, each [1, mel_bins, T] or [mel_bins, T]

        Returns:
            List of condition latents, each (32, dim)
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹ä¸èƒ½ç‚ºNoneï¼Œéœ€è¦å‚³å…¥å·²è¼‰å…¥çš„UnifiedVoiceæ¨¡å‹")

        # æ¨™æº–åŒ–è¼¸å…¥æ ¼å¼
        normalized_mels = []
        for mel in mels:
            if isinstance(mel, np.ndarray):
                mel = torch.from_numpy(mel)
            # ç¢ºä¿æ˜¯3ç¶­
            if mel.ndim == 2:
                mel = mel.unsqueeze(0)
            elif mel.ndim == 3 and mel.shape[0] == 1:
                pass
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„melå½¢ç‹€: {mel.shape}")
            normalized_mels.append(mel.squeeze(0))  # [mel_bins, T]

        # æ‰¾åˆ°æœ€å¤§é•·åº¦ä¸¦ padding
        max_len = max(mel.shape[-1] for mel in normalized_mels)
        padded_mels = []
        mel_lengths = []

        for mel in normalized_mels:
            current_len = mel.shape[-1]
            mel_lengths.append(current_len)

            if current_len < max_len:
                pad_len = max_len - current_len
                mel = torch.nn.functional.pad(mel, (0, pad_len))

            padded_mels.append(mel)

        # Stack æˆ batch
        batch_mels = torch.stack(padded_mels).to(self.device)  # [B, mel_bins, T]
        mel_lengths = torch.tensor(mel_lengths, device=self.device)

        # æ‰¹æ¬¡æ¨ç†
        with torch.no_grad():
            batch_conditions = self.model.get_conditioning(batch_mels, mel_lengths)
            # è½‰å› list
            conditions = [
                cond.cpu().float().numpy()
                for cond in batch_conditions
            ]

        return conditions


class MedoidCalculator:
    """Medoidè¨ˆç®—å™¨é¡"""
    
    @staticmethod
    def _prepare_medoid_devices(
        default_device: str,
        medoid_devices: Optional[Union[str, List[Union[str, int]]]]
    ) -> List[torch.device]:
        """è§£æä½¿ç”¨è€…å‚³å…¥çš„è£ç½®åˆ—è¡¨"""
        if medoid_devices is None:
            candidates: List[str] = [default_device]
        elif isinstance(medoid_devices, str):
            candidates = [dev.strip() for dev in medoid_devices.split(',') if dev.strip()]
        else:
            candidates = []
            for dev in medoid_devices:
                if isinstance(dev, int):
                    candidates.append(f"cuda:{dev}")
                else:
                    candidates.append(str(dev))

        parsed_devices: List[torch.device] = []
        seen: set = set()
        for dev in candidates:
            if not dev:
                continue
            if dev.isdigit():
                dev = f"cuda:{dev}"
            try:
                torch_device = torch.device(dev)
            except Exception:
                logger.warning(f"ç„¡æ³•è§£æè£ç½® {dev}ï¼Œå·²å¿½ç•¥")
                continue

            if torch_device.type == 'cuda' and not torch.cuda.is_available():
                logger.warning(f"è£ç½® {dev} ä¸å¯ç”¨ï¼Œæ”¹ç”¨ CPU")
                continue

            key = str(torch_device)
            if key in seen:
                continue
            seen.add(key)
            parsed_devices.append(torch_device)

        if not parsed_devices:
            logger.warning("æœªæä¾›æœ‰æ•ˆçš„ GPU è£ç½®ï¼Œå°‡ä½¿ç”¨ CPU è¨ˆç®— medoid")
            parsed_devices = [torch.device('cpu')]

        return parsed_devices

    @staticmethod
    def compute_distance_matrix(
        data: np.ndarray,
        metric: str = 'euclidean',
        device: str = 'cpu',
        block_size: int = 512,
        medoid_devices: Optional[Union[str, List[Union[str, int]]]] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä»¥å€å¡Šæ–¹å¼è¨ˆç®—è·é›¢çŸ©é™£ï¼Œæ”¯æ´å¤š GPU å¹³è¡Œé‹ç®—

        Args:
            data: (N, features) æ•¸çµ„
            metric: è·é›¢åº¦é‡æ–¹å¼
            device: é è¨­è£ç½®
            block_size: æ¯å€‹è¨ˆç®—å€å¡Šçš„æ¨£æœ¬æ•¸
            medoid_devices: è‡ªè¨‚è£ç½®åˆ—è¡¨

        Returns:
            (è·é›¢çŸ©é™£, æ¯å€‹æ¨£æœ¬è·é›¢å’Œ)
        """
        n = data.shape[0]
        if n == 0:
            return np.zeros((0, 0), dtype=np.float32), np.zeros(0, dtype=np.float64)

        block_size = max(1, int(block_size))
        block_size = min(block_size, n)
        row_chunks = [(start, min(start + block_size, n)) for start in range(0, n, block_size)]
        num_chunks = len(row_chunks)

        devices = MedoidCalculator._prepare_medoid_devices(device, medoid_devices)
        data_tensor = torch.from_numpy(data).to(torch.float32)

        dist_matrix = np.zeros((n, n), dtype=np.float32)
        dist_sums = np.zeros(n, dtype=np.float64)
        chunk_locks = [threading.Lock() for _ in row_chunks]
        task_queue: Queue = Queue()

        for idx in range(num_chunks):
            task_queue.put(idx)

        def worker(device_obj: torch.device) -> None:
            nonlocal metric
            while True:
                try:
                    row_idx = task_queue.get_nowait()
                except Empty:
                    break

                start_i, end_i = row_chunks[row_idx]
                chunk_i = data_tensor[start_i:end_i].to(device_obj, non_blocking=True)

                try:
                    for col_idx in range(row_idx, num_chunks):
                        start_j, end_j = row_chunks[col_idx]
                        chunk_j = data_tensor[start_j:end_j].to(device_obj, non_blocking=True)

                        if metric == 'euclidean':
                            block = torch.cdist(chunk_i, chunk_j, p=2)
                        elif metric == 'cosine':
                            norm_i = F.normalize(chunk_i, dim=1)
                            norm_j = F.normalize(chunk_j, dim=1)
                            block = 1 - torch.matmul(norm_i, norm_j.transpose(0, 1))
                        else:
                            raise ValueError(f"ä¸æ”¯æ´çš„è·é›¢åº¦é‡: {metric}")

                        block_np = block.cpu().numpy().astype(np.float32)
                        dist_matrix[start_i:end_i, start_j:end_j] = block_np
                        dist_sums[start_i:end_i] += block_np.sum(axis=1, dtype=np.float64)

                        if col_idx != row_idx:
                            dist_matrix[start_j:end_j, start_i:end_i] = block_np.T
                            col_contrib = block_np.sum(axis=0, dtype=np.float64)
                            with chunk_locks[col_idx]:
                                dist_sums[start_j:end_j] += col_contrib

                        del chunk_j, block, block_np
                finally:
                    del chunk_i
                    task_queue.task_done()

        threads = []
        for dev in devices:
            thread = threading.Thread(target=worker, args=(dev,), daemon=True)
            thread.start()
            threads.append(thread)

        task_queue.join()
        for thread in threads:
            thread.join()

        return dist_matrix, dist_sums
    
    @classmethod
    def find_medoid_condition(
        cls,
        condition_files: List[str],
        distance_metric: str = 'euclidean',
        device: str = 'cpu',
        batch_size: int = 512,
        max_samples: int = 5000,
        gpu_ids: Optional[List[int]] = None,
        medoid_devices: Optional[List[str]] = None
    ) -> Dict:
        """
        æ‰¾å‡ºcondition latentåˆ†ä½ˆä¸­å¿ƒçš„æ¨£æœ¬ï¼ˆmedoidï¼‰

        Args:
            condition_files: conditionæª”æ¡ˆè·¯å¾‘åˆ—è¡¨
            distance_metric: è·é›¢åº¦é‡æ–¹å¼
            device: è¨ˆç®—è£ç½® ('cpu' æˆ– 'cuda')
            batch_size: è·é›¢çŸ©é™£è¨ˆç®—çš„æ‰¹æ¬¡å¤§å°ï¼ˆè¡Œï¼‰
            max_samples: åˆ—åˆ†å¡Šå¤§å°ï¼ˆç”¨æ–¼è¨˜æ†¶é«”å„ªåŒ–ï¼‰
            gpu_ids: GPU ID åˆ—è¡¨ï¼Œç”¨æ–¼å¤š GPU ä¸¦è¡Œï¼ˆä¾‹å¦‚ [0,1,2,3]ï¼‰

        Returns:
            medoidè³‡è¨Šå­—å…¸
        """
        total_files = len(condition_files)
        device_list = medoid_devices
        if device_list is None and gpu_ids:
            device_list = [f"cuda:{gid}" for gid in gpu_ids]
        use_multi_gpu = device_list is not None and len(device_list) > 1

        if use_multi_gpu:
            logger.info(f"é–‹å§‹è¨ˆç®—medoidï¼Œå…± {total_files} å€‹æ¨£æœ¬ï¼ˆå¤š GPU å€å¡Šè¨ˆç®—ï¼‰")
            logger.info(f"ä½¿ç”¨è£ç½®: {device_list}")
        else:
            logger.info(f"é–‹å§‹è¨ˆç®—medoidï¼Œå…± {total_files} å€‹æ¨£æœ¬ï¼ˆå–®è£ç½®å€å¡Šè¨ˆç®—ï¼‰")
        
        # è®€å–æ‰€æœ‰condition latent
        conditions = []
        condition_infos = []
        
        for condition_path in condition_files:
            if not os.path.exists(condition_path):
                logger.warning(f"Conditionæª”æ¡ˆä¸å­˜åœ¨: {condition_path}ï¼Œè·³é")
                continue
            
            try:
                cond = np.load(condition_path)
                conditions.append(cond)
                condition_infos.append({
                    'index': len(conditions) - 1,
                    'condition_path': condition_path
                })
            except Exception as e:
                logger.error(f"è¼‰å…¥conditionæª”æ¡ˆå¤±æ•—: {condition_path}, éŒ¯èª¤: {e}")
                continue
        
        if len(conditions) == 0:
            raise RuntimeError('æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„conditionæª”æ¡ˆ')
        
        logger.info(f"æˆåŠŸè®€å–{len(conditions)}å€‹condition latent")
        
        # å°‡æ‰€æœ‰conditionå †ç–Šæˆä¸€å€‹æ•¸çµ„
        conditions_array = np.stack(conditions, axis=0)
        logger.info(f"Conditionsé™£åˆ—å½¢ç‹€: {conditions_array.shape}")
        
        # å°‡è³‡æ–™å±•å¹³ä»¥ä¾¿è¨ˆç®—è·é›¢
        N, H, W = conditions_array.shape
        data_flat = conditions_array.reshape(N, H * W)
        
        # è¨ˆç®—è·é›¢çŸ©é™£ï¼ˆä½¿ç”¨åˆ†æ‰¹é¿å… OOMï¼‰
        logger.info("è¨ˆç®—è·é›¢çŸ©é™£...")
        block_size = max_samples if max_samples > 0 else batch_size
        block_size = max(1, min(batch_size, block_size))
        if block_size != batch_size:
            logger.info(f"æ¡ç”¨å€å¡Šå¤§å° {block_size} (ç”± batch_size={batch_size}, chunk_size={max_samples} è¨ˆç®—)")
        else:
            logger.info(f"æ¡ç”¨å€å¡Šå¤§å° {block_size}")

        dist_matrix, dist_sums = cls.compute_distance_matrix(
            data_flat,
            distance_metric,
            device,
            block_size=block_size,
            medoid_devices=device_list
        )

        # æ‰¾å‡ºmedoid
        logger.info("å°‹æ‰¾medoid...")
        medoid_idx = np.argmin(dist_sums)
        medoid_info = condition_infos[medoid_idx]
        medoid_condition = conditions[medoid_idx]
        
        logger.info(f"æ‰¾åˆ°medoid: ç´¢å¼•{medoid_idx}")
        logger.info(f"Medoidåˆ°æ‰€æœ‰æ¨£æœ¬çš„è·é›¢ä¹‹å’Œ: {dist_sums[medoid_idx]:.6f}")
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        stats = {
            'total_samples': len(conditions),
            'medoid_index': int(medoid_idx),
            'medoid_distance_sum': float(dist_sums[medoid_idx]),
            'distance_metric': distance_metric,
            'distance_stats': {
                'mean': float(np.mean(dist_sums)),
                'std': float(np.std(dist_sums)),
                'min': float(np.min(dist_sums)),
                'max': float(np.max(dist_sums))
            }
        }
        
        result = {
            'medoid_info': medoid_info,
            'medoid_condition': medoid_condition,
            'statistics': stats,
            'distance_matrix': dist_matrix
        }

        # æ¸…ç†è‡¨æ™‚è®Šæ•¸å’Œ GPU ç·©å­˜
        del conditions_array, data_flat, dist_matrix, dist_sums
        gc.collect()

        return result


def load_unified_voice_model(
    config_path: str, 
    model_path: str, 
    device: str = 'cuda'
):
    """
    è¼‰å…¥UnifiedVoiceæ¨¡å‹
    
    Args:
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘
        model_path: æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘
        device: è¨ˆç®—è£ç½®
    
    Returns:
        è¼‰å…¥å¥½çš„UnifiedVoiceæ¨¡å‹
    """
    sys.path.append('.')
    from indextts.gpt.model import UnifiedVoice
    
    # è¼‰å…¥é…ç½®
    cfg = OmegaConf.load(config_path)
    
    # å»ºç«‹æ¨¡å‹
    model = UnifiedVoice(**cfg.gpt)
    
    # è¼‰å…¥é è¨“ç·´æ¬Šé‡
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æª¢æŸ¥é»æª”æ¡ˆæœªæ‰¾åˆ°: {model_path}")
    
    logger.info(f"æ­£åœ¨è¼‰å…¥UnifiedVoiceæ¨¡å‹æª¢æŸ¥é»: {model_path}")
    state = torch.load(model_path, map_location='cpu', weights_only=True)
    model.load_state_dict(
        state['model'] if 'model' in state else state, 
        strict=True
    )
    model.eval().to(device)
    
    logger.info("UnifiedVoiceæ¨¡å‹è¼‰å…¥å®Œæˆ")
    return model


def parse_audio_line(line: str) -> Tuple[str, str]:
    """è§£æéŸ³è¨Šåˆ—è¡¨æª”æ¡ˆä¸­çš„ä¸€è¡Œ"""
    line = line.strip()
    if not line:
        raise ValueError("ç©ºè¡Œ")
    
    if "\t" in line:
        return line.split("\t", 1)
    elif "|" in line:
        return line.split("|", 1)
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ ¼å¼: {line}")


def save_medoid_results(
    medoid_result: Dict, 
    output_dir: str
) -> None:
    """å„²å­˜medoidè¨ˆç®—çµæœ"""
    # å„²å­˜medoidè³‡è¨Š
    medoid_info_path = os.path.join(output_dir, "medoid_info.json")
    with open(medoid_info_path, 'w', encoding='utf-8') as f:
        json.dump({
            'medoid_info': medoid_result['medoid_info'],
            'statistics': medoid_result['statistics'],
            'medoid_condition_shape': list(medoid_result['medoid_condition'].shape)
        }, f, ensure_ascii=False, indent=2)
    fix_permissions(medoid_info_path)
    
    # å„²å­˜medoid condition
    medoid_condition_path = os.path.join(output_dir, "medoid_condition.npy")
    np.save(medoid_condition_path, medoid_result['medoid_condition'])
    fix_permissions(medoid_condition_path)
    
    # å„²å­˜è·é›¢çŸ©é™£
    distance_matrix_path = os.path.join(output_dir, "distance_matrix.npy")
    np.save(distance_matrix_path, medoid_result['distance_matrix'])
    fix_permissions(distance_matrix_path)
    
    logger.info("Medoidè¨ˆç®—å®Œæˆ:")
    logger.info(f"  - Medoidè³‡è¨Š: {medoid_info_path}")
    logger.info(f"  - Medoid condition: {medoid_condition_path}")
    logger.info(f"  - è·é›¢çŸ©é™£: {distance_matrix_path}")
    logger.info(f"  - Medoidæ¨£æœ¬: {medoid_result['medoid_info']['condition_path']}")


def split_dataset(
    metadata_file: str, 
    output_dir: str
) -> Tuple[str, str]:
    """åˆ†å‰²è³‡æ–™é›†ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†"""
    with open(metadata_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # éš¨æ©Ÿæ‰“äº‚è³‡æ–™
    np.random.shuffle(lines)

    # è¨ˆç®—åˆ†å‰²é»
    valid_size = int(len(lines) * (1 - TRAIN_SPLIT_RATIO))

    
    # åˆ†å‰²è³‡æ–™
    valid_lines = lines[-valid_size:]
    train_lines = lines[:-valid_size]
    
    # å„²å­˜è¨“ç·´é›†
    train_file = os.path.join(output_dir, 'metadata_train.jsonl')
    with open(train_file, 'w', encoding='utf-8') as f:
        f.writelines(train_lines)
    fix_permissions(train_file)
    
    # å„²å­˜é©—è­‰é›†
    valid_file = os.path.join(output_dir, 'metadata_valid.jsonl')
    with open(valid_file, 'w', encoding='utf-8') as f:
        f.writelines(valid_lines)
    fix_permissions(valid_file)
    
    logger.info("è³‡æ–™é›†åˆ†å‰²å®Œæˆ:")
    logger.info(f"  - è¨“ç·´é›†: {train_file} ({len(train_lines)}æ¢è³‡æ–™)")
    logger.info(f"  - é©—è­‰é›†: {valid_file} ({len(valid_lines)}æ¢è³‡æ–™)")
    
    return train_file, valid_file


def check_and_generate_speaker_info(output_dir: str) -> None:
    """æª¢æŸ¥æ‰€æœ‰ part æ˜¯å¦è™•ç†å®Œæˆï¼Œå¦‚æœæ˜¯å‰‡ç”Ÿæˆ speaker_info.json"""
    import glob
    from collections import defaultdict

    processed_data_dir = os.path.dirname(output_dir)

    # æª¢æŸ¥æœ‰å¤šå°‘å€‹ audio_list part æª”æ¡ˆ
    audio_list_dir = os.path.join(processed_data_dir, "..", "audio_list")
    audio_list_files = glob.glob(os.path.join(audio_list_dir, "audio_list_part_*.txt"))
    total_parts = len(audio_list_files)

    # æª¢æŸ¥æœ‰å¤šå°‘å€‹å·²è™•ç†çš„ part ç›®éŒ„
    processed_dirs = glob.glob(os.path.join(processed_data_dir, 'audio_list_part_*'))
    completed_parts = len([d for d in processed_dirs if os.path.exists(os.path.join(d, 'metadata.jsonl'))])

    logger.info(f"é€²åº¦: {completed_parts}/{total_parts} å€‹ part å·²è™•ç†å®Œæˆ")

    if completed_parts < total_parts:
        logger.info("ç­‰å¾…å…¶ä»– part è™•ç†å®Œæˆ...")
        return

    # æ‰€æœ‰ part éƒ½è™•ç†å®Œäº†ï¼Œç”Ÿæˆ speaker_info.json
    logger.info("ğŸ‰ æ‰€æœ‰ part å·²è™•ç†å®Œæˆï¼Œé–‹å§‹ç”Ÿæˆ speaker_info.json...")

    speaker_info_file = os.path.join(processed_data_dir, 'speaker_info.json')
    lock_file = speaker_info_file + '.lock'

    # ä½¿ç”¨æª”æ¡ˆé–ç¢ºä¿åªæœ‰ä¸€å€‹é€²ç¨‹ç”Ÿæˆ
    import fcntl
    try:
        with open(lock_file, 'w') as lock:
            fcntl.flock(lock.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)

            # æª¢æŸ¥æ˜¯å¦å·²ç¶“ç”Ÿæˆéäº†
            if os.path.exists(speaker_info_file):
                logger.info("speaker_info.json å·²å­˜åœ¨ï¼Œè·³éç”Ÿæˆ")
                fcntl.flock(lock.fileno(), fcntl.LOCK_UN)
                return

            # æ”¶é›†æ‰€æœ‰ speaker è³‡è¨Š
            speaker_data = defaultdict(list)
            total_samples = 0

            for proc_dir in sorted(processed_dirs):
                metadata_file = os.path.join(proc_dir, 'metadata.jsonl')
                if not os.path.exists(metadata_file):
                    continue

                with open(metadata_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        data = json.loads(line)
                        audio_path = data["audio"]
                        path_parts = Path(audio_path).parts

                        # æå– speaker ID
                        data_idx = path_parts.index("data") if "data" in path_parts else -1
                        if data_idx >= 0 and len(path_parts) > data_idx + 2:
                            drama_name = path_parts[data_idx + 1]
                            character_id = path_parts[data_idx + 2]
                            speaker_id = f"{drama_name}_{character_id}"
                        else:
                            speaker_id = os.path.basename(proc_dir)

                        speaker_data[speaker_id].append(data)
                        total_samples += 1

            # ç”Ÿæˆ speaker_info
            speaker_info_list = []
            for speaker_id in sorted(speaker_data.keys()):
                lines = speaker_data[speaker_id]
                total_duration = sum(line["duration"] for line in lines)
                ref_dir = os.path.dirname(lines[0]["codes"])

                speaker_info = {
                    "speaker": speaker_id,
                    "avg_duration": total_duration / len(lines),
                    "sample_num": len(lines),
                    "total_duration": total_duration,
                    "train_jsonl": os.path.abspath(os.path.join(ref_dir, "metadata_train.jsonl")),
                    "valid_jsonl": os.path.abspath(os.path.join(ref_dir, "metadata_valid.jsonl")),
                    "medoid_condition": os.path.abspath(os.path.join(ref_dir, "medoid_condition.npy")),
                }
                speaker_info_list.append(speaker_info)

            # å„²å­˜
            with open(speaker_info_file, 'w', encoding='utf-8') as f:
                json.dump(speaker_info_list, f, ensure_ascii=False, indent=4)
            fix_permissions(speaker_info_file)

            logger.info(f"âœ… speaker_info.json ç”Ÿæˆå®Œæˆ")
            logger.info(f"   - ç¸½ speaker æ•¸: {len(speaker_info_list)}")
            logger.info(f"   - ç¸½æ¨£æœ¬æ•¸: {sum(s['sample_num'] for s in speaker_info_list)}")

            fcntl.flock(lock.fileno(), fcntl.LOCK_UN)

    except BlockingIOError:
        # å…¶ä»–é€²ç¨‹æ­£åœ¨ç”Ÿæˆï¼Œè·³é
        logger.info("å…¶ä»–é€²ç¨‹æ­£åœ¨ç”Ÿæˆ speaker_info.jsonï¼Œè·³é")
        return


def save_speaker_info(
    audio_list_path: str,
    output_dir: str,
    lines: List[str]
) -> None:
    """å„²å­˜èªªè©±äººè³‡è¨Š"""
    # å¾ WAV è·¯å¾‘æå– speaker ID (äººç‰©ç›®éŒ„)
    first_line = json.loads(lines[0])
    wav_path = first_line["audio"]  # metadata ä¸­çš„æ¬„ä½åç¨±æ˜¯ "audio"
    path_parts = Path(wav_path).parts
    data_idx = path_parts.index("data") if "data" in path_parts else -1
    if data_idx >= 0 and len(path_parts) > data_idx + 2:
        drama_name = path_parts[data_idx + 1]
        character_id = path_parts[data_idx + 2]
        speaker_id = f"{drama_name}_{character_id}"
    else:
        speaker_id = Path(audio_list_path).stem

    total_duration = sum(json.loads(line)["duration"] for line in lines)
    speaker_info = {
        "speaker": speaker_id,
        "avg_duration": total_duration / len(lines),
        "sample_num": len(lines),
        "total_duration": total_duration,
        "train_jsonl": os.path.abspath(os.path.join(output_dir, "metadata_train.jsonl")),
        "valid_jsonl": os.path.abspath(os.path.join(output_dir, "metadata_valid.jsonl")),
        "medoid_condition": os.path.abspath(os.path.join(output_dir, "medoid_condition.npy")),
    }
    
    speaker_info_file = os.path.join(output_dir, "..", 'speaker_info.json')
    lock_file = speaker_info_file + '.lock'

    # ä½¿ç”¨æª”æ¡ˆé–é¿å…å¤š GPU åŒæ™‚å¯«å…¥
    import fcntl
    import time

    max_retries = 10
    for attempt in range(max_retries):
        try:
            # å–å¾—é–
            with open(lock_file, 'w') as lock:
                fcntl.flock(lock.fileno(), fcntl.LOCK_EX)

                # è®€å–ç¾æœ‰è³‡è¨Šæˆ–å»ºç«‹æ–°åˆ—è¡¨
                if os.path.exists(speaker_info_file):
                    try:
                        with open(speaker_info_file, 'r', encoding='utf-8') as f:
                            speaker_info_list = json.load(f)
                        logger.debug(f"è®€å–åˆ° {len(speaker_info_list)} å€‹ç¾æœ‰ speaker")
                    except (json.JSONDecodeError, IOError) as e:
                        # å¦‚æœæª”æ¡ˆæå£ï¼Œé‡æ–°å»ºç«‹
                        logger.warning(f"speaker_info.json æå£ï¼Œé‡æ–°å»ºç«‹: {e}")
                        speaker_info_list = []
                else:
                    logger.debug("speaker_info.json ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°æª”æ¡ˆ")
                    speaker_info_list = []

                # ç§»é™¤èˆŠçš„åŒå speakerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                current_speaker_id = speaker_info["speaker"]
                speaker_info_list = [s for s in speaker_info_list if s["speaker"] != current_speaker_id]

                # åŠ å…¥æ–°çš„ speaker è³‡è¨Š
                speaker_info_list.append(speaker_info)

                # å„²å­˜æ›´æ–°å¾Œçš„è³‡è¨Š
                with open(speaker_info_file, 'w', encoding='utf-8') as f:
                    json.dump(speaker_info_list, f, ensure_ascii=False, indent=4)
                fix_permissions(speaker_info_file)

                # é‡‹æ”¾é–
                fcntl.flock(lock.fileno(), fcntl.LOCK_UN)
                break
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"å¯«å…¥ speaker_info.json å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}")
                time.sleep(0.5)
            else:
                raise


def setup_models(
    config: OmegaConf, 
    args: argparse.Namespace
) -> Tuple[DiscreteVAE, Optional[object]]:
    """è¨­å®šå’Œè¼‰å…¥æ¨¡å‹"""
    # è¼‰å…¥DiscreteVAEæ¨¡å‹
    logger.info("æ­£åœ¨è¼‰å…¥ DiscreteVAE æ¨¡å‹...")
    dvae = DiscreteVAE(**config.vqvae)
    
    dvae_checkpoint_path = os.path.join(FINETUNE_MODEL_DIR, config.dvae_checkpoint)
    pre_trained_dvae = torch.load(
        dvae_checkpoint_path, 
        map_location=args.device, 
        weights_only=True
    )
    dvae.load_state_dict(
        pre_trained_dvae["model"] if "model" in pre_trained_dvae else pre_trained_dvae,
        strict=True
    )
    dvae.eval()
    dvae.to(args.device)
    del pre_trained_dvae
    
    # è¼‰å…¥UnifiedVoiceæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    unified_voice_model = None
    if args.extract_condition:
        config_path = os.path.join(FINETUNE_MODEL_DIR, CONFIG_FILENAME)
        unified_voice_model = load_unified_voice_model(
            config_path, args.model_path, args.device
        )
    
    return dvae, unified_voice_model


def process_audio_files(
    args: argparse.Namespace,
    config: OmegaConf,
    audio_processor: AudioProcessor,
    condition_extractor: Optional[ConditionExtractor],
    output_dir: str,
    batch_size: int = 16,
    num_workers: int = 8
) -> Tuple[str, List[str]]:
    """
    æ‰¹æ¬¡è™•ç†éŸ³è¨Šæª”æ¡ˆåˆ—è¡¨

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆå»ºè­° 8-16ï¼‰
        num_workers: DataLoader worker æ•¸é‡ï¼ˆå»ºè­° 4-8ï¼‰
    """
    metadata_file = os.path.join(output_dir, METADATA_FILENAME)
    metadata_tmp_file = metadata_file + ".tmp"
    condition_files = []
    metadata_path = Path(metadata_file)
    metadata_tmp_path = Path(metadata_tmp_file)

    # å¦‚æœå­˜åœ¨èˆŠçš„ tmp æ–‡ä»¶ï¼Œå…ˆåˆªé™¤
    if metadata_tmp_path.exists():
        metadata_tmp_path.unlink()

    # å‰µå»º Dataset å’Œ DataLoader
    dataset = AudioDataset(args.audio_list)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_batch,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False
    )

    try:
        # æ‰¹æ¬¡è™•ç†
        with tqdm(total=len(dataset), desc="è™•ç†éŸ³è¨Šæª”æ¡ˆ") as pbar:
            for batch in dataloader:
                # è·³éç©ºæ‰¹æ¬¡
                if len(batch['valid_indices']) == 0:
                    pbar.update(batch_size)
                    continue

                # æ‰¹æ¬¡è™•ç†éŸ³é »
                processed_batch = []
                for audio, sr, wav_path, text in zip(
                    batch['audios'],
                    batch['srs'],
                    batch['wav_paths'],
                    batch['texts']
                ):
                    try:
                        # è™•ç†éŸ³è¨Šè³‡æ–™
                        processed_audio, mel, codes = audio_processor.process_audio_data(audio, sr)

                        processed_batch.append({
                            'wav_path': wav_path,
                            'text': text,
                            'processed_audio': processed_audio,
                            'mel': mel,
                            'codes': codes,
                            'success': True
                        })
                    except Exception as e:
                        logger.error(f"è™•ç†éŸ³è¨Šå¤±æ•—: {wav_path}, {e}")
                        processed_batch.append({
                            'wav_path': wav_path,
                            'success': False
                        })

                # æ‰¹æ¬¡æå– conditionï¼ˆå¦‚æœéœ€è¦ï¼‰
                if condition_extractor is not None:
                    # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„ mel
                    mels_batch = [item['mel'] for item in processed_batch if item['success']]

                    if len(mels_batch) > 0:
                        try:
                            # æ‰¹æ¬¡æ¨ç† condition
                            conditions = condition_extractor.extract_condition_latent_batch(mels_batch)

                            # åˆ†é…å›å„å€‹æ¨£æœ¬
                            cond_idx = 0
                            for item in processed_batch:
                                if item['success']:
                                    item['condition'] = conditions[cond_idx]
                                    cond_idx += 1
                        except Exception as e:
                            logger.error(f"æ‰¹æ¬¡æå– condition å¤±æ•—: {e}")
                            # é™ç´šåˆ°é€å€‹è™•ç†
                            for item in processed_batch:
                                if item['success']:
                                    try:
                                        item['condition'] = condition_extractor.extract_condition_latent(item['mel'])
                                    except Exception as e:
                                        logger.error(f"æå– condition å¤±æ•—: {item['wav_path']}, {e}")
                                        item['success'] = False

                # æ‰¹æ¬¡ä¿å­˜çµæœ
                for item in processed_batch:
                    if not item['success']:
                        continue

                    try:
                        # è¨ˆç®—éŸ³è¨Šæ™‚é•·
                        duration = item['processed_audio'].shape[-1] / config.dataset.mel.sample_rate

                        # ä¿å­˜ç‰¹å¾µæ–‡ä»¶
                        base_name = os.path.basename(item['wav_path'])
                        out_codebook = os.path.join(output_dir, f"{base_name}_codes.npy")
                        out_mel = os.path.join(output_dir, f"{base_name}_mel.npy")

                        np.save(out_codebook, item['codes'].detach().cpu().numpy())
                        np.save(out_mel, item['mel'].detach().cpu().numpy())
                        fix_permissions(out_codebook)
                        fix_permissions(out_mel)

                        # ä¿å­˜ condition
                        condition_path = None
                        if 'condition' in item:
                            condition_path = os.path.join(output_dir, f"{base_name}_condition.npy")
                            np.save(condition_path, item['condition'])
                            fix_permissions(condition_path)
                            condition_files.append(condition_path)

                        # å¯«å…¥å…ƒè³‡æ–™åˆ° tmp æ–‡ä»¶
                        data_entry = {
                            "audio": item['wav_path'],
                            "text": item['text'],
                            "codes": os.path.abspath(out_codebook),
                            "mels": os.path.abspath(out_mel),
                            "duration": round(duration, 4)
                        }

                        if condition_path:
                            data_entry["condition"] = os.path.abspath(condition_path)

                        with open(metadata_tmp_file, "a", encoding="utf-8") as out_f:
                            out_f.write(json.dumps(data_entry, ensure_ascii=False) + "\n")

                    except Exception as e:
                        logger.error(f"ä¿å­˜å¤±æ•—: {item['wav_path']}, {e}")

                # æ›´æ–°é€²åº¦
                pbar.update(len(batch['audios']))
        
        # æˆåŠŸå®Œæˆå¾Œï¼Œå°‡ tmp æ–‡ä»¶é‡å‘½åç‚ºæ­£å¼æ–‡ä»¶
        if metadata_tmp_path.exists():
            metadata_tmp_path.rename(metadata_path)
            fix_permissions(metadata_path)
            logger.info(f"å…ƒè³‡æ–™å¯«å…¥å®Œæˆ: {metadata_file}")

    except Exception as e:
        logger.error(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        # ç™¼ç”ŸéŒ¯èª¤æ™‚ä¿ç•™ tmp æ–‡ä»¶ä»¥ä¾¿é™¤éŒ¯ï¼Œä½†ä¸ç”Ÿæˆæ­£å¼æ–‡ä»¶
        raise
    finally:
        # æ¸…ç† DataLoader å’Œ worker é€²ç¨‹
        del dataloader
        del dataset
        gc.collect()

    return metadata_file, condition_files


def main():
    """ä¸»å‡½å¼"""
    parser = argparse.ArgumentParser(
        description="Process audio data using DiscreteVAE and find medoid condition."
    )
    parser.add_argument(
        "--audio_list", 
        type=str, 
        required=True, 
        help="Path to the input audio list file."
    )
    parser.add_argument(
        "--extract_condition", 
        action="store_true", 
        help="Extract condition latents and find medoid."
    )
    parser.add_argument(
        "--distance_metric", 
        default='euclidean', 
        choices=['euclidean', 'cosine'], 
        help="Distance metric for medoid calculation."
    )
    parser.add_argument(
        "--output_dir", 
        default=DEFAULT_OUTPUT_DIR, 
        help="Output directory for processed data."
    )
    parser.add_argument(
        "--model_path", 
        default=DEFAULT_MODEL_PATH, 
        help="Path to the UnifiedVoice model checkpoint."
    )
    parser.add_argument(
        "--device",
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help="Device to use for computation."
    )
    parser.add_argument(
        "--medoid_devices",
        type=str,
        default=None,
        help="Comma-separated list of devices for medoid calculation (e.g., 'cuda:0,cuda:1')."
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="æ‰¹æ¬¡å¤§å°ï¼ˆå»ºè­° 8-16ï¼Œæ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´ï¼‰"
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=8,
        help="DataLoader worker æ•¸é‡ï¼ˆå»ºè­° 4-8ï¼‰"
    )

    args = parser.parse_args()
    
    # æª¢æŸ¥é…ç½®æª”æ¡ˆ
    config_path = os.path.join(FINETUNE_MODEL_DIR, CONFIG_FILENAME)
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æª”æ¡ˆæœªæ‰¾åˆ°: {config_path}")
    
    # è¼‰å…¥é…ç½®
    config = OmegaConf.load(config_path)
    
    # è¨­å®šè¼¸å‡ºç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    audio_list_name = Path(args.audio_list).stem
    output_dir = os.path.join(args.output_dir, f"{audio_list_name}_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    fix_permissions(Path(output_dir).parent)
    fix_permissions(output_dir)
    
    # è¨­å®šæ¨¡å‹
    dvae, unified_voice_model = setup_models(config, args)
    
    # å»ºç«‹è™•ç†å™¨
    audio_processor = AudioProcessor(dvae, config.dataset.mel, args.device)
    condition_extractor = None
    if unified_voice_model is not None:
        condition_extractor = ConditionExtractor(unified_voice_model, args.device)
    
    # è™•ç†éŸ³è¨Šæª”æ¡ˆ
    metadata_file, condition_files = process_audio_files(
        args, config, audio_processor, condition_extractor, output_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    logger.info(f"ç‰¹å¾µæå–å®Œæˆï¼Œçµæœå„²å­˜åœ¨: {output_dir}")
    
    # è¨ˆç®—medoidï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.extract_condition and condition_files:
        logger.info("é–‹å§‹è¨ˆç®—medoid condition...")
        total_samples = len(condition_files)
        logger.info(f"å…± {total_samples} å€‹æ¨£æœ¬ï¼Œä½¿ç”¨åˆ†æ‰¹è¨ˆç®—ï¼ˆç„¡æ¡æ¨£ï¼Œå®Œæ•´ç²¾ç¢ºï¼‰")

        # å¾é…ç½®æ–‡ä»¶è®€å– medoid è¨ˆç®—åƒæ•¸
        config_file_path = "scripts/config.yaml"
        batch_size = 1000  # é»˜èªå€¼
        chunk_size = 2000  # é»˜èªå€¼

        if os.path.exists(config_file_path):
            try:
                medoid_config = OmegaConf.load(config_file_path)
                if hasattr(medoid_config, 'medoid_batch_size'):
                    batch_size = medoid_config.medoid_batch_size
                if hasattr(medoid_config, 'medoid_chunk_size'):
                    chunk_size = medoid_config.medoid_chunk_size
                logger.info(f"å¾é…ç½®æ–‡ä»¶è®€å–: medoid_batch_size={batch_size}, medoid_chunk_size={chunk_size}")
            except Exception as e:
                logger.warning(f"è®€å–é…ç½®æ–‡ä»¶å¤±æ•—ï¼Œä½¿ç”¨é»˜èªå€¼: {e}")
        else:
            logger.info(f"ä½¿ç”¨é»˜èªå€¼: batch_size={batch_size}, chunk_size={chunk_size}")

        # æª¢æ¸¬å¯ç”¨çš„ GPU / è£ç½®
        gpu_ids = None
        medoid_device_list: Optional[List[str]] = None
        if args.medoid_devices:
            medoid_device_list = [dev.strip() for dev in args.medoid_devices.split(',') if dev.strip()]
            if medoid_device_list:
                logger.info(f"ä½¿ç”¨ä½¿ç”¨è€…æŒ‡å®šè£ç½®è¨ˆç®— medoid: {medoid_device_list}")

        if medoid_device_list is None and torch.cuda.is_available() and args.device.startswith('cuda'):
            num_gpus = torch.cuda.device_count()
            if num_gpus > 1:
                gpu_ids = list(range(num_gpus))
                medoid_device_list = [f"cuda:{gid}" for gid in gpu_ids]
                logger.info(f"æª¢æ¸¬åˆ° {num_gpus} å¼µ GPUï¼Œä½¿ç”¨å¤š GPU ä¸¦è¡Œè¨ˆç®— medoid")
            else:
                logger.info("åªæœ‰ 1 å¼µ GPUï¼Œä½¿ç”¨å–® GPU è¨ˆç®— medoid")

        try:
            medoid_result = MedoidCalculator.find_medoid_condition(
                condition_files, args.distance_metric, args.device,
                batch_size=batch_size, max_samples=chunk_size,
                gpu_ids=gpu_ids,
                medoid_devices=medoid_device_list
            )
            save_medoid_results(medoid_result, output_dir)

            # æ¸…ç† GPU ç·©å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.warning(f"GPU è¨˜æ†¶é«”ä¸è¶³: {e}")
                logger.info("é™ç´šä½¿ç”¨ CPU è¨ˆç®— medoid...")
                # é™ç´šåˆ° CPU
                try:
                    torch.cuda.empty_cache()
                    medoid_result = MedoidCalculator.find_medoid_condition(
                        condition_files, args.distance_metric, 'cpu',
                        batch_size=batch_size, max_samples=chunk_size,
                        gpu_ids=None,
                        medoid_devices=['cpu']
                    )
                    save_medoid_results(medoid_result, output_dir)
                except Exception as e2:
                    logger.error(f"è¨ˆç®—medoidæ™‚å‡ºéŒ¯: {e2}")
            else:
                raise
        except Exception as e:
            logger.error(f"è¨ˆç®—medoidæ™‚å‡ºéŒ¯: {e}")
    
    # åˆ†å‰²è³‡æ–™é›†
    split_dataset(metadata_file, output_dir)

    # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰ part éƒ½è™•ç†å®Œäº†ï¼Œå¦‚æœæ˜¯å‰‡è‡ªå‹•ç”Ÿæˆ speaker_info.json
    check_and_generate_speaker_info(output_dir)

    # æ¸…ç† GPU è¨˜æ†¶é«”
    logger.info("æ¸…ç† GPU è¨˜æ†¶é«”...")

    # åŒæ­¥æ‰€æœ‰ GPU æ“ä½œ
    if torch.cuda.is_available():
        torch.cuda.synchronize()

    # åˆªé™¤æ¨¡å‹å’Œè™•ç†å™¨
    del dvae
    del unified_voice_model
    del audio_processor
    del condition_extractor

    # å¼·åˆ¶åƒåœ¾å›æ”¶
    gc.collect()

    # æ¸…ç©º GPU ç·©å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # å†æ¬¡åŒæ­¥ç¢ºä¿æ¸…ç†å®Œæˆ
        torch.cuda.synchronize()

    logger.info("è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    main()
